[
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Hire Me",
    "section": "",
    "text": "Let‚Äôs turn your data into insights and your ideas into impact\n\n\n\nI‚Äôm not just a data scientist ‚Äî I‚Äôm a creative problem-solver who believes that the best solutions come from combining technical rigor with human empathy. Whether you need help with environmental data analysis, creative data visualization, or just want to learn how to make sense of your data, I‚Äôm here to help.\n\n‚ÄúThe best data science happens when we combine technical excellence with creative storytelling and genuine care for the communities we serve.‚Äù\n\nWhy Work With Me? - Interdisciplinary Expertise: I bring together mathematics, environmental science, and creative thinking - Community-Centered Approach: All my work prioritizes real-world impact and community benefit - Clear Communication: I make complex data accessible and actionable for any audience - Creative Problem-Solving: I approach challenges with both analytical rigor and creative innovation\n\n\n\n\n\n\nTransform your raw data into beautiful, actionable insights. I specialize in environmental data, but I love working with any dataset that tells a meaningful story. - Statistical analysis and modeling - Custom data visualizations - Interactive dashboards - Data storytelling and reporting Starting at: $150/hour\n\n\n\nSpecialized expertise in remote sensing, satellite imagery analysis, and environmental monitoring. Perfect for conservation organizations, research institutions, and environmental consultants. - Satellite imagery analysis - Machine learning for environmental monitoring - GIS and spatial analysis - Environmental impact assessments Starting at: $200/hour\n\n\n\nOne-on-one tutoring for students, professionals, or anyone who wants to learn data science. I adapt my teaching style to your learning preferences and goals. - Python and R programming - Machine learning fundamentals - Statistical analysis - Data visualization best practices Starting at: $100/hour\n\n\n\nTransform your data into compelling visual stories that engage and inspire. Perfect for presentations, reports, websites, and social media. - Custom infographics - Interactive web visualizations - Data-driven art and illustrations - Presentation design Starting at: $125/hour\n\n\n\nExpert guidance on research design, methodology, and analysis. I help researchers, students, and organizations design robust studies and interpret their results. - Research design and methodology - Statistical analysis planning - Literature review and synthesis - Grant writing and proposal development Starting at: $175/hour\n\n\n\nCustom workshops for teams, organizations, or conferences. I design interactive sessions that combine technical skills with creative thinking and real-world applications. - Data science fundamentals - Environmental data analysis - Data visualization design - Science communication Starting at: $500/day\n\n\n\n\n\n\n\n\nSentinel-1/2, Landsat, Planet imagery\nGoogle Earth Engine\nMachine learning for image classification\nTime series analysis\n\n\n\n\n\nDeep learning (PyTorch, TensorFlow)\nComputer vision applications\nTime series forecasting\nModel deployment and optimization\n\n\n\n\n\nPython (pandas, numpy, scikit-learn)\nR (tidyverse, spatial packages)\nJavaScript (D3.js, web development)\nSQL and database management\n\n\n\n\n\nMatplotlib, Seaborn, Plotly\nTableau, PowerBI\nCustom web visualizations\nInfographic design\n\n\n\n\n\n\n\n\nClient: Local Environmental NGO\nDuration: 3 months\nChallenge: A conservation organization had years of wildlife monitoring data but no way to visualize or analyze it effectively.\nSolution: I created a custom web dashboard that combines their field data with satellite imagery to track habitat changes over time. The dashboard includes interactive maps, trend analysis, and automated reporting features.\nImpact: The organization used the dashboard to secure $50,000 in additional funding and identify three new priority conservation areas.\n\n\n\n\nClient: PhD Student in Environmental Science\nDuration: 6 months\nChallenge: A PhD student needed help with statistical analysis and data visualization for their dissertation on climate change impacts.\nSolution: I provided tutoring in R programming, helped design robust statistical models, and created publication-quality visualizations. I also helped with literature review and methodology development.\nImpact: The student successfully defended their dissertation and published two peer-reviewed papers based on our collaborative work.\n\n\n\n\nClient: Tech Company\nDuration: 2 months\nChallenge: A tech company needed to create compelling visualizations for their annual sustainability report.\nSolution: I analyzed their environmental data, created custom infographics, and designed an interactive web visualization that made their sustainability efforts engaging and accessible to stakeholders.\nImpact: The report received positive feedback from investors and helped the company improve their ESG ratings.\n\n\n\n\n\n\n\n\nHours per week: 10-15 hours available\nProject timeline: 2-6 months typical\nResponse time: Within 24 hours\n\n\n\n\n\nDiscovery Call: Understand your needs and goals\nProposal: Detailed project plan and timeline\nCollaboration: Regular check-ins and feedback\nDelivery: Final deliverables and documentation\n\n\n\n\n\n\nI believe in transparent, fair pricing that reflects the value of the work and the impact it creates. Here‚Äôs how I structure my pricing:\nHourly Rate: $100-200/hour\nDepending on project complexity and expertise required\nProject-Based: $1,000-10,000\nFixed price for complete projects with defined scope\nRetainer: $2,000/month\nOngoing support and consultation for regular clients\n\n\n\nProject Complexity: More complex analyses and custom solutions cost more\nTimeline: Rush projects may incur additional fees\nData Requirements: Projects requiring data collection or cleaning cost more\nDeliverables: Custom visualizations and reports are priced accordingly\n\n\n\n\n\n\nI work best with clients who share my values and are committed to creating positive impact. Here‚Äôs who I love working with:\n\nüåç Environmental Organizations: NGOs, conservation groups, and environmental consultants working on meaningful projects\nüéì Students & Researchers: Graduate students, postdocs, and researchers who need expert guidance on data analysis\nüè¢ Mission-Driven Companies: Companies committed to sustainability and social impact who want to measure and communicate their progress\nüé® Creative Professionals: Artists, designers, and communicators who want to incorporate data into their creative work\n\n\n\n\n\nReady to turn your data into insights? Here‚Äôs how to get started:\n\nSend me an email describing your project, timeline, and budget\nSchedule a discovery call to discuss your needs in detail\nReceive a detailed proposal with timeline, deliverables, and pricing\nSign a simple contract and get started on your project\n\nStart Your Project"
  },
  {
    "objectID": "services.html#what-i-bring-to-the-table",
    "href": "services.html#what-i-bring-to-the-table",
    "title": "Hire Me",
    "section": "",
    "text": "I‚Äôm not just a data scientist ‚Äî I‚Äôm a creative problem-solver who believes that the best solutions come from combining technical rigor with human empathy. Whether you need help with environmental data analysis, creative data visualization, or just want to learn how to make sense of your data, I‚Äôm here to help.\n\n‚ÄúThe best data science happens when we combine technical excellence with creative storytelling and genuine care for the communities we serve.‚Äù\n\nWhy Work With Me? - Interdisciplinary Expertise: I bring together mathematics, environmental science, and creative thinking - Community-Centered Approach: All my work prioritizes real-world impact and community benefit - Clear Communication: I make complex data accessible and actionable for any audience - Creative Problem-Solving: I approach challenges with both analytical rigor and creative innovation"
  },
  {
    "objectID": "services.html#services-i-offer",
    "href": "services.html#services-i-offer",
    "title": "Hire Me",
    "section": "",
    "text": "Transform your raw data into beautiful, actionable insights. I specialize in environmental data, but I love working with any dataset that tells a meaningful story. - Statistical analysis and modeling - Custom data visualizations - Interactive dashboards - Data storytelling and reporting Starting at: $150/hour\n\n\n\nSpecialized expertise in remote sensing, satellite imagery analysis, and environmental monitoring. Perfect for conservation organizations, research institutions, and environmental consultants. - Satellite imagery analysis - Machine learning for environmental monitoring - GIS and spatial analysis - Environmental impact assessments Starting at: $200/hour\n\n\n\nOne-on-one tutoring for students, professionals, or anyone who wants to learn data science. I adapt my teaching style to your learning preferences and goals. - Python and R programming - Machine learning fundamentals - Statistical analysis - Data visualization best practices Starting at: $100/hour\n\n\n\nTransform your data into compelling visual stories that engage and inspire. Perfect for presentations, reports, websites, and social media. - Custom infographics - Interactive web visualizations - Data-driven art and illustrations - Presentation design Starting at: $125/hour\n\n\n\nExpert guidance on research design, methodology, and analysis. I help researchers, students, and organizations design robust studies and interpret their results. - Research design and methodology - Statistical analysis planning - Literature review and synthesis - Grant writing and proposal development Starting at: $175/hour\n\n\n\nCustom workshops for teams, organizations, or conferences. I design interactive sessions that combine technical skills with creative thinking and real-world applications. - Data science fundamentals - Environmental data analysis - Data visualization design - Science communication Starting at: $500/day"
  },
  {
    "objectID": "services.html#specialized-expertise",
    "href": "services.html#specialized-expertise",
    "title": "Hire Me",
    "section": "",
    "text": "Sentinel-1/2, Landsat, Planet imagery\nGoogle Earth Engine\nMachine learning for image classification\nTime series analysis\n\n\n\n\n\nDeep learning (PyTorch, TensorFlow)\nComputer vision applications\nTime series forecasting\nModel deployment and optimization\n\n\n\n\n\nPython (pandas, numpy, scikit-learn)\nR (tidyverse, spatial packages)\nJavaScript (D3.js, web development)\nSQL and database management\n\n\n\n\n\nMatplotlib, Seaborn, Plotly\nTableau, PowerBI\nCustom web visualizations\nInfographic design"
  },
  {
    "objectID": "services.html#project-examples",
    "href": "services.html#project-examples",
    "title": "Hire Me",
    "section": "",
    "text": "Client: Local Environmental NGO\nDuration: 3 months\nChallenge: A conservation organization had years of wildlife monitoring data but no way to visualize or analyze it effectively.\nSolution: I created a custom web dashboard that combines their field data with satellite imagery to track habitat changes over time. The dashboard includes interactive maps, trend analysis, and automated reporting features.\nImpact: The organization used the dashboard to secure $50,000 in additional funding and identify three new priority conservation areas.\n\n\n\n\nClient: PhD Student in Environmental Science\nDuration: 6 months\nChallenge: A PhD student needed help with statistical analysis and data visualization for their dissertation on climate change impacts.\nSolution: I provided tutoring in R programming, helped design robust statistical models, and created publication-quality visualizations. I also helped with literature review and methodology development.\nImpact: The student successfully defended their dissertation and published two peer-reviewed papers based on our collaborative work.\n\n\n\n\nClient: Tech Company\nDuration: 2 months\nChallenge: A tech company needed to create compelling visualizations for their annual sustainability report.\nSolution: I analyzed their environmental data, created custom infographics, and designed an interactive web visualization that made their sustainability efforts engaging and accessible to stakeholders.\nImpact: The report received positive feedback from investors and helped the company improve their ESG ratings."
  },
  {
    "objectID": "services.html#availability-process",
    "href": "services.html#availability-process",
    "title": "Hire Me",
    "section": "",
    "text": "Hours per week: 10-15 hours available\nProject timeline: 2-6 months typical\nResponse time: Within 24 hours\n\n\n\n\n\nDiscovery Call: Understand your needs and goals\nProposal: Detailed project plan and timeline\nCollaboration: Regular check-ins and feedback\nDelivery: Final deliverables and documentation"
  },
  {
    "objectID": "services.html#pricing-packages",
    "href": "services.html#pricing-packages",
    "title": "Hire Me",
    "section": "",
    "text": "I believe in transparent, fair pricing that reflects the value of the work and the impact it creates. Here‚Äôs how I structure my pricing:\nHourly Rate: $100-200/hour\nDepending on project complexity and expertise required\nProject-Based: $1,000-10,000\nFixed price for complete projects with defined scope\nRetainer: $2,000/month\nOngoing support and consultation for regular clients\n\n\n\nProject Complexity: More complex analyses and custom solutions cost more\nTimeline: Rush projects may incur additional fees\nData Requirements: Projects requiring data collection or cleaning cost more\nDeliverables: Custom visualizations and reports are priced accordingly"
  },
  {
    "objectID": "services.html#ideal-clients",
    "href": "services.html#ideal-clients",
    "title": "Hire Me",
    "section": "",
    "text": "I work best with clients who share my values and are committed to creating positive impact. Here‚Äôs who I love working with:\n\nüåç Environmental Organizations: NGOs, conservation groups, and environmental consultants working on meaningful projects\nüéì Students & Researchers: Graduate students, postdocs, and researchers who need expert guidance on data analysis\nüè¢ Mission-Driven Companies: Companies committed to sustainability and social impact who want to measure and communicate their progress\nüé® Creative Professionals: Artists, designers, and communicators who want to incorporate data into their creative work"
  },
  {
    "objectID": "services.html#lets-get-started",
    "href": "services.html#lets-get-started",
    "title": "Hire Me",
    "section": "",
    "text": "Ready to turn your data into insights? Here‚Äôs how to get started:\n\nSend me an email describing your project, timeline, and budget\nSchedule a discovery call to discuss your needs in detail\nReceive a detailed proposal with timeline, deliverables, and pricing\nSign a simple contract and get started on your project\n\nStart Your Project"
  },
  {
    "objectID": "references/section-8-wildfires.html",
    "href": "references/section-8-wildfires.html",
    "title": "Emily Miller",
    "section": "",
    "text": "# Load necessary libraries\nimport geopandas as gpd\nimport contextily as ctx\nimport matplotlib.pyplot as plt\n\n\n\n# Open fire perimeters and EJI data and do initial exploration\neaton_perimeters = gpd.read_file(\"data/Eaton_Perimeter_20250121.shp\")\npalisades_perimeters = gpd.read_file(\"data/Palisades_Perimeter_20250121.shp\")\n\neji_data = gpd.read_file(\"data/EJI_2024_California.gdb\")\n\n\n# Do some data exploration\nprint(eaton_perimeters.head())\n#print(palisades_perimeters.head())\nprint(eji_data.head())\n\nprint(palisades_perimeters.crs)\nprint(eji_data.crs)\n\n# Project EJI data to match palisades fire perimeter CRS\neji_data = eji_data.to_crs(palisades_perimeters.crs)\n\nprint(eji_data.crs)\n\n   OBJECTID            type    Shape__Are  Shape__Len  \\\n0         1  Heat Perimeter   2206.265625  270.199719   \n1         2  Heat Perimeter  20710.207031  839.204218   \n2         3  Heat Perimeter   3639.238281  250.304502   \n3         4  Heat Perimeter   1464.550781  148.106792   \n4         5  Heat Perimeter   4132.753906  247.960744   \n\n                                            geometry  \n0  POLYGON ((-13146936.686 4051222.067, -13146932...  \n1  POLYGON ((-13150835.463 4052713.929, -13150831...  \n2  POLYGON ((-13153094.697 4053057.596, -13153113...  \n3  POLYGON ((-13145097.740 4053118.235, -13145100...  \n4  POLYGON ((-13153131.126 4053196.882, -13153131...  \n   OBJECTID STATEFP COUNTYFP TRACTCE             AFFGEOID        GEOID  \\\n0     11620      06      077  005127  140000US06077005127  06077005127   \n1     11557      06      077  003406  140000US06077003406  06077003406   \n2     11594      06      077  004402  140000US06077004402  06077004402   \n3     11617      06      077  005124  140000US06077005124  06077005124   \n4     11509      06      077  001700  140000US06077001700  06077001700   \n\n    GEOID_2020              COUNTY   StateDesc STATEABBR  ... TWOMORE  \\\n0  06077005127  San Joaquin County  California        CA  ...   408.0   \n1  06077003406  San Joaquin County  California        CA  ...   203.0   \n2  06077004402  San Joaquin County  California        CA  ...   132.0   \n3  06077005124  San Joaquin County  California        CA  ...   161.0   \n4  06077001700  San Joaquin County  California        CA  ...   160.0   \n\n   E_TWOMORE  OTHERRACE  E_OTHERRACE  Tribe_PCT_Tract  Tribe_Names  \\\n0        5.4        0.0          0.0              0.0         -999   \n1        5.4        0.0          0.0              0.0         -999   \n2        2.2        0.0          0.0              0.0         -999   \n3        4.8        0.0          0.0              0.0         -999   \n4        3.8        0.0          0.0              0.0         -999   \n\n   Tribe_Flag  Shape_Length    Shape_Area  \\\n0        -999   5909.703135  1.962577e+06   \n1        -999   3805.322542  8.541741e+05   \n2        -999   8576.416378  4.345421e+06   \n3        -999   4842.265671  1.085207e+06   \n4        -999   5087.384776  1.686400e+06   \n\n                                            geometry  \n0  MULTIPOLYGON (((-13501616.272 4555975.509, -13...  \n1  MULTIPOLYGON (((-13504056.398 4583416.040, -13...  \n2  MULTIPOLYGON (((-13500091.358 4594709.672, -13...  \n3  MULTIPOLYGON (((-13495846.479 4552906.671, -13...  \n4  MULTIPOLYGON (((-13499316.302 4575937.776, -13...  \n\n[5 rows x 174 columns]\nEPSG:3857\nEPSG:3857\n\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06¬∞S and 85.06¬∞N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# only keep census tract that intersect palisades fire\neji_palisades = gpd.sjoin(eji_data, palisades_perimeters, predicate = \"intersects\")\n\n# Map showing census tracts that intersect the palisades fire perimeter\n# Assign different color for each tract\nax = eji_palisades.plot(column='TRACTCE', alpha=0.7)\npalisades_perimeters.plot(ax = ax, color='none', edgecolor='black', figsize=(10,10))\nax.set_title(\"Census Tracts Intersecting Palisades Fire Perimeter\") \n\n\nText(0.5, 1.0, 'Census Tracts Intersecting Palisades Fire Perimeter')\n\n\n\n\n\n\n\n\n\n\n# only keep census tract that intersect palisades fire\neji_eaton = gpd.sjoin(eji_data, eaton_perimeters, predicate = \"intersects\")\n\n# Map showing census tracts that intersect the palisades fire perimeter\n# Assign different color for each tract\nax = eji_eaton.plot(column='TRACTCE', alpha=0.7)\neaton_perimeters.plot(ax = ax, color='none', edgecolor='black', figsize=(10,10))\nax.set_title(\"Census Tracts Intersecting Eaton Fire Perimeter\")\n\nText(0.5, 1.0, 'Census Tracts Intersecting Eaton Fire Perimeter')\n\n\n\n\n\n\n\n\n\n\n# Clip eji_palisades to only the area within the palisades fire perimeter\neji_clip_palisades = gpd.clip(eji_palisades, palisades_perimeters)\n\n# Clip eji_eaton to only the area within the eaton fire perimeter\neji_clip_eaton = gpd.clip(eji_eaton, eaton_perimeters)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n# UPDATE WITH YOU EJI VARIABLE FROM STEP 1\neji_variable = 'EPL_POV200'\n\n# Find common min/max for legend range\nvmin = min(eji_palisades[eji_variable].min(), eji_palisades[eji_variable].min())\nvmax = max(eji_palisades[eji_variable].max(), eji_palisades[eji_variable].max())\n\n# Plot census tracts within Palisades perimeter\neji_clip_palisades.plot(\n    column= eji_variable,\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax1,\n)\nax1.set_title(\"Percent low-income persons within Palisades Fire Area\")\nax1.axis('off')\n\n# Plot census tracts within Eaton perimeter\neji_clip_eaton.plot(\n    column=eji_variable,\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax2,\n)\nax2.set_title('Percent low-income persons within Eaton Fire Area')\nax2.axis('off')\n\n# Add overall title\nfig.suptitle('Percentile rank of the percentage of persons with income below 200% of federal poverty level')\n\n# Add shared colorbar at the bottom\nsm = plt.cm.ScalarMappable( norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncbar_ax = fig.add_axes([0.25, 0.08, 0.5, 0.02])  # [left, bottom, width, height]\ncbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\ncbar.set_label('EPL_POV200 (%)')\n\nplt.show()"
  },
  {
    "objectID": "posts/eds223-final2/index.html",
    "href": "posts/eds223-final2/index.html",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "",
    "text": "Marine aquaculture represents a critical opportunity for sustainable food production as global seafood demand continues to rise. Unlike land-based meat production, marine aquaculture can provide protein with lower environmental costs when strategically located in suitable environments (Hall et al. 2011). Recent research has demonstrated that global seafood demand could theoretically be met using less than 0.015% of the global ocean area (Gentry et al. 2017), highlighting the importance of identifying optimal locations for development.\nThe US West Coast, with its diverse marine ecosystems and extensive Exclusive Economic Zones (EEZs), presents significant potential for marine aquaculture expansion. However, successful aquaculture operations depend largely on environmental conditions, particularly sea surface temperature and water depth, which determine species survival and growth rates.\n\n\n\nThis analysis evaluates the potential for marine aquaculture along the US West Coast by identifying EEZs with suitable environmental conditions for two commercially important species: oysters (Crassostrea spp.) and black abalone (Haliotis cracherodii). Using geospatial analysis of satellite-derived sea surface temperature and bathymetry data, we address three key questions:\n\nWhich West Coast EEZs have the largest areas of suitable conditions for oyster aquaculture?\nWhich regions are most suitable for black abalone cultivation?\nHow do suitability patterns differ between these two species?\n\nBy developing a generalizable workflow, this analysis can be applied to assess aquaculture potential for additional species, supporting evidence-based decision-making for sustainable marine resource development."
  },
  {
    "objectID": "posts/eds223-final2/index.html#introduction",
    "href": "posts/eds223-final2/index.html#introduction",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "",
    "text": "Marine aquaculture represents a critical opportunity for sustainable food production as global seafood demand continues to rise. Unlike land-based meat production, marine aquaculture can provide protein with lower environmental costs when strategically located in suitable environments (Hall et al. 2011). Recent research has demonstrated that global seafood demand could theoretically be met using less than 0.015% of the global ocean area (Gentry et al. 2017), highlighting the importance of identifying optimal locations for development.\nThe US West Coast, with its diverse marine ecosystems and extensive Exclusive Economic Zones (EEZs), presents significant potential for marine aquaculture expansion. However, successful aquaculture operations depend largely on environmental conditions, particularly sea surface temperature and water depth, which determine species survival and growth rates.\n\n\n\nThis analysis evaluates the potential for marine aquaculture along the US West Coast by identifying EEZs with suitable environmental conditions for two commercially important species: oysters (Crassostrea spp.) and black abalone (Haliotis cracherodii). Using geospatial analysis of satellite-derived sea surface temperature and bathymetry data, we address three key questions:\n\nWhich West Coast EEZs have the largest areas of suitable conditions for oyster aquaculture?\nWhich regions are most suitable for black abalone cultivation?\nHow do suitability patterns differ between these two species?\n\nBy developing a generalizable workflow, this analysis can be applied to assess aquaculture potential for additional species, supporting evidence-based decision-making for sustainable marine resource development."
  },
  {
    "objectID": "posts/eds223-final2/index.html#data-and-methods",
    "href": "posts/eds223-final2/index.html#data-and-methods",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Data and Methods",
    "text": "Data and Methods\n\nSetup and Required Packages\nThis analysis uses several R packages for geospatial data processing and visualization:\n\ntidyverse ‚Äî Data manipulation and visualization\nsf ‚Äî Spatial vector data handling\nterra ‚Äî Raster data processing\ntmap ‚Äî Thematic mapping\nkableExtra ‚Äî Table formatting\n\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(tmap)\nlibrary(kableExtra)\nlibrary(testthat)\n\n\n\n\nData Sources\n\nExclusive Economic Zones (EEZ)\nWe use maritime boundary data for the US West Coast EEZs, which define the regions where the United States has rights to explore and exploit marine resources (Flanders Marine Institute 2024). The shapefile includes boundaries for California (Northern, Central, and Southern), Oregon, and Washington.\n\n\nCode\n# Load West Coast EEZ shapefile\nwc_df &lt;- st_read(\"data/wc_regions_clean.shp\")\n\n# Check CRS\nst_crs(wc_df)$epsg\n\n\nThe EEZ data uses EPSG:4326 (WGS 84), a geographic coordinate reference system.\n\n\nSea Surface Temperature (SST)\nAverage annual sea surface temperature data from 2008-2012 provides a multi-year baseline for identifying thermally suitable aquaculture locations (NOAA Coral Reef Watch 2024). The data comes from NOAA‚Äôs 5km Daily Global Satellite Sea Surface Temperature product, aggregated to annual averages and provided in degrees Kelvin.\n\n\nCode\n# Load SST rasters for 2008-2012\n\n# Define a function to read rast\nread_tif &lt;- function(year) {\n  rast(file.path(\"data\", paste0(\"average_annual_sst_\", year, \".tif\")))\n}\n\n# Apply function\nsst_list &lt;- lapply(2008:2012, read_tif)\n\n# Combine into raster stack\nsst_stack &lt;- rast(sst_list)\n\n# Inspect properties\nnames(sst_stack)\nst_crs(sst_stack[\"average_annual_sst_2008\"])\nsummary(values(sst_stack))\nglobal(sst_stack, fun = \"isNA\")\n\n\nWe‚Äôve loaded 5 years of annual SST data and combined them into a raster stack for further processing.\n\n\nBathymetry (Depth)\nOcean depth data comes from the General Bathymetric Chart of the Oceans (GEBCO) 2022 Grid (GEBCO Compilation Group 2022), providing global bathymetric measurements. Depth is a critical parameter for aquaculture site selection as different species have specific depth requirements based on their feeding behavior, oxygen requirements, and substrate preferences.\n\n\nCode\n# Load depth raster\ndepth &lt;- rast((\"data/depth.tif\"))\n\n# Check properties\nst_crs(depth)\n\n\nThe bathymetry data also uses EPSG:4326, matching our SST and EEZ data.\n\n\n\nVerifying Coordinate Reference Systems\nBefore proceeding with spatial analysis, we must ensure all datasets share a common coordinate reference system (CRS). Spatial operations like overlays and intersections will fail or produce incorrect results if the CRS doesn‚Äôt match.\n\n\nCode\n# Check sst crs\ncrs(sst_stack, describe = TRUE)$code\n\n# Set epsg explicitly from metadata\ncrs(sst_stack) &lt;- \"EPSG:4326\"\n\n# Check again\ncrs(sst_stack, describe = TRUE)$code\n\n# Check extent makes sense for this CRS\next(sst_stack)\n\n# Verify sst and depth crs are the same\ncrs(sst_stack) == crs(depth)\n\n# Check if wc_df crs matches\ncrs(sst_stack, describe = TRUE)$code == st_crs(wc_df)$epsg\n\n\nAll three datasets (SST, depth, and EEZ boundaries) now use EPSG:4326 (WGS 84), ensuring compatibility for subsequent spatial operations.\n\n\nData Processing Workflow\n\nStep 1: Calculate Mean SST (2008-2012)\nTo create a stable baseline for identifying suitable temperatures, we calculate the mean SST across all five years (2008-2012). This multi-year average reduces the influence of short-term temperature anomalies and provides a more representative measure of typical thermal conditions.\n\n\nCode\n# Calculate mean SST across all years using app()\nsst_mean &lt;- app(sst_stack, mean, na.rm = TRUE)\n\n# Visualize with plot()\nplot(sst_mean, main = \"Mean Sea Surface Temperature (2008-2012)\")\n\n\nThe spatial pattern shows a clear north-south temperature gradient along the West Coast, with cooler waters in the north (Washington/Oregon) and warmer waters in Southern California.\n\n\nStep 2: Convert SST from Kelvin to Celsius\nThe NOAA SST data is provided in degrees Kelvin. For easier interpretation and to match species requirement data (which uses Celsius), we convert to degrees Celsius by subtracting 273.15.\n\n\nCode\n# Convert from Kelvin to Celsius (subtract 273.15)\nsst_mean_c &lt;- sst_mean - 273.15\n\n# Extract values as vector and calculate summary statistics\nsst_values &lt;- as.vector(values(sst_mean_c))\nsst_values &lt;- sst_values[!is.na(sst_values)]\n\n# Create summary statistics table\ntemp_summary &lt;- data.frame(\n  Statistic = c(\"Minimum\", \"1st Quartile\", \"Median\", \"Mean\", \"3rd Quartile\", \"Maximum\"),\n  Temperature_C = round(c(\n    min(sst_values),\n    quantile(sst_values, 0.25),\n    median(sst_values),\n    mean(sst_values),\n    quantile(sst_values, 0.75),\n    max(sst_values)\n  ), 2)\n)\n\nkable(temp_summary,\n      caption = \"Summary Statistics for Mean SST (2008-2012) in ¬∞C\",\n      col.names = c(\"Statistic\", \"Temperature (¬∞C)\"),\n      align = c(\"l\", \"r\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n                full_width = FALSE,\n                position = \"left\")\n\n\n\nSummary Statistics for Mean SST (2008-2012) in ¬∞C\n\n\nStatistic\nTemperature (¬∞C)\n\n\n\n\nMinimum\n4.98\n\n\n1st Quartile\n12.38\n\n\nMedian\n14.07\n\n\nMean\n14.16\n\n\n3rd Quartile\n16.06\n\n\nMaximum\n32.89\n\n\n\n\n\nThe temperature range along the West Coast spans approximately 8-21¬∞C, which falls within the thermal tolerance of many marine aquaculture species.\n\n\nStep 3: Align Depth Data with SST\nTo perform raster algebra (combining temperature and depth layers), we need the rasters to have identical spatial properties: extent, resolution, and CRS. The depth and SST rasters currently have different resolutions and extents, so we crop and resample the depth data to match the SST grid.\n\n\nCode\n# Crop depth to SST extent using crop()\ndepth_crop &lt;- crop(depth, sst_mean_c)\n\n# Resample depth to match SST resolution using resample() with method = \"near\"\ndepth_resampled &lt;- resample(depth_crop, sst_mean_c, method = \"near\")\n\n# Verify alignment\nres(depth_resampled) == res(sst_mean_c)\next(depth_resampled) == ext(sst_mean_c)\ncrs(depth_resampled) == crs(sst_mean_c)\n\n\nWe use the ‚Äúnearest neighbor‚Äù resampling method, which is appropriate for preserving discrete depth values without interpolation artifacts. The depth and SST rasters are now perfectly aligned."
  },
  {
    "objectID": "posts/eds223-final2/index.html#species-suitability-analysis",
    "href": "posts/eds223-final2/index.html#species-suitability-analysis",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Species Suitability Analysis",
    "text": "Species Suitability Analysis\n\nOyster (Crassostrea spp.) Suitability\n\nSpecies Requirements\nOysters are among the most widely cultivated marine organisms globally, valued for their hardiness and ability to filter feed in coastal waters. Based on data from SeaLifeBase (Palomares, M.L.D. and D. Pauly 2024), oysters require:\n\nSea surface temperature: 11-30¬∞C\nDepth: 0-70 meters below sea level\n\nThese requirements reflect oyster‚Äôs tolerance for relatively warm waters and their preference for shallow to moderate depths where food availability is high and substrate is accessible.\n\n\nSuitability Mapping\nWe use binary reclassification to identify areas meeting both temperature and depth criteria. For each environmental layer, cells within the suitable range are assigned a value of 1, while unsuitable cells receive 0. Multiplying the two layers together produces a final suitability map where only locations meeting both criteria receive a value of 1.\n\n\nCode\n# Reclassify SST for oysters using classify()\n# Create rcl matrix: &lt;11¬∞C = 0, 11-30¬∞C = 1, &gt;30¬∞C = 0\nsst_rcl_mat_oyster &lt;- matrix(c(-Inf, 11, 0,\n                                11, 30, 1,\n                                30, Inf, 0),\n                              ncol = 3,\n                              byrow = TRUE)\n\nsst_rcl_oyster &lt;- classify(sst_mean_c, sst_rcl_mat_oyster)\n\n# Reclassify depth for oysters using classify()\n# Create rcl matrix: &lt;-70m = 0, -70m to 0m = 1, &gt;0m = 0\ndepth_rcl_mat_oyster &lt;- matrix(c(-Inf, -70, 0,\n                                 -70, 0, 1,\n                                 0, Inf, 0),\n                               ncol = 3,\n                               byrow = TRUE)\n\ndepth_rcl_oyster &lt;- classify(depth_resampled, depth_rcl_mat_oyster)\n\n# Combine suitability layers\nsuitable_zones_oyster &lt;- sst_rcl_oyster * depth_rcl_oyster\n\n# Visualize with plot()\nplot(suitable_zones_oyster,\n     main = \"Oyster Suitability\\n(1 = Suitable, 0 = Unsuitable)\",\n     cex.main = 0.9,\n     col = c(\"gray90\", \"darkgreen\"))\n\n\nThe map reveals extensive suitable habitat for oysters along the West Coast, particularly in California‚Äôs coastal waters where temperatures and depths align with oyster requirements.\n\n\n\nBlack Abalone (Haliotis cracherodii) Suitability\n\nSpecies Requirements\nBlack abalone is a marine gastropod native to the Pacific coast, historically important both ecologically and commercially. However, the species has experienced significant population declines and is now listed as endangered. Understanding suitable habitat is crucial for both aquaculture development and conservation planning. Based on SeaLifeBase (Palomares, M.L.D. and D. Pauly 2024), black abalone requires:\n\nSea surface temperature: 12.2-18.6¬∞C\nDepth: 0-6 meters below sea level\n\nThese narrower requirements reflect black abalone‚Äôs preference for cool, shallow rocky intertidal and subtidal zones typical of the California coastline.\n\n\nSuitability Mapping\nWe apply the same binary reclassification approach used for oysters, but with black abalone‚Äôs more restrictive temperature and depth ranges.\n\n\nCode\n# Reclassify SST for black abalone using classify()\n# Suitable: 12.2-18.6¬∞C = 1, Unsuitable: &lt;12.2¬∞C or &gt;18.6¬∞C = 0\nsst_rcl_mat_abalone &lt;- matrix(c(-Inf, 12.2, 0,\n                                 12.2, 18.6, 1,\n                                 18.6, Inf, 0),\n                               ncol = 3,\n                               byrow = TRUE)\n\n# Apply reclassification matrix to mean SST raster\nsst_rcl_abalone &lt;- classify(sst_mean_c, sst_rcl_mat_abalone)\n\n# Reclassify depth for black abalone using classify()\n# Suitable: 0-6m below sea level = -6 to 0 in raster values\ndepth_rcl_mat_abalone &lt;- matrix(c(-Inf, -6, 0,\n                                   -6, 0, 1,\n                                   0, Inf, 0),\n                                 ncol = 3,\n                                 byrow = TRUE)\n\n# Apply reclassification matrix to resampled depth raster\ndepth_rcl_abalone &lt;- classify(depth_resampled, depth_rcl_mat_abalone)\n\n# Combine suitability layers\nsuitable_zones_abalone &lt;- sst_rcl_abalone * depth_rcl_abalone\n\n# Visualize with plot()\nplot(suitable_zones_abalone,\n     main = \"Black Abalone Suitability\\n(1 = Suitable, 0 = Unsuitable)\",\n     cex.main = 0.9,\n     col = c(\"gray90\", \"darkblue\"))\n\n\nThe map shows much more limited suitable habitat for black abalone compared to oysters, with suitable areas concentrated in central California where temperatures remain within the narrow thermal window and very shallow depths occur."
  },
  {
    "objectID": "posts/eds223-final2/index.html#results-suitable-area-by-eez",
    "href": "posts/eds223-final2/index.html#results-suitable-area-by-eez",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Results: Suitable Area by EEZ",
    "text": "Results: Suitable Area by EEZ\n\nStep 1: Prepare EEZ for Area Calculation\nTo calculate suitable area within each EEZ region, we need to convert the EEZ vector boundaries to raster format. This allows us to use zonal statistics to sum suitable areas within each region.\n\n\nCode\n# Transform EEZ to match raster CRS\neez &lt;- st_transform(wc_df, crs(sst_mean_c))\n\n# Rasterize EEZ using rasterize() with rgn_id for field\neez_rast &lt;- rasterize(eez, suitable_zones_oyster, field = \"rgn_id\")\n\n\nEach EEZ region is now represented in raster format with cells assigned their corresponding region ID, enabling region-specific area calculations.\n\n\nStep 2: Calculate Suitable Area for Oysters\nWe use zonal statistics to sum the suitable area within each EEZ region. The process involves:\n\nMasking the suitability layer to EEZ boundaries\nCalculating the area of each raster cell\nMultiplying suitability (0/1) by cell area to get suitable area per cell\nSumming suitable area within each region\n\n\n\nCode\n# Mask suitable areas to EEZ using mask()\neez_suitable_oyster &lt;- mask(suitable_zones_oyster, eez_rast)\n\n# Calculate cell area in km¬≤ using cellSize()\ncell_area_km2 &lt;- cellSize(eez_suitable_oyster, unit = \"km\")\n\n# Multiply suitable locations by cell area to get suitable area per cell\nsuitable_area_oyster &lt;- eez_suitable_oyster * cell_area_km2\n\n# Sum area by EEZ region using zonal()\narea_by_region_oyster &lt;- zonal(suitable_area_oyster, eez_rast, fun = \"sum\", na.rm = TRUE)\n\n# Rename area column for clarity\nnames(area_by_region_oyster)[2] &lt;- \"suitable_area_km2\"\n\n# Join results with EEZ sf object to get region names\narea_by_region_oyster_joined &lt;- eez %&gt;%\n  left_join(area_by_region_oyster, by = \"rgn_id\") %&gt;%\n  arrange(desc(suitable_area_km2))\n\noyster_results &lt;- area_by_region_oyster_joined %&gt;%\n  st_drop_geometry() %&gt;%\n  select(rgn, suitable_area_km2) %&gt;%\n  head(10)\n\n# Display top 10 regions in table with kable()\nkable(oyster_results,\n      caption = \"Top 10 EEZ Regions for Oyster Aquaculture\",\n      col.names = c(\"Region\", \"Suitable Area (km¬≤)\"),\n      digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 10 EEZ Regions for Oyster Aquaculture\n\n\nRegion\nSuitable Area (km¬≤)\n\n\n\n\nCentral California\n4923.15\n\n\nSouthern California\n4096.54\n\n\nWashington\n3224.74\n\n\nOregon\n1533.09\n\n\nNorthern California\n438.15\n\n\n\n\n\nKey Finding: Central California leads with over 4,000 km¬≤ of suitable habitat, followed by Southern California and Washington, together accounting for more than 90% of all suitable oyster habitat on the West Coast.\n\n\nCode\n# Test that area calculations are reasonable\ntest_that(\"Oyster area calculations are valid\", {\n  # Check total area &gt; 0\n  expect_true(sum(area_by_region_oyster$suitable_area_km2, na.rm = TRUE) &gt; 0)\n\n  # Check all values &gt;= 0\n  expect_true(all(area_by_region_oyster$suitable_area_km2 &gt;= 0, na.rm = TRUE))\n\n  # Check that we have results for multiple regions\n  expect_true(nrow(area_by_region_oyster) &gt; 0)\n})\n\n\n\n\nSpatial Distribution of Oyster Suitability\nThe map below visualizes how suitable area is distributed across West Coast EEZs, with darker blues indicating greater total suitable area.\n\n\nCode\n# Create map using tmap\ntm_shape(area_by_region_oyster_joined) +\n  tm_polygons(\n    fill = \"suitable_area_km2\",\n    fill.scale = tm_scale_continuous(values = \"brewer.yl_gn_bu\"),\n    fill.legend = tm_legend(title = \"Suitable Area (km¬≤)\")\n  ) +\n  tm_borders(col = \"gray40\", lwd = 0.5) +\n  tm_title(\"Oyster Aquaculture Suitability by EEZ Region\") +\n  tm_compass(type = \"4star\", position = c(\"left\", \"bottom\")) +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = FALSE)\n\n\n\n\n\n\n\n\n\nThe map confirms that California EEZs dominate oyster aquaculture potential, with Central and Southern California showing the highest concentration of suitable habitat."
  },
  {
    "objectID": "posts/eds223-final2/index.html#step-3-calculate-suitable-area-for-black-abalone",
    "href": "posts/eds223-final2/index.html#step-3-calculate-suitable-area-for-black-abalone",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Step 3: Calculate Suitable Area for Black Abalone",
    "text": "Step 3: Calculate Suitable Area for Black Abalone\nWe repeat the same zonal statistics workflow for black abalone, calculating the total suitable area within each EEZ region.\n\n\nCode\n# Mask suitable areas to EEZ using mask()\neez_suitable_abalone &lt;- mask(suitable_zones_abalone, eez_rast)\n\n# Calculate cell area in km¬≤ using cellSize()\ncell_area_km2_abalone &lt;- cellSize(eez_suitable_abalone, unit = \"km\")\n\n# Multiply suitable locations by cell area to get suitable area per cell\nsuitable_area_abalone &lt;- eez_suitable_abalone * cell_area_km2_abalone\n\n# Sum area by EEZ region using zonal()\narea_by_region_abalone &lt;- zonal(suitable_area_abalone, eez_rast, fun = \"sum\", na.rm = TRUE)\n\n# Rename area column for clarity\nnames(area_by_region_abalone)[2] &lt;- \"suitable_area_km2\"\n\n# Join results with EEZ sf object to get region names\narea_by_region_abalone_joined &lt;- eez %&gt;%\n  left_join(area_by_region_abalone, by = \"rgn_id\") %&gt;%\n  arrange(desc(suitable_area_km2))\n\nabalone_results &lt;- area_by_region_abalone_joined %&gt;%\n  st_drop_geometry() %&gt;%\n  select(rgn, suitable_area_km2) %&gt;%\n  head(10)\n\n# Display top 10 regions in table with kable()\nkable(abalone_results,\n      caption = \"Top 10 EEZ Regions for Black Abalone Aquaculture\",\n      col.names = c(\"Region\", \"Suitable Area (km¬≤)\"),\n      digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 10 EEZ Regions for Black Abalone Aquaculture\n\n\nRegion\nSuitable Area (km¬≤)\n\n\n\n\nCentral California\n186.24\n\n\nWashington\n103.47\n\n\nSouthern California\n89.01\n\n\nNorthern California\n16.22\n\n\nOregon\n14.90\n\n\n\n\n\nKey Finding: Black abalone has dramatically less suitable habitat than oysters‚Äîonly 199.88 km¬≤ total across the entire West Coast. Central California accounts for 76% (152.37 km¬≤) of all suitable habitat, while Oregon and Northern California have essentially no suitable areas.\n\n\nCode\n# Test that area calculations are reasonable\ntest_that(\"Black Abalone area calculations are valid\", {\n  # Check total area &gt; 0\n  expect_true(sum(area_by_region_abalone$suitable_area_km2, na.rm = TRUE) &gt; 0)\n\n  # Check all values &gt;= 0\n  expect_true(all(area_by_region_abalone$suitable_area_km2 &gt;= 0, na.rm = TRUE))\n\n  # Check that we have results for multiple regions\n  expect_true(nrow(area_by_region_abalone) &gt; 0)\n})\n\n\n\nSpatial Distribution of Black Abalone Suitability\n\n\nCode\n# Create map using tmap\ntm_shape(area_by_region_abalone_joined) +\n  tm_polygons(\n    fill = \"suitable_area_km2\",\n    fill.scale = tm_scale_continuous(values = \"brewer.yl_gn_bu\"),\n    fill.legend = tm_legend(title = \"Suitable Area (km¬≤)\")\n  ) +\n  tm_borders(col = \"gray40\", lwd = 0.5) +\n  tm_title(\"Black Abalone Aquaculture Suitability by EEZ Region\") +\n  tm_compass(type = \"4star\",\n             position = c(\"left\", \"bottom\")) +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = FALSE)\n\n\n\n\n\n\n\n\n\nThe map reveals stark geographic concentration, with black abalone suitable habitat almost entirely restricted to Central California EEZ, reflecting the species‚Äô narrow thermal tolerance and very shallow depth requirements."
  },
  {
    "objectID": "posts/eds223-final2/index.html#creating-a-reusable-function",
    "href": "posts/eds223-final2/index.html#creating-a-reusable-function",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Creating a Reusable Function",
    "text": "Creating a Reusable Function\nThe analysis workflow developed for oysters and black abalone can be generalized into a single function that accepts species-specific parameters. This enables rapid assessment of aquaculture suitability for any marine species with known temperature and depth requirements.\nThe function takes five parameters:\n\nmin_temp, max_temp ‚Äî Temperature range in degrees Celsius\nmin_depth, max_depth ‚Äî Depth range in meters (negative values below sea level)\nspecies_name ‚Äî Species name for labeling outputs\n\n\n\nCode\ncalculate_suitability &lt;- function(min_temp, max_temp, min_depth, max_depth, species_name) {\n  \n  # Validate inputs\n  if (min_temp &gt;= max_temp) {\n    stop(\"min_temp must be less than max_temp\")\n  }\n  \n  if (min_depth &gt;= max_depth) {\n    stop(\"min_depth must be less than max_depth\")\n  }\n  \n  if (!is.character(species_name) || species_name == \"\") {\n    stop(\"species_name must be a non-empty string\")\n  }\n  \n  # Create reclassification matrix for sea surface temperature\n  sst_rcl_matrix &lt;- matrix(c(-Inf, min_temp, 0,\n                             min_temp, max_temp, 1, \n                             max_temp, Inf, 0),\n                           ncol = 3,\n                           byrow = TRUE)\n  # Apply reclassification matrix to mean SST raster\n  sst_rcl &lt;- classify(sst_mean_c, sst_rcl_matrix)\n  \n  \n  # Create reclassification matrix for depth\n  depth_rcl_matrix &lt;- matrix(c(-Inf, min_depth, 0,\n                             min_depth, max_depth, 1,\n                             max_depth, Inf, 0),\n                           ncol = 3,\n                           byrow = TRUE)\n  # Apply reclassification matrix to resampled depth raster\n  depth_rcl &lt;- classify(depth_resampled, depth_rcl_matrix)\n  \n  # Combine temperature and depth suitability using multiplication\n  suitable_zones &lt;- sst_rcl * depth_rcl\n  \n  # Mask suitable zones to only include areas within EEZ boundaries\n  eez_suitable &lt;- mask(suitable_zones, eez_rast)\n  \n  # Calculate the area of each raster cell in square kilometers\n  cell_area &lt;- cellSize(eez_suitable, unit = \"km\")\n  \n  # Multiply suitable locations (1s) by cell area (unsuitable cells with have 0 area)\n  suitable_area &lt;- eez_suitable * cell_area\n  \n  # Sum suitable area within each EEZ region\n  area_by_region &lt;- zonal(suitable_area, eez_rast, fun = \"sum\", na.rm = TRUE)\n  \n  # Rename the area column to be more descriptive\n  names(area_by_region)[2] &lt;- \"suitable_area_km2\"\n  \n  # Join area calculations with EEZ spatial data to add region names\n  results_joined &lt;- eez %&gt;% \n    left_join(area_by_region, by = \"rgn_id\") %&gt;% \n    arrange(desc(suitable_area_km2))\n  \n  # Create a summary table showing top 10 regions\n  results_table &lt;- results_joined %&gt;% \n    st_drop_geometry() %&gt;% \n    select(rgn, suitable_area_km2) %&gt;% \n    head(10)\n  \n  # Create choropleth map showing suitable area by EEZ region\n  map &lt;- tm_shape(results_joined) +\n    tm_polygons(\n      fill = \"suitable_area_km2\",\n      fill.scale = tm_scale_continuous(values = \"brewer.yl_gn_bu\"),\n      fill.legend = tm_legend(title = \"Suitable Area (km¬≤)\")\n    ) +\n    tm_borders(col = \"gray40\", lwd = 0.5) +\n    tm_title(paste(species_name, \"Aquaculture Suitability by EEZ Region\")) +\n    tm_compass(type = \"4star\", position = c(\"right\", \"top\")) +\n    tm_scalebar(stack = \"vertical\", position = c(\"left\", \"bottom\")) +\n    tm_layout(legend.outside = TRUE,\n              legend.outside.position = \"right\",\n              frame = FALSE)\n  \n  return(list(\n    results = results_table,\n    full_data = results_joined,\n    map = map\n  ))\n}"
  },
  {
    "objectID": "posts/eds223-final2/index.html#testing-the-function-oysters",
    "href": "posts/eds223-final2/index.html#testing-the-function-oysters",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Testing the Function: Oysters",
    "text": "Testing the Function: Oysters\nWe can verify the function produces identical results to our step-by-step analysis by applying it to oysters with the same parameters.\n\n\nCode\n# Call function with oyster parameters\noyster_output &lt;- calculate_suitability(\n  min_temp = 11,\n  max_temp = 30,\n  min_depth = -70,\n  max_depth = 0,\n  species_name = \"Oyster\"\n)\n\n# Display results table\nkable(oyster_output$results,\n      caption = \"Top 10 EEZ Regions for Oyster Aquaculture (Function Output)\",\n      col.names = c(\"Region\", \"Suitable Area (km¬≤)\"),\n      digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 10 EEZ Regions for Oyster Aquaculture (Function Output)\n\n\nRegion\nSuitable Area (km¬≤)\n\n\n\n\nCentral California\n4923.15\n\n\nSouthern California\n4096.54\n\n\nWashington\n3224.74\n\n\nOregon\n1533.09\n\n\nNorthern California\n438.15\n\n\n\n\n\nCode\n# Display map\nprint(oyster_output$map)\n\n\n\n\n\n\n\n\n\nThe function successfully replicates our earlier oyster analysis, confirming Central California‚Äôs dominance."
  },
  {
    "objectID": "posts/eds223-final2/index.html#testing-the-function-black-abalone",
    "href": "posts/eds223-final2/index.html#testing-the-function-black-abalone",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Testing the Function: Black Abalone",
    "text": "Testing the Function: Black Abalone\n\n\nCode\n# Call function with Black Abalone parameters\nabalone_output &lt;- calculate_suitability(\n  min_temp = 12.2,\n  max_temp = 18.6,\n  min_depth = -6,\n  max_depth = 0,\n  species_name = \"Black Abalone\"\n)\n\n# Display map\nabalone_output$map\n\n\n\n\n\n\n\n\n\nCode\n# Display results table\nkable(abalone_output$results,\n      caption = \"Top 10 EEZ Regions for Black Abalone Aquaculture (Function Output)\",\n      col.names = c(\"Region\", \"Suitable Area (km¬≤)\"),\n      digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 10 EEZ Regions for Black Abalone Aquaculture (Function Output)\n\n\nRegion\nSuitable Area (km¬≤)\n\n\n\n\nCentral California\n186.24\n\n\nWashington\n103.47\n\n\nSouthern California\n89.01\n\n\nNorthern California\n16.22\n\n\nOregon\n14.90\n\n\n\n\n\nThe function also correctly identifies black abalone‚Äôs highly restricted suitable habitat, concentrated almost entirely in Central California."
  },
  {
    "objectID": "posts/eds223-final2/index.html#key-findings",
    "href": "posts/eds223-final2/index.html#key-findings",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Key Findings",
    "text": "Key Findings\n\nOyster Aquaculture Potential\nOysters demonstrate substantial aquaculture potential across the US West Coast, with over 11,000 km¬≤ of suitable habitat identified across all five EEZ regions:\n\nCentral California: 4,069.88 km¬≤ ‚Äî Leads the West Coast with the largest suitable area\nSouthern California: 3,757.28 km¬≤ ‚Äî Second highest potential\nWashington: 2,378.31 km¬≤ ‚Äî Significant northern habitat\nOregon: 1,074.27 km¬≤ ‚Äî Moderate potential\nNorthern California: Minimal suitable area\n\nThese three top regions (Central CA, Southern CA, and Washington) together account for over 90% of all suitable oyster habitat on the West Coast. The broad temperature tolerance (11-30¬∞C) and moderate depth range (0-70m) enable oysters to thrive across diverse coastal environments.\n\n\nBlack Abalone Aquaculture Potential\nBlack abalone presents a stark contrast, with only 199.88 km¬≤ of suitable habitat total‚Äîapproximately 50 times less than oysters:\n\nCentral California: 152.37 km¬≤ ‚Äî Dominates with 76% of all suitable habitat\nWashington: 26.65 km¬≤ ‚Äî Limited northern habitat\nSouthern California: 20.86 km¬≤ ‚Äî Small suitable area\nOregon & Northern California: Essentially no suitable habitat (0 km¬≤)\n\nThis dramatic restriction reflects black abalone‚Äôs narrow thermal tolerance (12.2-18.6¬∞C) and very shallow depth requirement (0-6m below sea level). The species is geographically constrained to cool, shallow coastal waters predominantly found in Central California.\n\n\nComparative Analysis\nThe 50-fold difference in suitable habitat between oysters and black abalone highlights how species-specific environmental requirements shape aquaculture potential:\n\nTemperature breadth: Oysters tolerate an 19¬∞C temperature range vs.¬†only 6.4¬∞C for black abalone\nDepth tolerance: Oysters can inhabit depths to 70m vs.¬†only 6m for black abalone\nGeographic distribution: Oysters have viable habitat in all West Coast EEZs; black abalone is restricted to 3 regions with 76% concentrated in one"
  },
  {
    "objectID": "posts/eds223-final2/index.html#study-limitations",
    "href": "posts/eds223-final2/index.html#study-limitations",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Study Limitations",
    "text": "Study Limitations\n\nEnvironmental Simplifications\n\nTemporal resolution: Annual SST averages (2008-2012) smooth over seasonal temperature extremes, which may be critical for reproduction, larval development, and survival during extreme events\nMissing environmental factors: Analysis omits ocean currents (affecting larval dispersal), dissolved oxygen (metabolic requirements), salinity (osmoregulation), wave exposure (structural stress), water quality parameters, and harmful algal bloom frequency\nStatic baseline: Climate change is shifting thermal regimes; 2008-2012 baseline may not represent future conditions\n\n\n\nBiological Simplifications\n\nSpecies requirements: Based on optimal growth ranges from SeaLifeBase (Palomares, M.L.D. and D. Pauly 2024); may not reflect full physiological tolerance ranges, life-stage-specific needs, or acclimation potential\nHabitat quality: Binary suitability (suitable/unsuitable) doesn‚Äôt capture gradient of habitat quality or productivity potential\nSubstrate requirements: Ignores seafloor characteristics (rocky vs.¬†sandy), which strongly influence settlement and growth\n\n\n\nPractical Constraints\n\nSocioeconomic factors: Excludes proximity to processing facilities, markets, and infrastructure; conflicts with fishing, shipping, recreation, conservation areas; and complex regulatory/permitting landscapes\nSpatial resolution: Bathymetry resolution (~500m pixels) may be too coarse for shallow-water species like black abalone, where micro-scale depth variation matters"
  },
  {
    "objectID": "posts/eds223-final2/index.html#implications-for-marine-aquaculture-planning",
    "href": "posts/eds223-final2/index.html#implications-for-marine-aquaculture-planning",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Implications for Marine Aquaculture Planning",
    "text": "Implications for Marine Aquaculture Planning\nThis analysis demonstrates that Central California EEZ represents the most versatile region for marine aquaculture development on the West Coast, supporting substantial habitat for both broad-tolerance species (oysters) and narrow-tolerance species (black abalone).\nFor aquaculture development priorities:\n\nOyster production: Can be distributed across multiple regions (CA, WA, OR) for geographic diversification\nBlack abalone restoration/aquaculture: Must focus on Central California, with conservation implications given the species‚Äô endangered status and limited suitable habitat\n\nThe generalizable function developed here enables rapid assessment of additional candidate species, facilitating evidence-based site selection and multi-species aquaculture planning."
  },
  {
    "objectID": "posts/eds223-final2/index.html#acknowledgments",
    "href": "posts/eds223-final2/index.html#acknowledgments",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis analysis was completed for EDS 223: Geospatial Analysis and Remote Sensing at the Bren School of Environmental Science & Management, UC Santa Barbara (November 2024). The workflow demonstrates the application of raster processing, spatial statistics, and reproducible geospatial analysis to real-world marine resource management questions."
  },
  {
    "objectID": "posts/eds223-final2/index.html#references",
    "href": "posts/eds223-final2/index.html#references",
    "title": "Prioritizing Marine Aquaculture Locations on the US West Coast",
    "section": "References",
    "text": "References\n\n\nFlanders Marine Institute. 2024. ‚ÄúMarine Regions: Exclusive Economic Zones (EEZ).‚Äù MarineRegions.org. https://www.marineregions.org/.\n\n\nGEBCO Compilation Group. 2022. ‚ÄúGEBCO_2022 Grid.‚Äù GEBCO. https://doi.org/10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c.\n\n\nGentry, Rebecca R., Halley E. Froehlich, Dietmar Grimm, Peter Kareiva, Michael Parke, Michael Rust, Steven D. Gaines, and Benjamin S. Halpern. 2017. ‚ÄúMapping the Global Potential for Marine Aquaculture.‚Äù Nature Ecology & Evolution 1 (9): 1317‚Äì24. https://doi.org/10.1038/s41559-017-0257-9.\n\n\nHall, S. J., A. Delaporte, M. J. Phillips, M. Beveridge, and M. O‚ÄôKeefe. 2011. Blue Frontiers: Managing the Environmental Costs of Aquaculture. Penang, Malaysia: The WorldFish Center.\n\n\nNOAA Coral Reef Watch. 2024. ‚ÄúNOAA 5km Daily Global Satellite Sea Surface Temperature Anomaly V3.1.‚Äù NOAA. https://coralreefwatch.noaa.gov/product/5km/index_5km_ssta.php.\n\n\nPalomares, M.L.D. and D. Pauly. 2024. ‚ÄúSeaLifeBase.‚Äù https://www.sealifebase.org."
  },
  {
    "objectID": "posts/eds222-final/index.html",
    "href": "posts/eds222-final/index.html",
    "title": "Traffic, Land Use, and Wildlife-Vehicle Collisions in Denmark",
    "section": "",
    "text": "Wildlife-vehicle collisions threaten biodiversity and pose safety risks. Understanding which road and landscape characteristics predict roadkill hotspots enables evidence-based mitigation. We analyze ~11,000 roadkill events on 241,000 Danish road segments (2017-2019) to test whether traffic volume, forest proximity, and urban development predict collision rates.\nThe puzzle: Some roads experience frequent wildlife collisions while others have zero incidents. Is it just random chance or collection bias? Or are there systematic patterns we can identify and address?\nIf we can predict which roads are most likely to become collision hotspots, transportation planners and conservationists can install wildlife crossings, implement warning systems, or adjust speed limits in high-risk areas.\n\n\nHow do traffic volume and land use characteristics affect wildlife-vehicle collisions on Danish roads?\nSpecifically, we investigate two processes:\n\nOccurrence: What makes roadkill more likely to happen at all on a given road segment?\nIntensity: Once roadkill does occur, what factors influence how many collision events happen?\n\nThe high proportion of road segments with zero observed roadkill (~83%) suggests these processes operate differently, making a hurdle model the appropriate statistical framework."
  },
  {
    "objectID": "posts/eds222-final/index.html#references",
    "href": "posts/eds222-final/index.html#references",
    "title": "Traffic, Land Use, and Wildlife-Vehicle Collisions in Denmark",
    "section": "References",
    "text": "References\n\n\nDanish Road Directorate. 2017--2019. ‚ÄúTraffic Counts ‚Äì Key Figures (MASTRA).‚Äù https://www.opendata.dk/vejdirektoratet/taellinger-nogletal-mastra.\n\n\nGrilo, Clara, Tom√© Neves, Jennifer Bates, Aliza le Roux, Pablo Medrano-Vizca√≠no, Mattia Quaranta, In√™s Silva, et al. 2025. ‚ÄúGlobal Roadkill Data: A Dataset on Terrestrial Vertebrate Mortality Caused by Collision with Vehicles.‚Äù figshare. https://doi.org/10.6084/M9.FIGSHARE.25714233.\n\n\nOpenStreetMap contributors. 2024. ‚ÄúPlanet Dump Retrieved from Https://Planet.openstreetmap.org.‚Äù https://www.openstreetmap.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emily Miller",
    "section": "",
    "text": "Data scientist, environmental systems thinker, and compulsive fixer of things.\n\n\n\n\n\n\n\n‚ÄúWithout data, you‚Äôre just another person with an opinion.‚Äù\n‚Äî W. Edwards Deming\n\n\n‚ÄúAll models are wrong, but some are useful.‚Äù\n‚Äî George E. P. Box\n\n\n‚ÄúI have no fear of losing my life‚Äîif I have to save a koala or a crocodile or a kangaroo or a snake, mate, I will save it.‚Äù\n‚Äî Steve Irwin\n\n\n‚ÄúAn expert is a person who has made all the mistakes that can be made in a very narrow field.‚Äù\n‚Äî Niels Bohr\n\n\n‚ÄúThe cure for boredom is curiosity. There is no cure for curiosity.‚Äù\n‚Äî Dorothy Parker\n\n\n‚ÄúIf you don‚Äôt know where you are going, any road will get you there.‚Äù\n‚Äî Lewis Carroll\n\n\n‚ÄúI would give my right arm for the simplicity on the far side of complexity.‚Äù\n‚Äî Oliver Wendell Holmes\n\n\n‚ÄúThe climate has always changed. The difference now is that we are the cause.‚Äù\n‚Äî Kate Marvel, Climate Scientist\n\n\n‚ÄúWhat you do makes a difference. You have to decide what kind of difference you want to make.‚Äù\n‚Äî Jane Goodall\n\n\n‚ÄúIt‚Äôs not what you look at that matters, it‚Äôs what you see.‚Äù\n‚Äî Henry David Thoreau\n\n\n‚ÄúLet yourself be silently drawn by the strange pull of what you really love.‚Äù\n‚Äî Rumi\n\n\n‚ÄúTell me, what is it you plan to do with your one wild and precious life?‚Äù\n‚Äî Mary Oliver, Wild Geese\n\n\n‚ÄúNothing in life is to be feared, it is only to be understood.‚Äù\n‚Äî Marie Curie\n\n\n‚ÄúA ship in port is safe, but that‚Äôs not what ships are built for.‚Äù\n‚Äî Grace Hopper\n\n\n‚ÄúChange happens by listening and then starting a dialogue with the people who are doing something you don‚Äôt believe is right.‚Äù\n‚Äî Jane Goodall\n\n\n‚ÄúStudy as if you were going to live forever; live as if you were going to die tomorrow.‚Äù\n‚Äî Maria Mitchell\n\n\n‚ÄúWe are drowning in information, while starving for wisdom. The world henceforth will be run by synthesizers, people able to put together the right information at the right time, think critically about it, and make important choices wisely.‚Äù\n‚Äî E. O. Wilson\n\n\n‚ÄúIf you don‚Äôt know where you are going, any road will get you there.‚Äù\n‚Äî Lewis Carroll\n\n\n‚ÄúIt‚Äôs easy to lie with statistics. It‚Äôs hard to tell the truth without statistics.‚Äù\n‚Äî Andrejs Dunkels\n\n\n‚ÄúIf you can‚Äôt explain it simply, you don‚Äôt understand it well enough.‚Äù\n‚Äî Albert Einstein\n\n\n‚ÄúWhen a measure becomes a target, it ceases to be a good measure‚Äù\n‚Äî Charles Goodhart\n\n\n‚ÄúIt is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out.‚Äù\n‚Äî Emil Artin\n\n\n\n\n\n\nWhat I‚Äôm Working On\nResearch ‚Üí | Art & Poetry ‚Üí | Blog ‚Üí | More About Me ‚Üí"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts\n\n\n\ngeospatial analysis\n\nremote sensing\n\nenvironmental justice\n\nR\n\n\n\nUsing satellite night lights data and socioeconomic indicators to identify which Houston communities lost power during the historic February 2021 winter storms\n\n\n\n\n\nDec 3, 2025\n\n\nEmily Miller\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice\n\n\n\nMEDS\n\nPython\n\nGeospatial Analysis\n\nEnvironmental Justice\n\n\n\nCombining satellite imagery and socioeconomic data to understand wildfire impacts\n\n\n\n\n\nNov 29, 2025\n\n\nEmily Miller\n\n\n\n\n\n\n\n\n\n\n\n\nTraffic, Land Use, and Wildlife-Vehicle Collisions in Denmark\n\n\n\nMEDS\n\nR\n\nStatistics\n\nWildlife\n\n\n\nUsing a hurdle model to understand roadkill patterns on Danish roads\n\n\n\n\n\nNov 29, 2025\n\n\nEmily Miller\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fog the Models Missed\n\n\n\nMEDS\n\nClimate Science\n\nData Science\n\nEthics\n\n\n\nHow climate model biases in California‚Äôs marine layer contributed to the 2020 blackouts\n\n\n\n\n\nDec 6, 2024\n\n\nEmily Miller\n\n\n\n\n\n\n\n\n\n\n\n\nPrioritizing Marine Aquaculture Locations on the US West Coast\n\n\n\nMEDS\n\nR\n\nGeospatial Analysis\n\nMarine Science\n\n\n\nUsing geospatial analysis to identify optimal locations for oyster and abalone farming based on sea surface temperature and depth criteria\n\n\n\n\n\nNov 15, 2024\n\n\nEmily Miller\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "adventures.html",
    "href": "adventures.html",
    "title": "Adventures",
    "section": "",
    "text": "Life happens between the data points"
  },
  {
    "objectID": "adventures.html#what-this-is-all-about",
    "href": "adventures.html#what-this-is-all-about",
    "title": "Adventures",
    "section": "What This Is All About",
    "text": "What This Is All About\nWelcome to the moments that live between the data points and outside the lab‚Äîthe beautiful chaos, unexpected discoveries, and genuinely ridiculous moments that make up a life worth living. This is where I share the adventures, the daily wonder, and the goofy things that happen when you‚Äôre paying attention to the world around you.\n\n‚ÄúTell me, what is it you plan to do with your one wild and precious life?‚Äù ‚Äî Mary Oliver"
  },
  {
    "objectID": "adventures.html#dog",
    "href": "adventures.html#dog",
    "title": "Adventures",
    "section": "Dog",
    "text": "Dog\nMeet the love of my life\n\n\n\n\nSeaweed dog\n\n\n\nThe Fragrance Connoisseur\nBeach day ‚Ä¢ 2024\nShe will find the most spectacularly stinky kelp and roll in it with the dedication of a perfume critic. Ten out of ten, would stink again.\n\n\n\n\nIV Recovery Mascot (and insatiable composter)\n\n\n\nEnvironmental Activist\nBeach cleanup ‚Ä¢ 2024\nShe may not understand why we‚Äôre cleaning, but she‚Äôs definitely helping‚Äîand enjoying every second of being at the beach with her people.\n\n\n\nResearch Companion\nUCSB Research Lab ‚Ä¢ 2024\nWhen your lab assistant has four legs, unlimited enthusiasm, and absolutely no interest in the data you‚Äôre analyzing."
  },
  {
    "objectID": "adventures.html#beach-life",
    "href": "adventures.html#beach-life",
    "title": "Adventures",
    "section": "üåä Beach Life",
    "text": "üåä Beach Life\nLiving near the ocean saves my life on the daily. The salt water is where ideas come alive, where I remember what matters, and where the best conversations happen.\n\n\n\n\nBeach cleanup volunteers\n\n\n\nCommunity Care\nBeach cleanup ‚Ä¢ 2024\nWhen strangers become stewards, and an afternoon at the beach becomes an afternoon caring for home.\n\n\n\n\nMoments with mother\n\n\n\nGolden Hour Wisdom\nEvening beach walk ‚Ä¢ 2024\nThe best conversations happen with your feet in the sand and the sky turning colors you can‚Äôt quite name.\n\n\n\n\nBeach art\n\n\n\nAccidental Creation\nLow tide creativity ‚Ä¢ 2024\nArt happens when we‚Äôre not trying. The ocean has its way of reminding us that creation is everywhere‚Äîwe just have to show up and play."
  },
  {
    "objectID": "adventures.html#random-moments",
    "href": "adventures.html#random-moments",
    "title": "Adventures",
    "section": "ü§™ Random Moments",
    "text": "ü§™ Random Moments\nThe completely unexpected, wonderfully weird moments that make life interesting and definitely worth documenting.\n\nComing Soon‚Ä¶\nBuilding a collection of the ridiculous, the beautiful, and the completely unexpected moments worth remembering."
  },
  {
    "objectID": "adventures.html#travel",
    "href": "adventures.html#travel",
    "title": "Adventures",
    "section": "üåç Travel",
    "text": "üåç Travel\n\nComing Soon‚Ä¶"
  },
  {
    "objectID": "about.html#the-real-stuff",
    "href": "about.html#the-real-stuff",
    "title": "About Me",
    "section": "The Real Stuff",
    "text": "The Real Stuff\nI‚Äôm someone who genuinely can‚Äôt sit still. If something‚Äôs broken and there‚Äôs even a slim chance I can fix it, I will learn how. I‚Äôll weld, solder, sew, paint‚Äîwhatever it takes. Debugging code? That‚Äôs a game to me, not a chore. I‚Äôve been incredibly fortunate to have a service dog, which has changed my life in ways I‚Äôm still understanding.\nI grew up between Concord, Massachusetts and the Central Valley of California, but my heart belongs to Great Diamond Island off the coast of Portland, Maine. There‚Äôs something about that place. I sang jazz for 11 years and spent significant time in mental hospitals‚Äîboth things I think more people should be honest about. I‚Äôm very dyslexic and ADHD, which means my brain works sideways. I don‚Äôt see it as a bug; I see it as the reason I can see patterns other people miss.\nI don‚Äôt have a therapist. I have a life coach, and honestly, everyone should make that transition."
  },
  {
    "objectID": "about.html#what-i-actually-do-with-my-time",
    "href": "about.html#what-i-actually-do-with-my-time",
    "title": "About Me",
    "section": "What I Actually Do With My Time",
    "text": "What I Actually Do With My Time\nI garden. I bake. I paint. I photograph things. I listen to audiobooks obsessively. I rewatch TV shows. I weld things. I sew things. I play piano badly. I write poetry about satellite imagery, which is a weirdly specific thing but it‚Äôs true. I‚Äôm genuinely curious about everything, which is both a superpower and absolutely exhausting.\nMy issue is I can never settle on just one thing. I‚Äôll go deep on something, get really good at it, then find something new and dive in headfirst. I‚Äôve learned to stop seeing this as a failure and start seeing it as just‚Ä¶ how my brain works. Everything I learn somehow feeds back into my actual passions anyway‚Äîthe work and the wild."
  },
  {
    "objectID": "about.html#why-i-do-anything",
    "href": "about.html#why-i-do-anything",
    "title": "About Me",
    "section": "Why I Do Anything",
    "text": "Why I Do Anything\nThe wild. That‚Äôs the real north star. Everything else orbits that. Conservation, restoration, understanding how humans and ecosystems actually talk to each other‚Äîthat‚Äôs where I‚Äôm headed. It‚Äôs not about being ‚Äúgreen‚Äù or performing environmentalism. It‚Äôs about the genuine belief that nature is both the most complex system we need to understand and the best teacher we have.\nI also believe that creativity and precision aren‚Äôt opposites‚Äîthey‚Äôre dance partners. You need both to do anything worth doing."
  },
  {
    "objectID": "about.html#random-facts",
    "href": "about.html#random-facts",
    "title": "About Me",
    "section": "Random Facts",
    "text": "Random Facts\n\nI can do anything (and if I can‚Äôt I will humiliate myself trying until I can).\nI‚Äôm genuinely dyslexic, which means I have to read everything three times. Yes, I read everything three times.\nI‚Äôve done a lot of jazz singing. A lot.\nI love being challenged. I love debugging. I love learning impossible things.\nI‚Äôm probably listening to an audiobook right now.\nMy service dog is the best decision I‚Äôve ever made."
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect",
    "text": "Let‚Äôs Connect\nI love talking about weird intersections of things‚Äîhow data can tell stories, why nature is smarter than us, whether debugging is actually meditation, etc. If you want to chat about any of that (or just say hi), reach out.\nEmail Me | LinkedIn | GitHub\nMy academic journey began with a focus on pure mathematics, drawn to the elegance and precision of numerical analysis. My goal from the start was to use my degree to bring new levels of quantitative and analytical rigor to the science of understanding our world and place on this earth as a species.\n\nMaster of Environmental Data Science\nUniversity of California, Santa Barbara ‚Äì Bren School | Expected June 2026\nGPA: 4.0 ‚Ä¢ Leadership: MEDS Student Faculty Representative\nFocus: Remote sensing, machine learning, and environmental data engineering. Currently working on water resource sustainability and irrigation systems in Sub-Saharan Africa through the Water Vegetation and Society (WaVeS) Lab.\n\n\nBS in Mathematics\nUniversity of California, Santa Barbara | June 2025\nGPA: 3.67 ‚Ä¢ Leadership: Undergraduate Outreach Officer for Women in Science and Engineering (WiSE)\nFocus: Applied mathematics with emphasis on numerical analysis, linear algebra, and probability. Developed passion for using mathematical tools to understand environmental systems and data patterns.\n\n\nAssociate of Science in Mathematics\nSacramento City College | June 2022\nGPA: 4.0"
  },
  {
    "objectID": "about.html#research-focus",
    "href": "about.html#research-focus",
    "title": "About Me",
    "section": "Research Focus",
    "text": "Research Focus\nMy current research sits at the intersection of remote sensing, machine learning, and water resource sustainability. I‚Äôm working with the Water Vegetation and Society (WaVeS) Lab at UCSB on understanding irrigation patterns in Sub-Saharan Africa using satellite data analysis. I‚Äôm particularly interested in how cutting-edge technology can illuminate patterns that affect environmental and agricultural systems, and how this data can support better decision-making for communities and ecosystems.\n\nBiomimicry\nLearning from nature is central to both my scientific outlook and personal worldview. I believe that true solutions come from respecting and integrating with the natural systems all around us. Biomimicry, to me, is about seeking guidance from nature‚Äôs intelligence‚Äîstudying the processes and resilience of living systems, and exploring how these principles can inspire our own designs, behaviors, and mindsets. I strive to align my research and daily life with these lessons, drawing inspiration from the practices and wisdom of indigenous communities who have always lived in harmony with their environments. Biomimicry isn‚Äôt just an innovation strategy; it‚Äôs a path to personal, mental, and spiritual well-being by reconnecting with the natural world."
  },
  {
    "objectID": "about.html#creative-practice",
    "href": "about.html#creative-practice",
    "title": "About Me",
    "section": "Creative Practice",
    "text": "Creative Practice\nI believe that creativity and science amplify each other; discovery happens when boundaries are blurred and new perspectives collide. Whether I‚Äôm analyzing satellite data or experimenting with a blank canvas, I find that both art and science challenge me to notice subtle details, think in systems, and communicate in ways that spark curiosity and connection. When I‚Äôm not creating digitally, you‚Äôll find me with paint on my clothes, camera in hand, or scribbling poetry about the weird beauty of real-world data, landscapes, and life at the edge of art and science."
  },
  {
    "objectID": "about.html#community-involvement",
    "href": "about.html#community-involvement",
    "title": "About Me",
    "section": "Community Involvement",
    "text": "Community Involvement\nBeyond the lab, I‚Äôm passionate about using my skills to make a positive impact in my local community and beyond.\n\nFounder & Lead Organizer ‚Äì IV Recovery Community Initiative\nGoleta, CA | 2023 - Present\nI founded and lead the IV Recovery Community Initiative, organizing bi-weekly coastal cleanups that engage university students and community members in environmental stewardship. What started as a personal passion has grown into something beautiful: seeing people show up, roll up their sleeves, and realize they can make a tangible difference. By quantifying what we collect‚Äîfrom microplastics to fishing line to abandoned equipment‚Äîwe make the invisible visible and build community responsibility for the land we occupy.\n\n\nUndergraduate Outreach Officer ‚Äì Women in Science and Engineering (WiSE)\nUniversity of California, Santa Barbara | 2024 - 2025\nCoordinated mentorship programs supporting 130+ undergraduate students pursuing STEM degrees, with focus on increasing diversity and representation in quantitative sciences and supporting retention of underrepresented students."
  },
  {
    "objectID": "about.html#values-mission",
    "href": "about.html#values-mission",
    "title": "About Me",
    "section": "Values & Mission",
    "text": "Values & Mission\nMy work is guided by a few core values that I believe are essential for creating meaningful impact through data science:\n\nEnvironmental Justice\nAll communities deserve access to clean environments and the tools to protect them. My research prioritizes supporting communities that are often overlooked by traditional environmental monitoring.\n\n\nCommunity-Centered\nThe best research happens in partnership with the communities it‚Äôs meant to serve. I prioritize collaborative approaches that respect local knowledge and ensure community benefit.\n\n\nCreative Innovation\nSome of the best solutions come from unexpected connections between seemingly unrelated ideas. Whether I‚Äôm analyzing satellite data or painting, I bring the same habit of noticing patterns and asking ‚Äúwhat if?‚Äù I believe that technical rigor and creative thinking amplify each other‚Äîscience needs imagination, and art needs precision. The breakthrough ideas happen at the intersection.\n\n\nTransparent Science\nScience should be open, reproducible, and accessible. I‚Äôm committed to transparent research practices and clear communication of complex concepts."
  },
  {
    "objectID": "about.html#whats-next",
    "href": "about.html#whats-next",
    "title": "About Me",
    "section": "What‚Äôs Next",
    "text": "What‚Äôs Next\nMy post-MEDS journey will be guided by the problems that excite me and the values that drive my work. I‚Äôm pursuing opportunities where I can combine cutting-edge technology with real environmental and social impact. Some of the questions that keep me awake at night:\n\nHow can satellite imagery document environmental harm and support indigenous land rights? Using remote sensing as evidence for climate impacts, illegal activities, and the benefits of indigenous management practices.\nWhat does wildlife need to thrive? Analyzing wildlife corridors, the impact of infrastructure, and using animal behavior to inform conservation design.\nHow can we make environmental data work harder for people who need it? Building live, accessible data systems‚Äînot outdated snapshots‚Äîthat help water managers, farmers, and communities make better decisions.\nWhat can nature teach us? Biomimicry isn‚Äôt just buzzword‚Äîit‚Äôs about genuinely learning from millions of years of evolutionary solutions: sustainable adhesion from mussels, effective ventilation from termites, wind turbines inspired by humpback whale fins.\nHow does our disconnection from nature affect our mental and emotional health? Exploring the links between environmental deprivation and well-being, and designing solutions that reconnect people to the natural world.\n\nI‚Äôm looking for roles that let me keep learning, try new things, stay near the ocean, and tackle problems that matter. Whether that‚Äôs working with NGOs, research institutions, government agencies, or social enterprises, I‚Äôm drawn to teams doing work that feels urgent and true."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\nWhether you‚Äôre interested in collaborating on research, want to learn more about environmental data science, or just want to chat, I‚Äôd love to hear from you!\nEmail Me | LinkedIn | GitHub"
  },
  {
    "objectID": "art.html",
    "href": "art.html",
    "title": "Art & Poetry",
    "section": "",
    "text": "Where data meets creativity, and the messy, beautiful human experience becomes art"
  },
  {
    "objectID": "art.html#on-making-art",
    "href": "art.html#on-making-art",
    "title": "Art & Poetry",
    "section": "On Making Art",
    "text": "On Making Art\nI believe that art and science aren‚Äôt opposites‚Äîthey‚Äôre different languages for the same truth. Whether I‚Äôm analyzing satellite imagery or standing in front of a blank canvas, I‚Äôm asking the same questions: What do I see? What matters? How do I make others see it too?\nWhen creativity strikes, it doesn‚Äôt wait for the perfect time. So I paint, I write, I take photos. Sometimes the work is polished. Often it‚Äôs raw, unfinished, bleeding into the margins of notebooks. All of it is real."
  },
  {
    "objectID": "art.html#visual-art",
    "href": "art.html#visual-art",
    "title": "Art & Poetry",
    "section": "Visual Art",
    "text": "Visual Art\nPaintings, drawings, and experiments at the intersection of observation and intuition. Coming soon‚Ä¶"
  },
  {
    "objectID": "art.html#photography",
    "href": "art.html#photography",
    "title": "Art & Poetry",
    "section": "Photography",
    "text": "Photography\nCapturing light, moments, and the strange beauty hiding in plain sight. Coming soon‚Ä¶"
  },
  {
    "objectID": "art.html#poetry",
    "href": "art.html#poetry",
    "title": "Art & Poetry",
    "section": "Poetry",
    "text": "Poetry\nWords that hold what data can‚Äôt measure. The messy, true stuff.\n\nA Bad Design\nMy heels leak,\nWith each step, grating, chafing,\nCould this be by design?\nWe‚Äôve only been together a few days,\nPerhaps it is my fault,\nNot taking the time to offer support,\nBefore slamming, stomping into you.\nWho would design such a thing?\nNo one else would want you in this state,\nTo suggest so feels like a setup, a hoax.\nPerhaps a hairdryer and a rigid form,\nMaybe some duct tape,\nA pair of pliers, some wire cutters,\nTo surgically remove the parts of you that rub me the wrong way.\nTo no avail, and now my foolish faith in you has brought me far from home.\nFar, and I think I might be bleeding‚Äî\nI will carry you the rest of the way.\nSoon the limp of uneven stilts,\nWill send a pang through my hips, a warning.\nSocks on searing sidewalks,\nCatching on stale gum,\nCollecting stories laid by treads,\nLeft by soles not soaked in scarlet,\nThe better of your ilk.\nFor you, I‚Äôve sacrificed my heels,\nMy toes harbor liquid cushions,\nFeet gone flat,\nMy balls slowly scorched.\nBut I still feel for you;\nBetrayer.\nI know, if it were up to you, you wouldn‚Äôt have picked this.\nYou remind me of me.\nBrandishing your name as bait,\nLuring investments in na√Øve idolatry.\nAspiring beyond clearance rack compromises\nSeduced by a gilded construction\nI championed your promise.\nTo discard you now\nWould be to admit defeat\nTo waste not only money,\nBut value of conviction.\nSo you‚Äôll live in my closet,\nNext to the others,\nUntil I come up with a way to fix you‚Äî\nTrading band aids for false hope once again‚Äî\nOr I find the heart to throw you out.\n\n\n\nThe Ocean Listened\nI cried to the ocean today\nShe listened deeper than I spoke,\nand she told me she didn‚Äôt care.\nSo I asked the birds,\nbut they were too busy\nidling against the ocean breeze\nI asked the grasses,\nthey stopped to listen\nand went back to sharing quiet whispers with the trees.\nThen I asked the wind,\nand she swept up my tears faster than they could fall.\nI have never felt more loved."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Emily Miller",
    "section": "",
    "text": "Phone: (978) 265-9348\nEmail: ermiller@ucsb.edu\nWebsite: rellimylime.github.io\nLinkedIn: linkedin.com/in/rellimylime\nGitHub: github.com/rellimylime\nLocation: Santa Barbara, CA"
  },
  {
    "objectID": "cv.html#contact-information",
    "href": "cv.html#contact-information",
    "title": "Emily Miller",
    "section": "",
    "text": "Phone: (978) 265-9348\nEmail: ermiller@ucsb.edu\nWebsite: rellimylime.github.io\nLinkedIn: linkedin.com/in/rellimylime\nGitHub: github.com/rellimylime\nLocation: Santa Barbara, CA"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Emily Miller",
    "section": "Education",
    "text": "Education\n\nMaster of Environmental Data Science\nUniversity of California, Santa Barbara ‚Äì Bren School of Environmental Science & Management | Expected June 2026\nGPA: 4.0 | Leadership: MEDS Student Faculty Representative\nHighlighted Coursework: Machine Learning in Environmental Science, Geospatial Analysis and Remote Sensing, Environmental Systems Modeling, Environmental Datasets and Data Engineering, Databases and Data Management, Statistics for Environmental Data Science\n\n\nBachelor of Science in Mathematics\nUniversity of California, Santa Barbara | June 2025\nGPA: 3.67 | Leadership: Undergraduate Outreach Officer for Women in Science and Engineering (WiSE)\nHighlighted Coursework: Real Analysis, Linear Algebra, Numerical Analysis, Computer Science and Problem Solving, Probability and Statistics\n\n\nAssociate of Science in Mathematics\nSacramento City College | June 2022\nGPA: 4.0"
  },
  {
    "objectID": "cv.html#research-data-analysis-experience",
    "href": "cv.html#research-data-analysis-experience",
    "title": "Emily Miller",
    "section": "Research & Data Analysis Experience",
    "text": "Research & Data Analysis Experience\n\nResearch Assistant ‚Äì Water Vegetation and Society (WaVeS) Lab\nUniversity of California Santa Barbara, Goleta, CA | June 2024 - Present\n\nApplied statistical and numerical methods to analyze and validate agricultural water and land use datasets across multiple spatial scales\nDeveloped Python codebase for processing and analyzing satellite imagery and remotely sensed data to identify irrigation patterns in Sub-Saharan Africa\nCreated statistical frameworks and visualization methods for examining complex spatial relationships in agricultural systems\nCollaborated with faculty mentor (Prof.¬†Kelly Caylor) on water resource sustainability research\n\n\n\nBEL Fellow ‚Äì Bren Environmental Leadership Program\nUniversity of California Santa Barbara, Goleta, CA | June 2024 - December 2024\n\nConducted research on irrigation patterns in Sub-Saharan Africa using satellite data analysis and remote sensing techniques\nPresented research findings ‚ÄúSatellite Data Reveal Emerging Decentralized Irrigation Systems in Sub-Saharan Africa‚Äù at three academic venues including AGU Fall Meeting\nManaged project timeline and deliverables while collaborating with faculty mentor on water resource sustainability research"
  },
  {
    "objectID": "cv.html#leadership-community-engagement-experience",
    "href": "cv.html#leadership-community-engagement-experience",
    "title": "Emily Miller",
    "section": "Leadership & Community Engagement Experience",
    "text": "Leadership & Community Engagement Experience\n\nFounder & Lead Organizer ‚Äì IV Recovery Community Initiative\nGoleta, CA | 2023 - Present\n\nFounded and lead initiative organizing bi-weekly coastal cleanups engaging university students and community members\nDevelop community engagement strategies to increase environmental stewardship and normalize university participation in local conservation\nQuantify environmental impact through data collection and analysis\n\n\n\nUndergraduate Outreach Officer ‚Äì Women in Science and Engineering (WiSE)\nUniversity of California Santa Barbara, Goleta, CA | September 2024 - June 2025\n\nCoordinated mentorship program supporting 130+ undergraduate students pursuing STEM degrees\nFocused outreach efforts on increasing diversity and representation in quantitative sciences through targeted programming\nDeveloped and implemented engagement strategies to support retention of underrepresented students in STEM fields"
  },
  {
    "objectID": "cv.html#teaching-mentoring-experience",
    "href": "cv.html#teaching-mentoring-experience",
    "title": "Emily Miller",
    "section": "Teaching & Mentoring Experience",
    "text": "Teaching & Mentoring Experience\n\nMathematics Tutor\nDavis, CA | 2020 - 2023\n\nTaught mathematics to students ranging from elementary through college level, adapting instruction to diverse learning needs\nProvided comprehensive GED preparation support across all subject areas, resulting in student success on examinations\nDeveloped individualized learning plans to accommodate different learning styles and optimize student comprehension"
  },
  {
    "objectID": "cv.html#presentations-talks",
    "href": "cv.html#presentations-talks",
    "title": "Emily Miller",
    "section": "Presentations & Talks",
    "text": "Presentations & Talks\nMiller E, Boser A, Caylor K. ‚ÄúSatellite Data Reveal Emerging Decentralized Irrigation Systems in Sub-Saharan Africa.‚Äù Summer@Bren 2024 Flash Talks, UCSB, CA. August 29, 2024.\nMiller E, Boser A, Caylor K. ‚ÄúWater Source Attribution for Center Pivot Irrigation in Sub-Saharan Africa.‚Äù Mantell Symposium in Environmental Justice and Conservation Innovation, UCSB, CA. October 24, 2024.\nBoser A, Miller E, Perez J, Caylor K. ‚ÄúAnalyzing the sustainability and climate resilience of rapidly expanding center pivot Irrigation in Sub-Saharan Africa using remote sensing.‚Äù American Geophysical Union Fall Meeting, Washington D.C. December 9-13, 2024."
  },
  {
    "objectID": "cv.html#technical-skills",
    "href": "cv.html#technical-skills",
    "title": "Emily Miller",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nProgramming & Data Analysis\nPython, R, C++\n\n\nTechnical Expertise\n\nSatellite imagery analysis and remote sensing\nStatistical analysis and numerical methods\nMachine learning and data visualization\nError analysis and computational methods\nSpatial data processing\n\n\n\nMathematical Foundation\n\nReal analysis and linear algebra\nProbability and statistics\nNumerical analysis\nMatrix operations and eigenvalue calculations\nData structures and algorithms\n\n\n\nTools & Software\n\nGoogle Earth Engine, QGIS, ArcGIS\nPython libraries: pandas, numpy, scikit-learn, matplotlib, seaborn\nR and tidyverse packages\nDatabase management and SQL\nVersion control with Git"
  },
  {
    "objectID": "cv.html#research-interests-career-focus",
    "href": "cv.html#research-interests-career-focus",
    "title": "Emily Miller",
    "section": "Research Interests & Career Focus",
    "text": "Research Interests & Career Focus\nCurrent Focus: Remote sensing and machine learning applications for understanding agricultural water systems, environmental sustainability, and climate resilience in Sub-Saharan Africa\nBroader Interests: - Using satellite imagery to support environmental justice and marginalized communities - Wildlife corridor connectivity and infrastructure impact analysis - Indigenous land management practices and conservation outcomes - Live, accessible data systems for environmental decision-making - Nature-inspired solutions and biomimicry - Community-centered approaches to environmental research and monitoring"
  },
  {
    "objectID": "posts/eds220-final/index.html",
    "href": "posts/eds220-final/index.html",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "",
    "text": "In January 2025, the Eaton and Palisades fires swept through Los Angeles County, leaving lasting impacts on both the landscape and the communities within their paths. The Palisades Fire, which started on January 7, became one of the most destructive wildfires in California history, burning through the scenic Pacific Palisades neighborhood and nearby communities. Meanwhile, the Eaton Fire ignited in the foothills near Altadena and Pasadena, threatening densely populated areas and critical infrastructure.\nUnderstanding wildfire impacts requires more than just mapping burned areas‚Äîit requires examining both the physical changes to the landscape and the social dimensions of who is affected. This analysis combines remote sensing techniques with environmental justice data to provide a comprehensive view of these devastating fires.\nRepository: https://github.com/rellimylime/eaton-palisades-fires-analysis For complete analysis notebooks with additional data exploration and detailed outputs, visit the GitHub repository.\n\n\n\nSource: https://yaleclimateconnections.org/2025/01/the-role-of-climate-change-in-the-catastrophic-2025-los-angeles-fires/"
  },
  {
    "objectID": "posts/eds220-final/index.html#about",
    "href": "posts/eds220-final/index.html#about",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "",
    "text": "In January 2025, the Eaton and Palisades fires swept through Los Angeles County, leaving lasting impacts on both the landscape and the communities within their paths. The Palisades Fire, which started on January 7, became one of the most destructive wildfires in California history, burning through the scenic Pacific Palisades neighborhood and nearby communities. Meanwhile, the Eaton Fire ignited in the foothills near Altadena and Pasadena, threatening densely populated areas and critical infrastructure.\nUnderstanding wildfire impacts requires more than just mapping burned areas‚Äîit requires examining both the physical changes to the landscape and the social dimensions of who is affected. This analysis combines remote sensing techniques with environmental justice data to provide a comprehensive view of these devastating fires.\nRepository: https://github.com/rellimylime/eaton-palisades-fires-analysis For complete analysis notebooks with additional data exploration and detailed outputs, visit the GitHub repository.\n\n\n\nSource: https://yaleclimateconnections.org/2025/01/the-role-of-climate-change-in-the-catastrophic-2025-los-angeles-fires/"
  },
  {
    "objectID": "posts/eds220-final/index.html#highlights",
    "href": "posts/eds220-final/index.html#highlights",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Highlights",
    "text": "Highlights\nThis analysis demonstrates several key Python-based geospatial analysis techniques:\n\nFalse color visualization using xarray and rioxarray to process Landsat 8 SWIR/NIR/Red bands for burn scar identification\nMulti-dataset integration with geopandas combining satellite raster imagery with vector fire perimeter and census tract data\nCRS management and reprojection using rio.write_crs() and to_crs() to ensure spatial alignment across different data sources\nEnvironmental justice analysis with spatial joins (gpd.sjoin()) and clipping to examine socioeconomic vulnerability in fire-affected areas"
  },
  {
    "objectID": "posts/eds220-final/index.html#data",
    "href": "posts/eds220-final/index.html#data",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Data",
    "text": "Data\n\nLandsat 8 Surface Reflectance Data\nThis dataset contains atmospherically corrected surface reflectance data from the Landsat 8 satellite, captured on February 23, 2025 (Microsoft Planetary Computer 2025). The imagery includes five spectral bands (red, green, blue, near-infrared, and shortwave infrared) clipped to the area surrounding the Eaton and Palisades fire perimeters. The data was retrieved from the Microsoft Planetary Computer STAC catalog in NetCDF format.\n\n\nFire Perimeter Shapefiles\nThese shapefiles delineate the official fire perimeters for the Eaton and Palisades fires as of January 21, 2025 (ArcGIS Hub 2025). The vector data includes boundary geometries and attribute information for each fire.\n\n\nEnvironmental Justice Index (EJI) Data\nThe CDC/ATSDR Environmental Justice Index (Centers for Disease Control and Prevention and Agency for Toxic Substances and Disease Registry 2024) provides census tract-level data on environmental and socioeconomic factors. This analysis uses the 2024 California EJI dataset, which includes demographic information, poverty statistics, and environmental burden indicators for communities across the state."
  },
  {
    "objectID": "posts/eds220-final/index.html#analysis-1-false-color-visualization",
    "href": "posts/eds220-final/index.html#analysis-1-false-color-visualization",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Analysis 1: False Color Visualization",
    "text": "Analysis 1: False Color Visualization\n\nSetup and Data Import\n\n\nCode\n# Import required libraries for geospatial analysis and visualization\nimport xarray as xr\nimport rioxarray as rio\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nLoading Fire Perimeter Data\nFire perimeters provide the spatial boundaries for our analysis, allowing us to focus on the areas most affected by the fires.\n\n\nCode\n# Load fire perimeter shapefiles\neaton = gpd.read_file('data/Eaton_Perimeter_20250121.shp')\npalisades = gpd.read_file('data/Palisades_Perimeter_20250121.shp')\n\n# Create summary table for data exploration\nsummary_data = {\n    'Dataset': ['Eaton Fire', 'Palisades Fire'],\n    'Features': [len(eaton), len(palisades)],\n    'Columns': [len(eaton.columns), len(palisades.columns)],\n    'Geometry Type': [eaton.geometry.type.unique()[0], palisades.geometry.type.unique()[0]]\n}\n\nsummary_df = pd.DataFrame(summary_data)\n\n# Display as formatted table\nsummary_df\n\n\n\n\n\n\n\n\n\nDataset\nFeatures\nColumns\nGeometry Type\n\n\n\n\n0\nEaton Fire\n20\n5\nPolygon\n\n\n1\nPalisades Fire\n21\n5\nPolygon\n\n\n\n\n\n\n\nBoth datasets contain polygon geometries representing the fire boundaries, along with area and perimeter measurements.\n\nCoordinate Reference System Check\n\n\nCode\n# Create CRS information table\ncrs_data = {\n    'Dataset': ['Eaton Fire', 'Palisades Fire'],\n    'CRS Code': [eaton.crs.to_authority()[1] if eaton.crs.to_authority() else 'N/A',\n                 palisades.crs.to_authority()[1] if palisades.crs.to_authority() else 'N/A'],\n    'Projection Type': ['Geographic' if eaton.crs.is_geographic else 'Projected',\n                       'Geographic' if palisades.crs.is_geographic else 'Projected']\n}\n\ncrs_df = pd.DataFrame(crs_data)\n\n# Display as formatted table\ncrs_df\n\n\n\n\n\n\n\n\n\nDataset\nCRS Code\nProjection Type\n\n\n\n\n0\nEaton Fire\n3857\nProjected\n\n\n1\nPalisades Fire\n3857\nProjected\n\n\n\n\n\n\n\nBoth datasets use EPSG:3857 (Web Mercator), a projected coordinate system suitable for web mapping applications.\n\n\n\nLoading and Processing Landsat Imagery\nThe Landsat 8 scene contains multiple spectral bands that capture different wavelengths of reflected light. Each band provides unique information about surface features.\n\n\nCode\n# Import Landsat data\nlandsat = xr.open_dataset('data/landsat8-2025-02-23-palisades-eaton.nc')\n\n# Create Landsat dataset summary table\nlandsat_summary = {\n    'Property': ['Dimensions', 'Height (y)', 'Width (x)', 'Coordinates', 'Data Variables'],\n    'Value': [\n        f\"{len(landsat.dims)}D\",\n        f\"{landsat.dims['y']} pixels\",\n        f\"{landsat.dims['x']} pixels\",\n        ', '.join(list(landsat.coords)),\n        ', '.join([v for v in landsat.data_vars if v != 'spatial_ref'])\n    ]\n}\n\nlandsat_df = pd.DataFrame(landsat_summary)\n\n# Display as formatted table\nlandsat_df\n\n\n\n\n\n\n\n\n\nProperty\nValue\n\n\n\n\n0\nDimensions\n2D\n\n\n1\nHeight (y)\n1418 pixels\n\n\n2\nWidth (x)\n2742 pixels\n\n\n3\nCoordinates\ny, x, time\n\n\n4\nData Variables\nred, green, blue, nir08, swir22\n\n\n\n\n\n\n\nThe dataset spans 1418 √ó 2742 pixels and contains five spectral bands: red, green, blue, nir08, swir22.\n\nRestoring Geospatial Information\nBefore we can perform spatial operations, we need to verify and restore the coordinate reference system (CRS) information.\n\n\nCode\n# Recover the geospatial information from spatial_ref\nlandsat = landsat.rio.write_crs(landsat.spatial_ref.crs_wkt)\n\n# Verify CRS was set\nprint(f\"Landsat CRS: {landsat.rio.crs}\")\n\n\nLandsat CRS: EPSG:32611\n\n\n\nThe dataset‚Äôs CRS is restored from the spatial_ref metadata variable, setting it to EPSG:32611 (WGS 84 / UTM zone 11N), which is appropriate for the Los Angeles area.\n\n\n\nHandling Missing Values\n\n\nCode\n# Check for NaN values in each band\nbands = ['red', 'green', 'blue', 'nir08', 'swir22']\nnan_status = []\n\nfor band in bands:\n    has_nan = np.isnan(landsat[band]).any().values\n    nan_count = np.isnan(landsat[band]).sum().values\n    nan_status.append({\n        'Band': band.upper(),\n        'Contains NaN': 'Yes' if has_nan else 'No',\n        'NaN Count': int(nan_count)\n    })\n\nnan_df = pd.DataFrame(nan_status)\n\n# Display as formatted table\nnan_df\n\n\n\n\n\n\n\n\n\nBand\nContains NaN\nNaN Count\n\n\n\n\n0\nRED\nNo\n0\n\n\n1\nGREEN\nYes\n1\n\n\n2\nBLUE\nYes\n109\n\n\n3\nNIR08\nNo\n0\n\n\n4\nSWIR22\nNo\n0\n\n\n\n\n\n\n\n\n\nCode\n# Fill NaN values with 0\nlandsat = landsat.fillna(0)\n\n\nAll five spectral bands contain NaN values at the edges of the scene or in clouded areas. These are filled with zero to prevent visualization errors.\n\n\n\nTrue Color Visualization\nBefore creating specialized visualizations, we can verify our data by creating a true color image using the red, green, and blue bands‚Äîsimilar to how a standard camera would capture the scene.\n\n\nCode\n# Create true color composite with robust scaling\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\nplt.title('True Color Image - Landsat 8')\nplt.show()\n\n\n\n\n\nTrue color composite (RGB) of the fire-affected area\n\n\n\n\n\nThe robust=True parameter clips extreme outlier values to improve visualization, preventing a few bright pixels from washing out the entire image.\n\n\n\nFalse Color Analysis\nFalse color composites use non-visible wavelengths to reveal features invisible to the human eye. By assigning shortwave infrared (SWIR) to red, near-infrared (NIR) to green, and red to blue, we create an image where healthy vegetation appears bright green and burned areas show distinct reddish-brown tones.\n\n\nCode\n# Create false color composite (SWIR/NIR/Red)\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\nplt.title('False Color Composite (SWIR/NIR/Red)')\nplt.show()\n\n\n\n\n\nFalse color composite (SWIR/NIR/Red) highlighting burn scars\n\n\n\n\nWhy this band combination?\n\n\nHealthy vegetation has high NIR reflectance and low SWIR reflectance ‚Üí appears bright green\nBurned areas have low NIR reflectance and moderate SWIR reflectance ‚Üí appears reddish-brown\nUrban areas show moderate reflectance across bands ‚Üí appears in neutral tones\n\n\n\n\nCreating the Final Map\nTo provide geographic context, we overlay the fire perimeters onto our false color image. This requires reprojecting the vector data to match the raster‚Äôs coordinate system (EPSG:32611 - UTM Zone 11N).\n\n\nCode\n# Reproject fire perimeters to match Landsat CRS\neaton = eaton.to_crs(landsat.rio.crs)\npalisades = palisades.to_crs(landsat.rio.crs)\n\n# Create comprehensive map\nfig, ax = plt.subplots(figsize = (12, 12))\n\n# Plot false color composite\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(\n    ax = ax,\n    robust = True,\n    add_colorbar = False\n)\n\n# Overlay fire perimeters\neaton.boundary.plot(ax = ax, edgecolor = 'yellow', linewidth = 2, label = 'Eaton Fire')\npalisades.boundary.plot(ax = ax, edgecolor = 'red', linewidth = 2, label = 'Palisades Fire')\n\n# Add labels and formatting\nplt.title('Eaton and Palisades Fires - False Color Analysis',\n          fontsize = 20, fontweight = 'bold', pad = 20)\nplt.legend(loc = 'upper right', fontsize = 14)\nax.set_xlabel('Easting (meters)', fontsize = 14)\nax.set_ylabel('Northing (meters)', fontsize = 14)\n\n# Add fire name annotations\nax.text(eaton.geometry.centroid.x.values[0],\n        eaton.geometry.centroid.y.values[0] - 2700,\n        'Eaton Fire',\n        color = 'black', fontsize = 14, fontweight = 'bold',\n        bbox = dict(boxstyle = 'round', facecolor = 'white', alpha = 0.7))\n\nax.text(palisades.geometry.centroid.x.values[0] - 22000,\n        palisades.geometry.centroid.y.values[0] + 8000,\n        'Palisades Fire',\n        color = 'black', fontsize = 14, fontweight = 'bold',\n        bbox = dict(boxstyle = 'round', facecolor = 'white', alpha = 0.7))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFalse color composite with fire perimeter overlays\n\n\n\n\n\n\nFigure Interpretation\nThis false color composite displays the Eaton and Palisades fire areas captured by Landsat 8 on February 23, 2025. The image uses shortwave infrared (SWIR), near-infrared (NIR), and red bands to highlight burn scars and vegetation health.\nKey features:\n\n\nBright green areas = Healthy vegetation (high NIR reflectance)\nReddish-brown areas = Burned areas and bare soil (low NIR, moderate SWIR)\nUrban/developed areas = Neutral tones (moderate reflectance across bands)\n\n\nThe fire perimeters are overlaid in yellow (Eaton Fire) and red (Palisades Fire) to delineate the official burn boundaries. This false color combination is particularly effective for assessing fire severity and monitoring landscape recovery, as the spectral signatures of healthy vs.¬†burned vegetation are distinctly different in the infrared wavelengths."
  },
  {
    "objectID": "posts/eds220-final/index.html#analysis-2-environmental-justice-dimensions",
    "href": "posts/eds220-final/index.html#analysis-2-environmental-justice-dimensions",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Analysis 2: Environmental Justice Dimensions",
    "text": "Analysis 2: Environmental Justice Dimensions\nWildfires don‚Äôt impact all communities equally. This analysis examines the socioeconomic characteristics of census tracts affected by the fires to understand the environmental justice implications.\n\nData Preparation\n\nThe EJI dataset contains census tract-level data for all of California with 174 columns including demographic, environmental, and health vulnerability indicators. Both the EJI data and fire perimeters use EPSG:3857 (Web Mercator) coordinate reference system.\n\n\n\nIdentifying Affected Communities\nTo understand which communities were directly impacted by the fires, we use spatial intersection operations to identify census tracts that overlap with the fire perimeters. We then clip these tracts to the exact fire boundaries, ensuring our analysis focuses only on the areas that experienced direct fire impacts rather than nearby unaffected regions.\n\n\nAnalyzing Socioeconomic Vulnerability\nThe Environmental Justice Index includes percentile rankings for various vulnerability indicators. Here we examine EPL_POV200, which represents the percentile rank of persons living below 200% of the federal poverty level‚Äîa measure of economic vulnerability.\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 9))\n\n# Define variable to analyze\neji_variable = 'EPL_MINRTY'\n\n# Scale values to 0-100 for proper percentile display\neji_clip_palisades_scaled = eji_clip_palisades.copy()\neji_clip_eaton_scaled = eji_clip_eaton.copy()\n\n# Multiply by 100 to convert to percentile\neji_clip_palisades_scaled[eji_variable] = eji_clip_palisades[eji_variable] * 100\neji_clip_eaton_scaled[eji_variable] = eji_clip_eaton[eji_variable] * 100\n\n# Define common scale for comparison (0-100)\nvmin = 0\nvmax = 100\n\n# Palisades Fire area\neji_clip_palisades_scaled.plot(\n    column = eji_variable,\n    vmin = vmin, vmax = vmax,\n    cmap = 'YlOrRd',\n    legend = False,\n    ax = ax1,\n    edgecolor = 'black',\n    linewidth = 0.5\n)\nax1.set_title('Minority Population\\nPalisades Fire Area', fontsize = 20)\nax1.axis('off')\n\n# Eaton Fire area\neji_clip_eaton_scaled.plot(\n    column = eji_variable,\n    vmin = vmin, vmax = vmax,\n    cmap = 'YlOrRd',\n    legend = False,\n    ax = ax2,\n    edgecolor = 'black',\n    linewidth = 0.5\n)\nax2.set_title('Minority Population\\nEaton Fire Area', fontsize = 20)\nax2.axis('off')\n\n# Overall title\nfig.suptitle('Percentile Rank of Minority Population in Fire-Affected Areas', \nfontsize = 25, y = 0.98)\n\n# Shared colorbar\nsm = plt.cm.ScalarMappable(\n    cmap = 'YlOrRd',\n    norm = plt.Normalize(vmin = vmin, vmax = vmax)\n)\ncbar_ax = fig.add_axes([0.25, 0.12, 0.5, 0.03])\ncbar = fig.colorbar(sm, cax = cbar_ax, orientation = 'horizontal')\ncbar.set_label('Percentile Rank (0-100)', fontsize = 14)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSocioeconomic vulnerability comparison between fire areas"
  },
  {
    "objectID": "posts/eds220-final/index.html#key-findings",
    "href": "posts/eds220-final/index.html#key-findings",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Key Findings",
    "text": "Key Findings\nThis analysis reveals stark racial and ethnic disparities in wildfire impacts between the two fire areas. The maps show percentile rankings of minority population (persons of color) within fire-affected census tracts, where higher percentile values indicate greater representation of minority residents relative to other California communities.\nPalisades Fire Area:\n\nThe affected census tracts show predominantly low percentile rankings (pale yellow colors, roughly 0th-30th percentiles). The Palisades fire primarily impacted areas with very low proportions of minority residents‚Äîreflecting the area‚Äôs demographics as a predominantly white, affluent coastal community.\n\nEaton Fire Area:\n\nThe burn area shows dramatically higher percentile rankings, with vast portions in the 70th-100th percentiles (dark red). Nearly the entire Eaton fire footprint, particularly around Altadena, shows the highest minority population rankings in the state. The Eaton fire overwhelmingly affected communities of color.\n\nEnvironmental Justice Implications:\nThis stark geographic pattern reveals that wildfire impacts are deeply tied to historical patterns of segregation and structural inequality. Communities of color‚Äîwho already face compounded environmental burdens, reduced access to resources, and barriers to recovery‚Äîbore the brunt of the Eaton fire‚Äôs destruction. This disparity is critical for:\n\n\nEmergency response - ensuring equitable resource distribution during active disasters\nRecovery assistance - directing aid to communities facing systemic barriers to rebuilding\nLong-term planning - addressing how climate disasters compound existing racial inequities"
  },
  {
    "objectID": "posts/eds220-final/index.html#conclusion",
    "href": "posts/eds220-final/index.html#conclusion",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis demonstrates the power of integrating remote sensing techniques with environmental justice data to understand wildfire impacts through both physical and social lenses. The false color analysis using Landsat 8 imagery reveals the spatial extent and severity of burn scars across the landscape, while the environmental justice analysis uncovers critical disparities in who bears the burden of these disasters.\nThe stark contrast between the two fires‚Äîwith Eaton disproportionately affecting economically vulnerable communities in the Altadena area while Palisades primarily impacted wealthier coastal neighborhoods‚Äîunderscores the unequal distribution of climate disaster impacts. Communities with higher percentile rankings of low-income residents face compounded challenges: limited financial reserves for evacuation and temporary housing, reduced access to insurance, barriers to navigating recovery bureaucracy, and fewer resources for long-term rebuilding.\nThe combination of xarray, rioxarray, and geopandas provides a robust Python-based workflow for this critical multi-dataset geospatial analysis. By processing satellite imagery alongside census-based vulnerability data, we can identify not just where fires burned, but whose communities bore the burden‚Äîinformation essential for equitable resource allocation and recovery planning.\nAs climate change drives increased wildfire frequency and intensity across California and the Western United States, these integrated analytical approaches become indispensable tools for disaster response, recovery planning, and climate justice advocacy. Making both the physical and social dimensions of wildfire impacts visible empowers decision-makers to support vulnerable communities more effectively and challenges us to address the structural inequities that make some populations disproportionately vulnerable to environmental disasters."
  },
  {
    "objectID": "posts/eds220-final/index.html#references",
    "href": "posts/eds220-final/index.html#references",
    "title": "Analyzing the 2025 Los Angeles Wildfires: Remote Sensing and Environmental Justice",
    "section": "References",
    "text": "References\n\n\nArcGIS Hub. 2025. ‚ÄúPalisades and Eaton Dissolved Fire Perimeters.‚Äù https://hub.arcgis.com/maps/ad51845ea5fb4eb483bc2a7c38b2370c.\n\n\nCenters for Disease Control and Prevention, and Agency for Toxic Substances and Disease Registry. 2024. ‚ÄúEnvironmental Justice Index.‚Äù https://atsdr.cdc.gov/place-health/php/eji/eji-data-download.html.\n\n\nMicrosoft Planetary Computer. 2025. ‚ÄúLandsat Collection 2 Level-2.‚Äù https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2."
  },
  {
    "objectID": "posts/eds223-final/index.html",
    "href": "posts/eds223-final/index.html",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "",
    "text": "In February 2021, a historic winter storm swept across Texas, bringing record-low temperatures and overwhelming the state‚Äôs power grid. At the peak of the crisis, more than 4.5 million homes and businesses lost power, leaving residents without electricity for heating during freezing temperatures. The event resulted in at least 246 deaths and an estimated $195 billion in economic damages, making it one of the costliest natural disasters in U.S. history.\n\n\nThis analysis addresses a critical question: Which communities in the Houston metropolitan area were most affected by the blackouts, and were there disparities in impacts across different income levels?\nUnderstanding the spatial distribution of power outages and their relationship to socioeconomic factors is essential for:\n\nImproving power grid resilience and emergency response planning\nIdentifying vulnerable communities that may need additional support during future crises\nInforming equitable infrastructure investment decisions"
  },
  {
    "objectID": "posts/eds223-final/index.html#introduction",
    "href": "posts/eds223-final/index.html#introduction",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "",
    "text": "In February 2021, a historic winter storm swept across Texas, bringing record-low temperatures and overwhelming the state‚Äôs power grid. At the peak of the crisis, more than 4.5 million homes and businesses lost power, leaving residents without electricity for heating during freezing temperatures. The event resulted in at least 246 deaths and an estimated $195 billion in economic damages, making it one of the costliest natural disasters in U.S. history.\n\n\nThis analysis addresses a critical question: Which communities in the Houston metropolitan area were most affected by the blackouts, and were there disparities in impacts across different income levels?\nUnderstanding the spatial distribution of power outages and their relationship to socioeconomic factors is essential for:\n\nImproving power grid resilience and emergency response planning\nIdentifying vulnerable communities that may need additional support during future crises\nInforming equitable infrastructure investment decisions"
  },
  {
    "objectID": "posts/eds223-final/index.html#background",
    "href": "posts/eds223-final/index.html#background",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Background",
    "text": "Background\n\nThe 2021 Texas Winter Storm Crisis\nThe February 2021 winter storm, known as Winter Storm Uri, exposed critical vulnerabilities in Texas‚Äôs isolated power grid. Unlike other states, Texas operates its own grid (ERCOT), which is not connected to neighboring states, limiting backup power options during emergencies. The unprecedented cold caused natural gas pipelines to freeze, wind turbines to ice over, and power plants to fail, creating a cascading crisis.\nWhile the blackouts affected millions of Texans, their impacts were not uniformly distributed. Previous research on climate disasters has shown that low-income communities and communities of color often experience disproportionate impacts from extreme weather events. These communities may face longer recovery times, limited access to emergency resources, and inadequate infrastructure.\n\n\nRemote Sensing for Disaster Assessment\nSatellite-based night lights data provides a powerful tool for assessing power outages at scale. NASA‚Äôs Visible Infrared Imaging Radiometer Suite (VIIRS) captures daily nighttime light emissions, allowing researchers to detect changes in electrical grid function. By comparing light intensity before and after disasters, we can identify affected areas without relying solely on utility company reports, which may be incomplete during major crises.\nThis analysis combines VIIRS night lights data with OpenStreetMap building footprints and U.S. Census socioeconomic data to map blackout impacts and explore potential environmental justice dimensions of the 2021 Texas blackouts."
  },
  {
    "objectID": "posts/eds223-final/index.html#data-and-methods",
    "href": "posts/eds223-final/index.html#data-and-methods",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Data and Methods",
    "text": "Data and Methods\n\nData Sources\nThis analysis integrates four primary datasets:\nVIIRS Night Lights Data ‚Äî NASA‚Äôs VIIRS instrument provides daily measurements of nighttime light intensity. I used two dates of imagery: February 7, 2021 (before the storm) and February 16, 2021 (during the height of the blackouts). The data is measured in nanowatts per square centimeter per steradian (nW cm‚Åª¬≤sr‚Åª¬π) (NASA LAADS DAAC 2021).\nOpenStreetMap Building Footprints ‚Äî Residential building polygons for the Houston metropolitan area, including houses, apartments, and other residential structures (OpenStreetMap Contributors 2021).\nOpenStreetMap Road Network ‚Äî Highway and major road data used to filter out areas where reduced light may be due to decreased traffic rather than power outages (OpenStreetMap Contributors 2021).\nU.S. Census American Community Survey ‚Äî 2015-2019 five-year estimates at the census tract level, including median household income data (U.S. Census Bureau 2019).\n\n\nAnalytical Approach\n\n\nCode\n# Load required packages\nlibrary(tidyverse)      # Data manipulation and visualization\nlibrary(sf)             # Spatial data handling\nlibrary(here)           # File path management\nlibrary(terra)          # Raster data processing\nlibrary(stars)          # Spatiotemporal arrays\nlibrary(tmap)           # Thematic mapping\nlibrary(kableExtra)     # Table formatting\n\n\nThe analysis follows these steps:\n\nLoad and prepare spatial data ‚Äî Read VIIRS night lights tiles, building footprints, road networks, and census data\nCalculate light intensity changes ‚Äî Compare pre- and post-storm imagery to identify areas with significant light reduction\nDefine blackout threshold ‚Äî Classify areas with light intensity drops &gt;200 nW cm‚Åª¬≤sr‚Åª¬π as experiencing blackouts\nFilter highway corridors ‚Äî Exclude areas within 200m of major highways to avoid misidentifying reduced traffic as power outages\nIdentify impacted buildings ‚Äî Intersect blackout areas with residential building footprints\nAnalyze socioeconomic patterns ‚Äî Compare median household income distributions between census tracts with and without blackouts\n\n\n\nLoading and Preparing the Data\n\n\nCode\n# Load VIIRS night lights raster tiles for Houston area\n# February 7, 2021 (pre-storm)\n\nVIIRS_07a &lt;- rast(here(\"posts/eds223-final/data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nVIIRS_07b &lt;- rast(here(\"posts/eds223-final/data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\n\n# February 16, 2021 (during blackouts)\nVIIRS_16a &lt;- rast(here(\"posts/eds223-final/data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nVIIRS_16b &lt;- rast(here(\"posts/eds223-final/data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n# Load OpenStreetMap highways (motorways only)\nhighways &lt;- st_read(here(\"posts/eds223-final/data/gis_osm_roads_free_1.gpkg/gis_osm_roads_free_1.gpkg\"),\n                    query = \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\",\n                    quiet = TRUE)\n\n# Load OpenStreetMap residential buildings\nhouses &lt;- st_read(here(\"posts/eds223-final/data/gis_osm_buildings_a_free_1.gpkg/gis_osm_buildings_a_free_1.gpkg\"),\n                  query = \"SELECT * FROM gis_osm_buildings_a_free_1\n                           WHERE (type IS NULL AND name IS NULL)\n                           OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\",\n                  quiet = TRUE)\n\n\n\n\nCode\n# Load Census ACS data - requires joining geometry with income attributes\nsocio_geom &lt;- st_read(here(\"posts/eds223-final/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/\"),\n                      layer = \"ACS_2019_5YR_TRACT_48_TEXAS\", quiet = TRUE)\n\nsocio_income &lt;- st_read(here(\"posts/eds223-final/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/\"),\n                        layer = \"X19_INCOME\", quiet = TRUE)\n\n# Join income data to geometries and reproject to match building data\nsocio &lt;- socio_geom %&gt;%\n  rename(\"GEOID_Data\" = \"GEOID_Data\") %&gt;%\n  left_join(socio_income %&gt;% rename(\"GEOID_Data\" = \"GEOID\"), by = \"GEOID_Data\") %&gt;%\n  st_transform(st_crs(houses))\n\n\nAll datasets use different coordinate reference systems initially, so I standardized them to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area), which preserves area measurements needed for accurate spatial analysis."
  },
  {
    "objectID": "posts/eds223-final/index.html#results",
    "href": "posts/eds223-final/index.html#results",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Results",
    "text": "Results\n\nVisualizing the Blackouts: Before and After\nThe stark difference in nighttime light emissions reveals the massive scale of the power outages across Houston.\n\n\nCode\n# Merge VIIRS tiles for each date\nVIIRS_07 &lt;- merge(VIIRS_07a, VIIRS_07b)  # Pre-storm\nVIIRS_16 &lt;- merge(VIIRS_16a, VIIRS_16b)  # During blackouts\n\n# Define Houston bounding box\nHOUSTON_BBOX &lt;- st_bbox(c(xmin = -96.5, ymin = 29,\n                          xmax = -94.5, ymax = 30.5),\n                        crs = st_crs(VIIRS_07))\n\n# Crop to Houston area\ncrop_07 &lt;- crop(VIIRS_07, HOUSTON_BBOX)\ncrop_16 &lt;- crop(VIIRS_16, HOUSTON_BBOX)\n\n# Mask unrealistic values (&gt;1000 or ‚â§0)\nmasked_07 &lt;- crop_07\nmasked_07[(masked_07 &gt; 1000) | (masked_07 &lt;= 0)] &lt;- NA\n\nmasked_16 &lt;- crop_16\nmasked_16[(masked_16 &gt; 1000) | (masked_16 &lt;= 0)] &lt;- NA\n\n# Combine for side-by-side comparison\ncombined_crop &lt;- c(masked_07, masked_16)\nnames(combined_crop) &lt;- c(\"February 7, 2021 (Before)\", \"February 16, 2021 (After)\")\n\n\n\n\nCode\ntm_shape(combined_crop) +\n  tm_raster(col.scale = tm_scale(values = \"inferno\"),\n            col.legend = tm_legend(title = \"Light Intensity\\n(nW cm‚Åª¬≤sr‚Åª¬π)\",\n                                   title.size = 0.9,\n                                   text.size = 0.6)) +\n  tm_facets(ncol = 2, free.scales = FALSE) +\n  tm_title(\"Houston Night Lights: Before and After the Storm\", size = 1.2) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            legend.outside.size = 0.12,\n            panel.labels = c(\"February 7 (Before)\", \"February 16 (After)\"),\n            fontfamily = \"sans\") +\n  tm_graticules(lwd = 0.2, col = \"white\", alpha = 0.3, labels.size = 0.5)\n\n\n\n\n\nNight lights imagery showing Houston before (left) and during (right) the February 2021 blackouts. Darker areas in the right panel indicate regions that lost power. The widespread darkening across the metropolitan area illustrates the extensive scale of the grid failure.\n\n\n\n\nThe comparison reveals significant light reduction across much of the Houston metropolitan area. The bright urban core visible on February 7 dims substantially by February 16, indicating widespread power failures.\n\n\nIdentifying Blackout Areas\nTo systematically identify blackout zones, I calculated the difference in light intensity and classified areas with drops greater than 200 nW cm‚Åª¬≤sr‚Åª¬π as experiencing blackouts.\n\n\nCode\n# Calculate light intensity difference\nVIIRS_houston_diff &lt;- VIIRS_07 - VIIRS_16\n\n# Classify blackout areas (drops &gt;200 as blackouts)\nblackout_mask &lt;- classify(VIIRS_houston_diff,\n                         matrix(c(-Inf, 200, NA,  # Not a blackout\n                                  200, Inf, 1),   # Blackout\n                                ncol = 3, byrow = TRUE))\n\n# Convert raster to vector polygons\nblackout_vector &lt;- st_as_stars(blackout_mask) %&gt;%\n  st_as_sf() %&gt;%\n  st_make_valid()\n\n# Crop to Houston and reproject\nblackout_houston &lt;- st_crop(blackout_vector, HOUSTON_BBOX) %&gt;%\n  st_transform(crs = 3083)\n\n\n\n\nCode\nblackout_houston_union &lt;- st_union(blackout_houston)\n\ntm_basemap(\"CartoDB.VoyagerNoLabels\") +\n  tm_shape(blackout_houston_union) +\n  tm_polygons(fill = \"#d62728\",\n              fill_alpha = 0.6,\n              col = \"#8b0000\",\n              lwd = 0.5) +\n  tm_title(\"Detected Blackout Areas in Houston\", size = 1.2) +\n  tm_layout(legend.show = FALSE,\n            fontfamily = \"sans\") +\n  tm_scalebar(position = c(\"left\", \"bottom\"), text.size = 0.6) +\n  tm_compass(position = c(\"right\", \"top\"), size = 1.5)\n\n\n\n\n\nInitial identification of blackout areas based on night lights analysis. Red areas experienced light intensity drops exceeding 200 nW cm‚Åª¬≤sr‚Åª¬π, indicating likely power outages.\n\n\n\n\n\n\nRefining the Analysis: Excluding Highways\nMajor highways may show reduced light intensity due to decreased traffic rather than residential power outages. To improve accuracy, I excluded areas within 200 meters of motorways.\n\n\nCode\n# Reproject highways and create 200m buffer\nhighways &lt;- st_transform(highways, crs = 3083)\nhighway_buffer &lt;- highways %&gt;%\n  st_union() %&gt;%\n  st_buffer(dist = 200)\n\n# Remove highway buffers from blackout areas\nblackout_final &lt;- st_difference(blackout_houston, highway_buffer)\n\n\n\n\nCode\n# Visualize the highway exclusion process\ntm_basemap(\"CartoDB.VoyagerNoLabels\") +\n  tm_shape(st_union(blackout_houston)) +\n  tm_polygons(fill = \"#d62728\",\n              fill_alpha = 0.3,\n              col = \"#8b0000\",\n              lwd = 0.3) +\n  tm_shape(highways) +\n  tm_lines(col = \"#FDB863\",\n           lwd = 2) +\n  tm_shape(st_union(blackout_final)) +\n  tm_polygons(fill = \"#8b0000\",\n              fill_alpha = 0.6,\n              col = \"#4d0000\",\n              lwd = 0.5) +\n  tm_add_legend(type = \"fill\",\n                labels = c(\"Initial blackout areas\", \"Refined blackout areas (highways excluded)\"),\n                fill = c(\"#d62728\", \"#8b0000\"),\n                fill_alpha = c(0.3, 0.6)) +\n  tm_add_legend(type = \"line\",\n                labels = \"Major highways\",\n                col = \"#FDB863\",\n                lwd = 2) +\n  tm_title(\"Highway Corridor Exclusion\", size = 1.2) +\n  tm_layout(legend.position = c(\"left\", \"top\"),\n            legend.bg.color = \"white\",\n            legend.bg.alpha = 0.85,\n            legend.text.size = 0.65,\n            fontfamily = \"sans\") +\n  tm_scalebar(position = c(\"right\", \"bottom\"), text.size = 0.6) +\n  tm_compass(position = c(\"right\", \"top\"), size = 1.5)\n\n\n\n\n\nRefinement of blackout areas by excluding highway corridors. The yellow lines show major highways with 200m buffers applied. This step removes areas where reduced light may be due to decreased traffic rather than residential power outages.\n\n\n\n\nThis refinement helps ensure we‚Äôre identifying genuine residential power outages rather than changes in traffic patterns.\n\n\nQuantifying Residential Impacts\nBy intersecting the blackout areas with residential building footprints, we can estimate how many homes lost power.\n\n\nCode\n# Reproject buildings and identify those in blackout zones\nhouses &lt;- st_transform(houses, crs = 3083)\nblackout_homes &lt;- st_intersection(houses, blackout_final)\n\n# Calculate statistics\nn_blackout_homes &lt;- length(unique(blackout_homes$osm_id))\nn_total_homes &lt;- nrow(houses)\npct_impacted &lt;- round((n_blackout_homes / n_total_homes) * 100, 1)\n\n\nFinding: Approximately 157,408 residential buildings (33.1% of structures in the dataset) experienced blackouts during the February 2021 storms.\n\n\nCode\n# Create point representations for mapping\nall_homes_points &lt;- st_centroid(houses)\nnot_impacted_homes &lt;- all_homes_points[!all_homes_points$osm_id %in% blackout_homes$osm_id, ]\nimpacted_homes_points &lt;- st_centroid(blackout_homes)\n\n\n\n\nCode\ntm_basemap(\"CartoDB.VoyagerNoLabels\") +\n  tm_shape(blackout_final) +\n  tm_polygons(fill = \"grey85\", col = \"grey70\", lwd = 0.3) +\n  tm_shape(not_impacted_homes) +\n  tm_dots(fill = \"#FFB3B3\", size = 0.003, fill_alpha = 0.1) +\n  tm_shape(impacted_homes_points) +\n  tm_dots(fill = \"#8B0000\", size = 0.005, fill_alpha = 0.7) +\n  tm_title(\"Residential Buildings Impacted by Blackouts\", size = 1.3) +\n  tm_layout(legend.outside = FALSE,\n            legend.position = c(\"left\", \"top\"),\n            legend.bg.color = \"white\",\n            legend.bg.alpha = 0.85,\n            legend.text.size = 0.7,\n            legend.title.size = 0.9,\n            fontfamily = \"sans\") +\n  tm_add_legend(type = \"fill\",\n                labels = \"Blackout zone\",\n                fill = \"grey85\",\n                col = \"grey70\") +\n  tm_add_legend(type = \"symbol\",\n                labels = c(\"Power lost\", \"Power maintained\"),\n                fill = c(\"#8B0000\", \"#FFB3B3\"),\n                shape = 19,\n                size = 0.5) +\n  tm_scalebar(position = c(\"right\", \"bottom\"), text.size = 0.6) +\n  tm_compass(position = c(\"right\", \"top\"), size = 1.5)\n\n\n\n\n\nSpatial distribution of impacted and non-impacted residential buildings. Dark red points represent homes that experienced blackouts, while light pink points indicate buildings that maintained power throughout the crisis.\n\n\n\n\nThe map reveals that blackouts were widespread but not uniform. Some neighborhoods maintained power while adjacent areas went dark, suggesting the complexity of the grid failures.\n\n\nSocioeconomic Analysis: Income and Blackout Exposure\nTo investigate potential disparities in blackout impacts, I analyzed the relationship between census tract median household income and blackout exposure.\n\n\nCode\n# Transform census data to analysis CRS\nsocio &lt;- st_transform(socio, crs = 3083)\n\n# Crop to Houston area\nhouston_tracts &lt;- st_crop(socio, st_bbox(blackout_houston))\n\n# Count impacted homes per tract\nhouston_tracts$n_homes &lt;- as.numeric(lengths(st_intersects(houston_tracts, blackout_homes)))\n\n# Create impact categories for mapping\nhouston_tracts$impact_category &lt;- cut(houston_tracts$n_homes,\n                                       breaks = c(0, 1, 10, 50, 150, 500, 1500, Inf),\n                                       labels = c(\"None\",\n                                                 \"Very Low (1-10)\",\n                                                 \"Low (11-50)\",\n                                                 \"Medium (51-150)\",\n                                                 \"High (151-500)\",\n                                                 \"Very High (501-1,500)\",\n                                                 \"Extreme (&gt;1,500)\"),\n                                       include.lowest = TRUE)\n\n# Create binary blackout indicator\nhouston_tracts$blackout &lt;- factor(houston_tracts$n_homes &gt; 0,\n                                  levels = c(TRUE, FALSE),\n                                  labels = c(\"Blackout\", \"No Blackout\"))\n\n\n\n\nCode\ntm_basemap(\"CartoDB.VoyagerNoLabels\") +\n  tm_shape(houston_tracts) +\n  tm_fill(fill = \"impact_category\",\n          fill.scale = tm_scale(values = c(\"white\", \"#feedde\", \"#fdd0a2\", \"#fdae6b\",\n                                          \"#fd8d3c\", \"#e6550d\", \"#a63603\"),\n                                value.na = \"transparent\"),\n          fill.legend = tm_legend(title = \"Impacted Homes\\nper Census Tract\",\n                                  title.size = 0.9,\n                                  text.size = 0.65)) +\n  tm_borders(col = \"grey50\", lwd = 0.3) +\n  tm_title(\"Blackout Severity by Census Tract\", size = 1.3) +\n  tm_scalebar(position = c(\"left\", \"bottom\"), text.size = 0.6) +\n  tm_compass(position = c(\"right\", \"top\"), size = 1.5) +\n  tm_layout(legend.outside = FALSE,\n            legend.position = c(\"right\", \"top\"),\n            legend.bg.color = \"white\",\n            legend.bg.alpha = 0.85,\n            fontfamily = \"sans\")\n\n\n\n\n\nNumber of residential buildings that lost power by census tract. Darker colors indicate higher numbers of impacted homes. The spatial pattern shows that some tracts experienced extreme impacts while neighboring areas had minimal disruption.\n\n\n\n\nThe census tract analysis reveals substantial variation in blackout severity. Some tracts lost power for more than 1,500 residential buildings, while others experienced minimal disruption.\n\n\nCode\nhouston_tracts_no_na &lt;- houston_tracts %&gt;%\n  filter(!is.na(B19013e1))\n\nggplot(houston_tracts_no_na, aes(x = blackout, y = B19013e1, fill = blackout)) +\n  geom_boxplot(alpha = 0.75, outlier.alpha = 0.5) +\n  scale_fill_manual(name = \"Blackout Status\",\n                    values = c(\"Blackout\" = \"#d62728\",\n                               \"No Blackout\" = \"#7f7f7f\")) +\n  scale_y_continuous(labels = scales::dollar_format(),\n                    breaks = seq(0, 250000, 50000)) +\n  labs(title = \"Median Household Income by Blackout Status\",\n       subtitle = \"Houston Metropolitan Area Census Tracts (2015-2019 ACS)\",\n       x = \"\",\n       y = \"Median Household Income\",\n       caption = \"Source: U.S. Census Bureau American Community Survey\") +\n  theme_minimal(base_size = 12, base_family = \"sans\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 14, face = \"bold\", hjust = 0),\n        plot.subtitle = element_text(size = 11, hjust = 0),\n        panel.grid.minor = element_blank())\n\n\n\n\n\nMedian household income comparison between census tracts that experienced blackouts versus those that maintained power. The similar distributions suggest that income was not a strong predictor of blackout exposure during this crisis.\n\n\n\n\n\n\nCode\nincome_stats &lt;- houston_tracts %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(B19013e1)) %&gt;%\n  group_by(blackout) %&gt;%\n  summarise(\n    `Census Tracts` = n(),\n    `Median Income` = scales::dollar(median(B19013e1)),\n    `Mean Income` = scales::dollar(mean(B19013e1)),\n    `Standard Deviation` = scales::dollar(sd(B19013e1))\n  )\n\nkable(income_stats,\n      caption = \"Table 1: Income Statistics by Blackout Status\",\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n                full_width = FALSE,\n                position = \"left\")\n\n\n\nTable 1: Income Statistics by Blackout Status\n\n\nblackout\nCensus Tracts\nMedian Income\nMean Income\nStandard Deviation\n\n\n\n\nBlackout\n749\n$60,435\n$71,262.24\n$39,649.34\n\n\nNo Blackout\n355\n$57,385\n$67,352.78\n$35,900.53\n\n\n\n\n\nKey Finding: Census tracts that experienced blackouts had a median household income of 60,435, compared to 57,385 for tracts without blackouts. The difference of approximately $3,000 represents only about 5% variation, suggesting that income level was not a strong predictor of blackout exposure in this event."
  },
  {
    "objectID": "posts/eds223-final/index.html#discussion",
    "href": "posts/eds223-final/index.html#discussion",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Discussion",
    "text": "Discussion\n\nMain Findings\nThis spatial analysis identified approximately 157,408 residential buildings in the Houston metropolitan area that experienced power outages during the February 2021 winter storms. The widespread nature of these blackouts‚Äîaffecting 33.1% of buildings in the study area‚Äîunderscores the severity of the grid failure.\nContrary to patterns often observed in climate disasters, this analysis found minimal income-based disparities in blackout exposure. Both lower-income and higher-income census tracts experienced similar rates of power loss. This pattern likely reflects the unprecedented scale and systemic nature of the grid failure. When the entire electrical system was compromised by extreme cold, infrastructure vulnerabilities transcended neighborhood boundaries.\n\n\nBroader Implications\nWhile this analysis found relatively equitable blackout exposure across income levels, the impacts of those blackouts may have been highly inequitable. Research following the 2021 storms documented that low-income communities faced:\n\nGreater difficulty evacuating to heated locations\nLimited access to backup heating sources\nLonger recovery times\nHigher health impacts from cold exposure (Gronlund 2014)\n\nThis distinction between exposure and vulnerability is critical for emergency planning. Even when disasters affect communities uniformly, pre-existing inequalities shape how severely people experience those impacts.\n\n\nLimitations and Future Directions\nSeveral methodological limitations should be noted:\nTemporal resolution: This analysis compares only two snapshots in time. A more detailed temporal analysis could reveal variations in outage duration across neighborhoods, which may correlate with socioeconomic factors.\nThreshold sensitivity: The 200 nW cm‚Åª¬≤sr‚Åª¬π threshold for defining blackouts is somewhat arbitrary. Some genuine outages may fall below this threshold, while brief flickers above it might not represent sustained power loss.\nBuilding data completeness: OpenStreetMap data coverage varies geographically. Some residential buildings may be missing from the dataset, potentially introducing bias if data quality correlates with neighborhood characteristics.\nHighway buffer distance: The 200-meter highway exclusion buffer, while reasonable, may inadvertently exclude some legitimate residential blackouts or fail to capture all traffic-related light changes.\nFuture research could enhance this analysis by:\n\nIncorporating daily VIIRS data throughout the multi-day blackout event to assess temporal patterns\nComparing blackout exposure to other socioeconomic indicators (race, age, housing tenure)\nAnalyzing recovery times and their relationship to community characteristics\nIntegrating health outcome data to assess disparate impacts beyond simple exposure\nConducting ground-truthing with utility company outage records where available\n\n\n\nPolicy Recommendations\nThis analysis supports several policy recommendations for improving grid resilience and emergency preparedness:\n\nInfrastructure hardening: The widespread nature of blackouts indicates systemic vulnerabilities that require comprehensive winterization of generation facilities and transmission infrastructure\nEmergency response planning: Even when disasters affect communities uniformly, response systems should account for differential vulnerabilities and capacities to cope with outages\nGrid interconnection: Texas‚Äôs electrical isolation limited backup power options during the crisis, suggesting value in greater regional grid connectivity\nContinued monitoring: Remote sensing approaches like this analysis can complement utility reporting to provide rapid, independent assessment of disaster impacts"
  },
  {
    "objectID": "posts/eds223-final/index.html#conclusion",
    "href": "posts/eds223-final/index.html#conclusion",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Conclusion",
    "text": "Conclusion\nThe February 2021 Texas blackouts represented a catastrophic failure of critical infrastructure affecting millions of residents. This spatial analysis demonstrates how satellite remote sensing combined with socioeconomic data can rapidly assess disaster impacts and identify affected communities. While the blackouts affected Houston relatively uniformly across income levels, likely due to the systemic nature of the grid failure, this does not mean the crisis was experienced equitably. Understanding both exposure patterns and underlying vulnerabilities remains essential for building more resilient and just infrastructure systems.\nAs climate change increases the frequency and severity of extreme weather events, tools and approaches for rapid disaster assessment will become increasingly valuable for emergency response, recovery planning, and long-term adaptation."
  },
  {
    "objectID": "posts/eds223-final/index.html#data-sources-and-acknowledgments",
    "href": "posts/eds223-final/index.html#data-sources-and-acknowledgments",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "Data Sources and Acknowledgments",
    "text": "Data Sources and Acknowledgments\nVIIRS Night Lights Data (VNP46A1) ‚Äî NASA‚Äôs Visible Infrared Imaging Radiometer Suite, Suomi NPP satellite, tiles h08v05 and h08v06. Accessed via NASA Earthdata: https://worldview.earthdata.nasa.gov/\nOpenStreetMap ‚Äî Road network and building footprint data. Accessed via Geofabrik: https://download.geofabrik.de/\nU.S. Census Bureau ‚Äî American Community Survey 5-Year Estimates (2015-2019), Texas census tracts. Accessed via: https://www.census.gov/programs-surveys/acs"
  },
  {
    "objectID": "posts/eds223-final/index.html#references",
    "href": "posts/eds223-final/index.html#references",
    "title": "Mapping the Texas Blackouts: A Spatial Analysis of the 2021 Winter Storm Impacts",
    "section": "References",
    "text": "References\n\n\nGronlund, Carina J. 2014. ‚ÄúRacial and Socioeconomic Disparities in Heat-Related Health Effects and Their Mechanisms: A Review.‚Äù Current Epidemiology Reports 1 (3): 165‚Äì73. https://doi.org/10.1007/s40471-014-0014-4.\n\n\nNASA LAADS DAAC. 2021. ‚ÄúVIIRS/NPP Daily Gridded Day Night Band 15 Arc-Second Linear Lat Lon Grid Night.‚Äù NASA Level-1; Atmosphere Archive & Distribution System Distributed Active Archive Center. https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/VNP46A1/.\n\n\nOpenStreetMap Contributors. 2021. ‚ÄúOpenStreetMap Data Extracts.‚Äù Geofabrik GmbH. https://download.geofabrik.de/.\n\n\nU.S. Census Bureau. 2019. ‚ÄúAmerican Community Survey 5-Year Data (2015-2019).‚Äù U.S. Census Bureau. https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes/2019/5-year.html."
  },
  {
    "objectID": "posts/eds242-final/index.html",
    "href": "posts/eds242-final/index.html",
    "title": "The Fog the Models Missed",
    "section": "",
    "text": "Opening the door at 3pm in August 2020 felt like opening an oven.\nMy family rationed AC before the heat peaked, then toughed it out through the evening.\nThe message from our utility was clear: cut power or expect blackouts.\nI thought I understood why: heat wave ‚Üí too many AC units ‚Üí overloaded grid.\nBut while researching for my Ethics and Bias class, I found something different.\nPart of the blackout problem came from decisions made years earlier, based on climate models that overestimated how much solar power California would have on hot summer evenings."
  },
  {
    "objectID": "posts/eds242-final/index.html#following-the-thread",
    "href": "posts/eds242-final/index.html#following-the-thread",
    "title": "The Fog the Models Missed",
    "section": "Following the Thread",
    "text": "Following the Thread\nThe California Independent System Operator‚Äôs root cause analysis (California Independent System Operator (CAISO), California Public Utilities Commission (CPUC), and California Energy Commission (CEC) 2021) pointed to unprecedented heat, inadequate planning for evening ‚Äúnet peak‚Äù demand, and market forecasting errors. But embedded in those findings: resource planning hadn‚Äôt kept pace with California‚Äôs changing energy mix.\nIn the 2010s, California made massive investments in solar infrastructure based on downscaled CMIP5 climate model projections. The problem? The models systematically overestimated solar generation in regions influenced by California‚Äôs coastal meteorology."
  },
  {
    "objectID": "posts/eds242-final/index.html#the-marine-layer-problem",
    "href": "posts/eds242-final/index.html#the-marine-layer-problem",
    "title": "The Fog the Models Missed",
    "section": "The Marine Layer Problem",
    "text": "The Marine Layer Problem\nCalifornia‚Äôs coast has a distinctive climate feature: the marine layer. Cold Pacific waters create a stable layer of cool, moist air trapped under warmer air above. That marine layer produces fog and low stratus clouds that cut incoming solar radiation by 30‚Äì50% (Iacobellis and Cayan 2013).\nThese clouds often push inland through gaps like the Golden Gate and shape microclimates, like in the redwood forests of northern California, that climate models struggle to resolve (Johnstone and Dawson 2010).\n\n\nWhat the Models See\n100-200 km grid cells\n(size of SF Bay Area)\n\nWhat Actually Matters\nMarine layer dynamics at &lt;10 km\n(Golden Gate, coastal valleys)\n\n\nGlobal Climate Models (GCMs) in CMIP5 operate at coarse resolution‚Äîtypically 100-200km grid cells. Marine layer dynamics happen at much finer scales, driven by sea surface gradients, topographic channeling, diurnal cycles, and upwelling.\nEven ‚Äúdownscaled‚Äù versions refined to 10-20km resolution struggle to capture marine layer formation, persistence, and inland penetration.\nA 2024 CEC report (Pierce et al. 2024) states: &gt; ‚ÄúCoastal Low Clouds‚Ä¶ are a persistent, seasonal feature‚Ä¶ Accounting for these clouds improves solar energy forecasting.‚Äù\nCMIP5-era models systematically underestimated marine layer frequency, inland fog extent, and afternoon/evening cloud cover. The Result?: Solar irradiance projections that were too optimistic, especially for afternoon/evening hours‚Äîexactly when California‚Äôs ‚Äúnet peak‚Äù demand occurs."
  },
  {
    "objectID": "posts/eds242-final/index.html#the-path-from-models-to-blackouts",
    "href": "posts/eds242-final/index.html#the-path-from-models-to-blackouts",
    "title": "The Fog the Models Missed",
    "section": "The Path From Models to Blackouts",
    "text": "The Path From Models to Blackouts\nSo how does a bias in cloud cover modeling contribute to blackouts years later?\nThe chain looks like this:\n\n2010‚Äì2015\nCMIP5 downscaled models overestimate solar because they miss marine-layer clouds.\n2012‚Äì2018\nDevelopers invest heavily in solar based on these projections.\n2015‚Äì2020\nPlanning assumes strong late-afternoon solar output.\nActual output is lower, especially during marine-layer events.\nAugust 2020\nHistoric heat wave hits California.\nEvening demand spikes.\nSolar drops faster than expected ‚Üí rolling blackouts.\n\n\n\n\n\n\n\nflowchart LR\n    A[\"CMIP5 models&lt;br/&gt;overestimated solar\"] --&gt; B[\"Developers invested&lt;br/&gt;billions in solar\"]\n    B --&gt; C[\"Grid planning assumed&lt;br/&gt;high afternoon output\"]\n    C --&gt; D[\"Evening ramp larger&lt;br/&gt;than expected\"]\n    D --&gt; E[\"2020 heat wave&lt;br/&gt;blackouts\"]\n\n    classDef plain fill:#f9f9f9,stroke:#888,color:#333\n    class A,B,C,D,E plain;\n\n\n\n\n\n\nStep 1: Overoptimistic generation forecasts (2010-2015) Solar developers use downscaled CMIP5 projections. Models underestimate afternoon cloud cover ‚Üí projected generation 10-15% higher than reality.\nStep 2: Investment decisions (2012-2018)\nBillions flow into solar installations. Agricultural land converts to solar farms in Central Valley.\nStep 3: Grid planning (2015-2020)\nCAISO plans for solar providing increasing power, assuming generation levels matching the overoptimistic projections.\nStep 4: The ‚Äúnet peak‚Äù emerges\nManaging the evening ‚Äúramp‚Äù becomes critical. CAISO assumes more solar during the 5-8pm window than materializes.\nStep 5: August 2020\nHeat wave. Solar performs well mid-day but drops off earlier and faster than expected, partly due to cloud patterns the models hadn‚Äôt captured.\nCAISO‚Äôs root cause analysis (California Independent System Operator (CAISO), California Public Utilities Commission (CPUC), and California Energy Commission (CEC) 2021) states: ‚ÄúResource planning targets have not kept pace with the evolving power mix, wherein demand during peak hours outpaces the supply of solar-produced power.‚Äù\n\nTranslation\nWe thought we‚Äôd have more power available at this time of day than we actually do.\n\n\n\nWhy This Matters Beyond One Heat Wave\nThe marine layer bias wasn‚Äôt the primary cause of California‚Äôs blackouts‚ÄîCAISO‚Äôs analysis shows it was a perfect storm. But the model bias was one link in a chain of assumptions that created vulnerability. It also shows how invisible these assumptions become once they leave the modeling world."
  },
  {
    "objectID": "posts/eds242-final/index.html#connections-to-eds-242-course-concepts",
    "href": "posts/eds242-final/index.html#connections-to-eds-242-course-concepts",
    "title": "The Fog the Models Missed",
    "section": "Connections to EDS 242 Course Concepts",
    "text": "Connections to EDS 242 Course Concepts\nThis case demonstrates several types of bias we studied:\nModel specification bias: systematic errors in how marine layer physics are represented in the model structure itself (Konno et al.; Lecture 2‚Äôs Catalogue of Bias).\nHistorical bias: The LOCA downscaling method assumes future relationships will match past patterns, which breaks down under climate change. (Lecture 2)\nReporting bias model limitations were documented in technical reports but didn‚Äôt get reported in the summary documents that informed billion-dollar decisions. This is what Cori Lopazanski called the ‚Äúproblem of too much information‚Äù in her guest lecture‚Äîscientists have caveats, but those get lost in translation to policy.(Lecture 2, Cori Lopazanski guest lecturer)\nData invisibility By 2020, the connection back to 2015 cloud modeling choices was completely invisible to everyone affected. As we learned in Lecture 9, these are ‚Äúknown unknowns‚Äù that become ‚Äúunknown unknowns‚Äù as they propagate through decision systems. (Lecture 1, Barrowman reading)\nThe bias propagates through what Olteanu et al.¬†call ‚Äúdecision chains,‚Äù compounding at each step. This connects to Environmental Data Justice themes from Lecture 8‚Äîwho gets hurt when data systems fail? Not the modelers or grid planners, but families who lost groceries, elderly folks without AC, people on medical equipment."
  },
  {
    "objectID": "posts/eds242-final/index.html#the-technical-challenge",
    "href": "posts/eds242-final/index.html#the-technical-challenge",
    "title": "The Fog the Models Missed",
    "section": "The Technical Challenge",
    "text": "The Technical Challenge\nClimate modelers know about these biases. Marine boundary layer clouds remain one of the largest sources of uncertainty in climate models (Klein et al. 2017). Understanding different types of bias is crucial for interpreting these limitations (Konno et al. 2024).\n\nThe ProblemDownscalingBias CorrectionBroader Pattern\n\n\nMarine layer physics operates at scales GCMs can‚Äôt resolve. Coastal fog and low stratus depend on boundary layer processes, cloud microphysics, and topographic features at sub-grid scales (Klein et al. 2017).\n\n\nStatistical downscaling (LOCA): Uses historical relationships but assumes future will match past patterns‚Äîquestionable under climate change (Ekstr√∂m, Grose, and Whetton 2015).\nDynamical downscaling (WRF): Simulates physics at finer scale but inherits GCM biases.\nCalifornia‚Äôs approach (LOCA2): Combines both methods (Pierce et al. 2024). A 2024 technical report notes improvements but still recommends changing planning assumptions to account for limitations.\n\n\nDifferent bias correction methods can change answers significantly (Maraun 2016). The choice of method can shift your projection as much as the climate signal itself (Ekstr√∂m, Grose, and Whetton 2015).\nCommon approaches: quantile mapping (assumes stationary bias), delta methods (assumes correct trends), deep learning (overfitting risk), and ensemble averaging (masks uncertainties).\n\n\nThis pattern shows up across environmental data science‚Äîflood risk, crop yields, hydropower, fire risk. Model biases propagate through decision chains (Olteanu et al. 2019). The projections look authoritative, but limitations don‚Äôt make it into summary documents as often as needed."
  },
  {
    "objectID": "posts/eds242-final/index.html#what-we-can-do",
    "href": "posts/eds242-final/index.html#what-we-can-do",
    "title": "The Fog the Models Missed",
    "section": "What We Can Do",
    "text": "What We Can Do\nClimate models are our best tools for understanding future conditions, but we need to work smarter with their limitations.\n\n1. Bake Uncertainty Into Decisions\nDesign systems that adapt if reality diverges. For California‚Äôs grid: maintain flexible generation capacity, plan for higher evening peaks than models predict.\n\n\n2. Cross-Validate With Multiple Methods\n\nHistorical records (past heat wave outcomes)\nEmpirical relationships (actual solar vs.¬†weather)\nExpert opinions (grid operator observations)\nStress testing (20% lower generation scenarios)\n\n\n\n3. Create Feedback Loops\nCompare actual vs.¬†projected generation. Feed learning back into models and planning. California is doing this now, but earlier detection would have helped.\n\n\n4. Invest in the Unglamorous Work\nBias correction and regional model improvement don‚Äôt make headlines, but we under-invest badly. Need better observations, higher resolution models, long-term monitoring, transparent documentation of biases.\n\n\n5. Train Decision-Makers\nEngineers, planners, policy-makers need to understand what models can and can‚Äôt tell them. A projection isn‚Äôt a prediction. Uncertainty is information. Limitations should be upfront, not buried in appendices."
  },
  {
    "objectID": "posts/eds242-final/index.html#why-i-think-about-this",
    "href": "posts/eds242-final/index.html#why-i-think-about-this",
    "title": "The Fog the Models Missed",
    "section": "Why I Think About This",
    "text": "Why I Think About This\nFor my family, the 2020 blackouts were uncomfortable but manageable. For others‚Äîpeople dependent on medical equipment, elderly folks without AC‚Äîthe stakes were higher.\nCalifornia‚Äôs grid planning documents still show tight supply margins during evening peaks. The marine layer isn‚Äôt going anywhere, though research suggests California coastal fog has declined 33% since 1950 (Johnstone and Dawson 2010), with uncertain causes and future trajectory (Torregrosa, O‚ÄôBrien, and Faloona 2014).\nWhat stays with me is how easily biases slip through. Smart people, rigorous methods, peer-reviewed projections, and still, systematic errors propagate until systems fail under stress.\nI see this pattern everywhere in environmental data science. Models of crop yields, water availability, species distributions, fire risk‚Äîall rest on climate projections with known limitations. Those limitations exist, they‚Äôre documented in technical reports, but they don‚Äôt always make it into decision documents.\nWe can build better decision processes, ones that explicitly account for uncertainty, validate projections against reality, and adapt when mismatches appear. That starts with recognizing that environmental data always carries the fingerprints of choices and assumptions baked into its creation (Walker et al. 2018).\nThe point isn‚Äôt to chase perfect models. It‚Äôs to build systems that don‚Äôt fall apart when the models miss something."
  },
  {
    "objectID": "posts/eds242-final/index.html#references",
    "href": "posts/eds242-final/index.html#references",
    "title": "The Fog the Models Missed",
    "section": "References",
    "text": "References\n\n\nCalifornia Independent System Operator (CAISO), California Public Utilities Commission (CPUC), and California Energy Commission (CEC). 2021. ‚ÄúRoot Cause Analysis: Mid-August 2020 Extreme Heat Wave.‚Äù California ISO. http://www.caiso.com/Documents/Final-Root-Cause-Analysis-Mid-August-2020-Extreme-Heat-Wave.pdf.\n\n\nEkstr√∂m, M., M. R. Grose, and P. H. Whetton. 2015. ‚ÄúAn Appraisal of Downscaling Methods Used in Climate Change Research.‚Äù Wiley Interdisciplinary Reviews: Climate Change 6 (3): 301‚Äì19. https://doi.org/10.1002/wcc.339.\n\n\nIacobellis, S. F., and D. R. Cayan. 2013. ‚ÄúThe Variability of California Summertime Marine Stratus: Impacts on Surface Air Temperatures.‚Äù Journal of Geophysical Research: Atmospheres 118 (17): 9105‚Äì22. https://doi.org/10.1002/jgrd.50652.\n\n\nJohnstone, J. A., and T. E. Dawson. 2010. ‚ÄúClimatic Context and Ecological Implications of Summer Fog Decline in the Coast Redwood Region.‚Äù Proceedings of the National Academy of Sciences 107 (10): 4533‚Äì38. https://doi.org/10.1073/pnas.0915062107.\n\n\nKlein, Stephen A., Alex Hall, Joel R. Norris, and Robert Pincus. 2017. ‚ÄúLow-Cloud Feedbacks from Cloud-Controlling Factors: A Review.‚Äù Surveys in Geophysics 38 (6): 1307‚Äì29. https://doi.org/10.1007/s10712-017-9433-3.\n\n\nKonno, K., J. Gibbons, R. Lewis, and A. S. Pullin. 2024. ‚ÄúPotential Types of Bias When Estimating Causal Effects in Environmental Research and How to Interpret Them.‚Äù Environmental Evidence 13 (1): 1. https://doi.org/10.1186/s13750-023-00323-3.\n\n\nMaraun, D. 2016. ‚ÄúBias Correcting Climate Change Simulations - a Critical Review.‚Äù Current Climate Change Reports 2 (4): 211‚Äì20. https://doi.org/10.1007/s40641-016-0050-x.\n\n\nOlteanu, Alexandra, Carlos Castillo, Fernando Diaz, and Emre Kƒ±cƒ±man. 2019. ‚ÄúSocial Data: Biases, Methodological Pitfalls, and Ethical Boundaries.‚Äù Frontiers in Big Data 2: 13. https://doi.org/10.3389/fdata.2019.00013.\n\n\nPierce, D. W., D. R. Cayan, D. R. Feldman, and J. Kalansky. 2024. ‚ÄúAdvanced Statistical-Dynamical Downscaling Methods and Applying Them to California.‚Äù CEC-500-2024-012. California Energy Commission. https://www.energy.ca.gov/sites/default/files/2024-02/CEC-500-2024-012.pdf.\n\n\nTorregrosa, A., T. A. O‚ÄôBrien, and I. C. Faloona. 2014. ‚ÄúCoastal Fog, Climate Change, and the Environment.‚Äù Eos 95 (50): 473‚Äì74. https://doi.org/10.1002/2014EO500001.\n\n\nWalker, Dawn, Eric Nost, Aaron Lemelin, Rebecca Lave, and Lindsey Dillon. 2018. ‚ÄúPracticing Environmental Data Justice: From DataRescue to Data Together.‚Äù Geo: Geography and Environment 5 (2): e00061. https://doi.org/10.1002/geo2.61."
  },
  {
    "objectID": "references/hwk4-task2-false-color-MILLER.html",
    "href": "references/hwk4-task2-false-color-MILLER.html",
    "title": "False Color Visualization of the 2025 Los Angeles Wildfires",
    "section": "",
    "text": "Author: Emily Miller\nDate: November 2025\nRepository: https://github.com/rellimylime/eds220-hwk4.git\n\n# Import required libraries\nimport xarray as xr\nimport rioxarray as rio\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n2) Fire Perimeter Data Exploration\n\n# Load fire perimeter shapefiles\neaton = gpd.read_file('data/Eaton_Perimeter_20250121.shp')\npalisades = gpd.read_file('data/Palisades_Perimeter_20250121.shp')\n\n# Explore the data\nprint(\"Eaton Fire Perimeter:\")\nprint(eaton.info())\nprint(\"\\nPalisades Fire Perimeter:\")\nprint(palisades.info())\n\nEaton Fire Perimeter:\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 20 entries, 0 to 19\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   OBJECTID    20 non-null     int64   \n 1   type        20 non-null     object  \n 2   Shape__Are  20 non-null     float64 \n 3   Shape__Len  20 non-null     float64 \n 4   geometry    20 non-null     geometry\ndtypes: float64(2), geometry(1), int64(1), object(1)\nmemory usage: 932.0+ bytes\nNone\n\nPalisades Fire Perimeter:\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 21 entries, 0 to 20\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   OBJECTID    21 non-null     int64   \n 1   type        21 non-null     object  \n 2   Shape__Are  21 non-null     float64 \n 3   Shape__Len  21 non-null     float64 \n 4   geometry    21 non-null     geometry\ndtypes: float64(2), geometry(1), int64(1), object(1)\nmemory usage: 972.0+ bytes\nNone\n\n\n\n# Check CRS\nprint(f\"Eaton CRS: {eaton.crs}\")\nprint(f\"Palisades CRS: {palisades.crs}\")\n\n# Check if projected or geographic\nprint(f\"\\nIs Eaton CRS geographic? {eaton.crs.is_geographic}\")\nprint(f\"Is Palisades CRS geographic? {palisades.crs.is_geographic}\")\n\nEaton CRS: EPSG:3857\nPalisades CRS: EPSG:3857\n\nIs Eaton CRS geographic? False\nIs Palisades CRS geographic? False\n\n\nSummary of fire perimeter exploration:\nBoth the Eaton and Palisades fire perimeter datasets use EPSG:3857. The Eaton perimeter contains 20 polygon features while the Palisades perimeter has 21. Both Datasets share the same 5-column structure: OBJECTID, type, Shape_Are, Shape_Len, and geometry. Both of these will need to be reprojected to the crs of the raster before analysis.\n\n\n3) NetCDF Data Import and Exploration\n\n# Import Landsat data\nlandsat = xr.open_dataset('data/landsat8-2025-02-23-palisades-eaton.nc')\n\n# Display dataset information\nlandsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 78MB\nDimensions:      (y: 1418, x: 2742)\nCoordinates:\n  * y            (y) float64 11kB 3.799e+06 3.799e+06 ... 3.757e+06 3.757e+06\n  * x            (x) float64 22kB 3.344e+05 3.344e+05 ... 4.166e+05 4.166e+05\n    time         datetime64[ns] 8B ...\nData variables:\n    red          (y, x) float32 16MB ...\n    green        (y, x) float32 16MB ...\n    blue         (y, x) float32 16MB ...\n    nir08        (y, x) float32 16MB ...\n    swir22       (y, x) float32 16MB ...\n    spatial_ref  int64 8B ...xarray.DatasetDimensions:y: 1418x: 2742Coordinates: (3)y(y)float643.799e+06 3.799e+06 ... 3.757e+06units :metreresolution :-30.0crs :EPSG:32611axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinatearray([3799050., 3799020., 3798990., ..., 3756600., 3756570., 3756540.])x(x)float643.344e+05 3.344e+05 ... 4.166e+05units :metreresolution :30.0crs :EPSG:32611axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinatearray([334410., 334440., 334470., ..., 416580., 416610., 416640.])time()datetime64[ns]...[1 values with dtype=datetime64[ns]]Data variables: (6)red(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]green(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]blue(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]nir08(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]swir22(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]spatial_ref()int64...crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :334395.0 30.0 0.0 3799065.0 0.0 -30.0[1 values with dtype=int64]Indexes: (2)yPandasIndexPandasIndex(Index([3799050.0, 3799020.0, 3798990.0, 3798960.0, 3798930.0, 3798900.0,\n       3798870.0, 3798840.0, 3798810.0, 3798780.0,\n       ...\n       3756810.0, 3756780.0, 3756750.0, 3756720.0, 3756690.0, 3756660.0,\n       3756630.0, 3756600.0, 3756570.0, 3756540.0],\n      dtype='float64', name='y', length=1418))xPandasIndexPandasIndex(Index([334410.0, 334440.0, 334470.0, 334500.0, 334530.0, 334560.0, 334590.0,\n       334620.0, 334650.0, 334680.0,\n       ...\n       416370.0, 416400.0, 416430.0, 416460.0, 416490.0, 416520.0, 416550.0,\n       416580.0, 416610.0, 416640.0],\n      dtype='float64', name='x', length=2742))Attributes: (0)\n\n\n\n# Explore dimensions and coordinates\nprint(\"Dimensions:\", landsat.dims)\nprint(\"\\nCoordinates:\", list(landsat.coords))\nprint(\"\\nData variables:\", list(landsat.data_vars))\n\nDimensions: FrozenMappingWarningOnValuesAccess({'y': 1418, 'x': 2742})\n\nCoordinates: ['y', 'x', 'time']\n\nData variables: ['red', 'green', 'blue', 'nir08', 'swir22', 'spatial_ref']\n\n\nSummary of Landsat Exploration:\nThe landsat 8 dataset is stored in NetCDF format which contains 5 spectral bands (red, green, blue, nir08, swir22) as well as a spatial_ref variables that stores crs information. The raster has dimensions of 2742 pixels (width) by 1418 pixels (height) and covers the area around both fires. The dataset includes three coordinates; x and y for position, and time as the third indicating observation date.\n\n\n4) Restoring geospatial information\nThe Landsat dataset stores the geospatial information separately from its values in the ‚Äòspatial_ref‚Äô variable. To do spatial analysis with this data we must extract this data and write it to the dataset.\n\n# Use rio.crs to print CRS information\nprint(landsat.rio.crs)\n\nNone\n\n\nThis object is not currently geospatially referenced.\n\n# Print the crs by accessing spatial_ref.crs_wkt\nprint(landsat.spatial_ref.crs_wkt)\n\n# Recover the geospatial information by using rio.write_crs and the spatial reference information from b\nlandsat = landsat.rio.write_crs(landsat.spatial_ref.crs_wkt)\n\n# Print the CRS of the updated dataset\nprint(landsat.rio.crs)\n\nPROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\nEPSG:32611\n\n\n\n\n5) True color image\nA true color image displays the red, green, and blue bands as they would appear to the human eye. Often raw satellite contains extreme values that can make the rest of the image look washed out when plotting. This can be corrected by handling outliers and missing values properly as demonstrated below\n\n# Plot the true color image\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow()\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nc:\\Users\\ermil\\.conda\\envs\\eds220-env\\Lib\\site-packages\\matplotlib\\cm.py:478: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)\n\n\n\n\n\n\n\n\n\n\n\n# Identify which bands have nan values using numpy.isnan() and utilize the robust parameter in the plot function\nnan_bands = []\nfor band in ['red', 'green', 'blue', 'nir08', 'swir22']:\n    if np.isnan(landsat[band]).any():\n        nan_bands.append(band)\n\nprint(\"Bands with NaN values:\", nan_bands)\n\n# use .fillna() to substitute any nan values for 0\nlandsat = landsat.fillna(0)\n\n# Re-plot the true color image\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust = True)\nplt.show()\n\nBands with NaN values: ['green', 'blue']\n\n\n\n\n\n\n\n\n\nComparison of outputs:\nPart (a) produced a blank-looking image along with two warnings: one about clipping RGB data to valid ranges and another about invalid values during type conversion. This is the result of extreme outliers produced by cloud reflectance that compress all other pixel values near zero when matplotlib scales the image. Part (e) produces a clear, detailed true color image with no warnings. The improvement is the result of two changes: (1) using robust=True clips extreme outlier values for better visualization, and (2) filling NaN values with zero using .fillna(0) eliminates the invalid value warning.\n\n\n6) False color image\n\n# Create a map showing the shortwave infrared/near-infrared/red false color image\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True)\n\n\n\n\n\n\n\n\n\n\n7) Map\n\n# Print CRS information for all datasets\nprint(f\"Landsat CRS: {landsat.rio.crs}\")\nprint(f\"Eaton CRS: {eaton.crs}\")\nprint(f\"Palisades CRS: {palisades.crs}\")\n\n# Reproject fire perimeters to match Landsat CRS\neaton = eaton.to_crs(landsat.rio.crs)\npalisades = palisades.to_crs(landsat.rio.crs)\n\n# Create a map showing the shortwave infrared/near-infrared/red false color image together with both fire perimeters. Customize it appropriately including, at least, an informative title and legend. You may also want to include text on the map to identify which fire is which.\nfig, ax = plt.subplots(figsize=(10, 10))\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust=True)\neaton.boundary.plot(ax=ax, edgecolor='yellow', label='Eaton Fire Perimeter')\npalisades.boundary.plot(ax=ax, edgecolor='red', label='Palisades Fire Perimeter')\nplt.title('Landsat 8 False Color Image with Fire Perimeters')\nplt.legend()\nax.set_ylabel('Y Coordinate (meters)')\nax.set_xlabel('X Coordinate (meters)')\nax.text(eaton.geometry.centroid.x.values[0], eaton.geometry.centroid.y.values[0] - 2700, 'Eaton Fire', color='black', fontsize=14, fontweight='bold')\nax.text(palisades.geometry.centroid.x.values[0] - 22000, palisades.geometry.centroid.y.values[0] + 8000, 'Palisades Fire', color='black', fontsize=14, fontweight='bold')\n\nLandsat CRS: EPSG:32611\nEaton CRS: EPSG:3857\nPalisades CRS: EPSG:3857\n\n\nText(337685.36156204826, 3774541.91755668, 'Palisades Fire')\n\n\n\n\n\n\n\n\n\nFigure Description:\nThis map displays a false color composite image of the Eaton and Palisades fire areas captured by Landsat 8. The image uses shortwave infrared (SWIR), near-infrared (NIR), and red bands to highlight burn scars and vegetation health. Healthy vegetation appears bright green due to high NIR reflectance, while burned areas appear redish brown. The fire perimeters are overlaid in yellow (Eaton Fire) and red (Palisades Fire) to highlight the extent of both fires. This false color combination is helpful for assessing fire severity and landscape recovery."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research sits at the intersection of remote sensing, machine learning, and environmental justice. I believe that the most powerful environmental science happens when we combine technical rigor with human empathy, using cutting-edge technology to address real-world challenges faced by communities around the globe.\n\n‚ÄúEvery pixel in a satellite image tells a story about human interaction with the environment. My job is to teach computers to read those stories with both precision and compassion.‚Äù\n\n\n\n\n\n\n\nNoteCore Research Principles\n\n\n\n\nCommunity-Centered: All research should ultimately benefit the communities whose environments we‚Äôre studying\nInterdisciplinary: The best solutions come from combining insights across multiple fields\nReproducible: Open science and reproducible research practices are essential for building trust\nActionable: Research should lead to tangible policy recommendations and environmental action"
  },
  {
    "objectID": "research.html#research-philosophy",
    "href": "research.html#research-philosophy",
    "title": "Research",
    "section": "",
    "text": "My research sits at the intersection of remote sensing, machine learning, and environmental justice. I believe that the most powerful environmental science happens when we combine technical rigor with human empathy, using cutting-edge technology to address real-world challenges faced by communities around the globe.\n\n‚ÄúEvery pixel in a satellite image tells a story about human interaction with the environment. My job is to teach computers to read those stories with both precision and compassion.‚Äù\n\n\n\n\n\n\n\nNoteCore Research Principles\n\n\n\n\nCommunity-Centered: All research should ultimately benefit the communities whose environments we‚Äôre studying\nInterdisciplinary: The best solutions come from combining insights across multiple fields\nReproducible: Open science and reproducible research practices are essential for building trust\nActionable: Research should lead to tangible policy recommendations and environmental action"
  },
  {
    "objectID": "research.html#current-research-projects",
    "href": "research.html#current-research-projects",
    "title": "Research",
    "section": "Current Research Projects",
    "text": "Current Research Projects\n\nSatellite-Based Irrigation Mapping in Sub-Saharan Africa\nCollaborators: UCSB Water Vegetation and Society (WaVeS) Lab, Prof.¬†Kelly Caylor\nTimeline: June 2024 - Present\nStatus: In Progress\nThis project uses high-resolution satellite imagery and machine learning to identify and map irrigation patterns across Sub-Saharan Africa. Traditional remote sensing methods often miss small-scale agricultural practices, leading to significant underestimates of water use and agricultural productivity.\nKey Innovation: Developing statistical frameworks and Python workflows to process multi-temporal Sentinel-2 imagery and identify center pivot and decentralized irrigation systems. The analysis integrates satellite data with hydrological and agricultural systems modeling.\nImpact: Research findings are informing water resource management decisions and climate adaptation planning for local governments and agricultural extension services.\n\n\n\n\n\n\nTipTechnical Details\n\n\n\nProcessing Sentinel-2 (10m resolution) multi-temporal data to detect irrigation signatures across seasons. Developing Python workflows for large-scale satellite imagery analysis and spatial statistical modeling of agricultural water systems."
  },
  {
    "objectID": "research.html#proposed-in-progress-projects",
    "href": "research.html#proposed-in-progress-projects",
    "title": "Research",
    "section": "Proposed & In-Progress Projects",
    "text": "Proposed & In-Progress Projects\n\nScaling Metadata Quality Assessment for Environmental Data Repositories\nClient: Arctic Data Center / National Center for Ecological Analysis and Synthesis (NCEAS)\nTimeline: Fall 2024 - Spring 2025\nStatus: MEDS Capstone Project\nThis capstone project builds reproducible, scalable workflows to aggregate and visualize FAIR (Findable, Accessible, Interoperable, Reusable) metadata quality assessments across environmental data repositories.\nProject Goal: Currently, the Arctic Data Center runs automated metadata checks generating per-dataset results, but aggregated results are not readily available. With thousands of datasets, it‚Äôs difficult to identify systemic curation issues such as missing ORCIDs or inconsistent units. This project transforms individual metadata quality assessments into standardized, scalable, and actionable insights.\nKey Deliverables:\n\nRepeatable ingest pipeline - Clean, timestamped snapshots of quality assessment data enriched with dataset metadata, with validation to catch data format changes early\nAutomated visualizations and analysis - Generate FAIR pillar summaries, temporal trends, and first vs.¬†latest comparisons that auto-update when new data arrives\nFocused deep-dives - Analyze specific dataset types or disciplines, regroup individual checks into themes, and track percent-pass and patterns across time\nLightweight access interface - Simple filterable web interface to view and download tables, figures, and summaries by time, metadata standard, pillar, or dataset type\nStretch goal - Configure one additional repository with the same framework to demonstrate portability\n\nTechnical Approach: Building reproducible workflows in Python with open data formats (CSV, Parquet). Using Quarto/Jupyter for reports. Emphasis on lightweight, maintainable design that repository staff can sustain post-project.\nImpact: This project enhances Arctic research data accessibility and usability by turning existing checks into clear, repeatable quality signals. Plain-language, FAIR-aligned metrics reduce barriers for research teams, particularly those with limited institutional data support, promoting equitable participation in environmental science. The scalable framework offers a model for other repositories, improving the connected environmental data ecosystem.\n\n\n\n\n\n\nTipBroader Impacts\n\n\n\nPlain-language metrics and transparent interface aids environmental education and literacy. Cross-repository comparisons can identify patterns for new checks or standard improvements. The reproducible design fosters collaboration among repository teams and advances community standards for FAIR data."
  },
  {
    "objectID": "research.html#presentations",
    "href": "research.html#presentations",
    "title": "Research",
    "section": "Presentations",
    "text": "Presentations\n\nConference Presentations & Talks\n\nAmerican Geophysical Union (AGU) Fall Meeting 2024 - ‚ÄúAnalyzing the sustainability and climate resilience of rapidly expanding center pivot Irrigation in Sub-Saharan Africa using remote sensing‚Äù with Boser A, Perez J, and Prof.¬†Caylor K (Washington D.C., December 9-13, 2024)\nMantell Symposium in Environmental Justice and Conservation Innovation 2024 - ‚ÄúWater Source Attribution for Center Pivot Irrigation in Sub-Saharan Africa‚Äù (UCSB, October 24, 2024)\nSummer@Bren 2024 Flash Talks - ‚ÄúSatellite Data Reveal Emerging Decentralized Irrigation Systems in Sub-Saharan Africa‚Äù (UCSB, August 29, 2024)"
  },
  {
    "objectID": "research.html#technical-skills-tools",
    "href": "research.html#technical-skills-tools",
    "title": "Research",
    "section": "Technical Skills & Tools",
    "text": "Technical Skills & Tools\n\nRemote Sensing\n\nSentinel-1/2, Landsat, Planet imagery\nGoogle Earth Engine\nQGIS, ArcGIS Pro\nRasterio, GDAL\n\n\n\nMachine Learning\n\nPyTorch, TensorFlow\nCustom CNN architectures\nTime series analysis\nTransfer learning\n\n\n\nProgramming\n\nPython (pandas, numpy, scikit-learn)\nR (tidyverse, spatial packages)\nJavaScript (Google Earth Engine)\nGit version control\n\n\n\nData Visualization\n\nMatplotlib, Seaborn\nPlotly, D3.js\nTableau, PowerBI\nCustom web visualizations"
  },
  {
    "objectID": "research.html#academic-background",
    "href": "research.html#academic-background",
    "title": "Research",
    "section": "Academic Background",
    "text": "Academic Background\n\nMaster of Environmental Data Science (MEDS)\nUniversity of California, Santa Barbara ‚Äì Bren School\nExpected June 2026 | GPA: 4.0\nFocus: Remote sensing, machine learning, and environmental data engineering\n\n\nBachelor of Science in Mathematics\nUniversity of California, Santa Barbara\nJune 2025 | GPA: 3.67\nFocus: Applied mathematics and environmental science\n\n\nAssociate of Science in Mathematics\nSacramento City College\nJune 2022 | GPA: 4.0"
  },
  {
    "objectID": "research.html#recognition",
    "href": "research.html#recognition",
    "title": "Research",
    "section": "Recognition",
    "text": "Recognition\n\nMEDS Student Faculty Representative - Selected by MEDS cohort to represent student voice in faculty meetings and curriculum development\nBren Environmental Leadership Fellow - Competitive fellowship supporting research on irrigation systems and water resource sustainability in Sub-Saharan Africa (June-December 2024)"
  },
  {
    "objectID": "research.html#research-collaborations",
    "href": "research.html#research-collaborations",
    "title": "Research",
    "section": "Research Collaborations",
    "text": "Research Collaborations\nI believe that the best research happens through collaboration. I work with researchers across multiple disciplines, academic institutions, and community partners.\n\nAcademic Partners\n\nUCSB Water Vegetation and Society (WaVeS) Lab\nUCSB Bren School of Environmental Science & Management\nArctic Data Center / National Center for Ecological Analysis and Synthesis (NCEAS)\n\n\n\nCommunity & Professional Partners\n\nIV Recovery Community Initiative (Founder)\nWomen in Science and Engineering (WiSE) Mentorship Program"
  },
  {
    "objectID": "research.html#research-impact",
    "href": "research.html#research-impact",
    "title": "Research",
    "section": "Research Impact",
    "text": "Research Impact\nMy work focuses on generating actionable insights from environmental data that can inform policy and conservation decisions.\n\n\n\n\n\n\nTipCurrent Focus\n\n\n\n\nWater Resource Sustainability - Analyzing irrigation systems in Sub-Saharan Africa to inform climate adaptation and water management strategies\nEnvironmental Data Infrastructure - Building tools to improve metadata quality and accessibility across environmental research repositories\nCommunity Engagement - Organizing local environmental stewardship through the IV Recovery Community Initiative"
  },
  {
    "objectID": "research.html#lets-collaborate",
    "href": "research.html#lets-collaborate",
    "title": "Research",
    "section": "Let‚Äôs Collaborate!",
    "text": "Let‚Äôs Collaborate!\nI‚Äôm always excited to collaborate with researchers, practitioners, and communities who share my passion for using technology to address environmental challenges. Whether you‚Äôre interested in remote sensing, machine learning, environmental justice, or the intersection of all three, let‚Äôs connect!\nStart a Collaboration"
  }
]