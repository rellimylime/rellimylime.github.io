---
title: "The Fog the Models Missed"
description: "How climate model biases in California's marine layer contributed to the 2020 blackouts"
author: Emily Miller
date: 2024-12-06
categories: [MEDS, Climate Science, Data Science, Ethics]
format: html
image: eds242-final-cover2.jpeg
citation:
  url: https://rellimylime.github.io/posts/eds242-final/
bibliography: references.bib
---

August 2020, Davis, California. Opening the door at 3pm felt like opening an oven, not unusual for a summer afternoon here, but this time someone had turned the setting to broil. My family had fallen into a routine that summer: one last blast of AC before the afternoon heat peaked, then we'd tough it out through the evening. This wasn't just us, it was a community-wide effort. The message from our utility was clear: if we wanted to avoid blackouts, everyone needed to cut power use during the hottest hours, right when we needed it most.

I thought I understood what was happening: extreme heat wave, everyone running AC, grid can't keep up. But while researching case studies for my Ethics and Bias in Environmental Data Science class, I discovered something unexpected. Part of California's blackout problem stemmed from decisions made years earlier, decisions based on climate models that systematically overestimated how much solar power would be available on hot summer evenings

------------------------------------------------------------------------

## Following the Thread

I started looking into this because I wanted to understand case studies of model bias that had real consequences. The 2020 blackouts kept coming up in my reading, but not for the reasons I expected.

The California Independent System Operator's root cause analysis [@caiso2021root] pointed to three main factors: unprecedented heat, inadequate planning for evening "net peak" demand, and market forecasting errors. But embedded in those findings was something more subtle: resource planning hadn't kept pace with California's changing energy mix.

Here's what I found when I dug deeper: In the 2010s, California made massive investments in solar infrastructure—billions of dollars' worth—based on projections of where and how much solar energy these installations would generate over their 25-30 year lifespans. Those projections came from downscaled climate models, specifically CMIP5 outputs that had been refined to California's regional scale.

The problem? The models systematically overestimated solar generation potential in certain regions, particularly areas influenced by California's complex coastal meteorology.

## The Marine Layer Problem

California's coast has a distinctive climate feature: the marine layer. Cold Pacific waters create a stable layer of cool, moist air that gets trapped under warmer air above, forming a strong temperature inversion. When conditions are right, this marine layer produces fog and low stratus clouds that can persist for hours or days.

Coastal low clouds can reduce incoming solar radiation by 30-50% when they're present [@iacobellis2013variability]. They affect coastal temperatures, drive inland through gaps like the Golden Gate, and historically have provided the necessary moisture to ecosystems like redwood forests [@johnstone2010fog].

Here's where the models struggle: Global Climate Models (GCMs) like those in CMIP5 operate at coarse spatial resolution, typically 100-200km grid cells. California's marine layer dynamics happen at much finer scales, driven by:

\- Sea surface temperature gradients

\- Topographic channeling through coastal ranges

\- Diurnal heating/cooling cycles

\- Offshore upwelling patterns

Even "downscaled" versions of these models, e.g. those refined to 10-20km resolution using statistical or dynamical techniques, have trouble capturing the formation, persistence, and inland penetration of coastal low clouds.

A 2024 California Energy Commission technical report on downscaling methods [@pierce2024downscaling] noted: "Coastal Low Clouds (CLC), including stratocumulus, stratus, and fog, are a persistent, seasonal feature of the region's climate... Accounting for these clouds improves solar energy forecasting."

The problem is that CMIP5-era models systematically underestimated:

1\. Marine layer frequency and persistence

2\. The inland extent of coastal fog

3\. Afternoon/evening cloud cover in coastal and near-coastal areas

The result? Solar irradiance projections that were too optimistic, especially for afternoon/evening hours; exactly when California's "net peak" demand (after accounting for mid-day solar) occurs.

------------------------------------------------------------------------

## The Path From Models to Blackouts

So how does a bias in cloud cover modeling in the 2010s contribute to blackouts in 2020? The chain looks like this:

**Step 1: Overoptimistic generation forecasts (2010-2015)** Solar developers and utilities use downscaled CMIP5 projections to estimate generation potential. Models underestimate afternoon cloud cover → projected generation is 10-15% higher than reality in affected regions.

**Step 2: Investment and siting decisions (2012-2018)**\
Based on these projections, billions are invested into solar installations. Project economics assume certain capacity factors (how much power gets generated relative to maximum capacity). Land use changes and agricultural land is converted to solar farms in the Central Valley.

**Step 3: Grid planning assumptions (2015-2020)**\
California's grid operator (CAISO) plans for a future where solar provides increasing amounts of power. But their "resource adequacy" models assume generation levels that match the overoptimistic projections.

**Step 4: The "net peak" emerges**\
As solar capacity grows, a new challenge appears: managing the evening "ramp" when solar generation drops off but demand stays high (people come home, turn on AC, cook dinner, etc.). CAISO's planning assumes more solar will be available during this critical 5-8pm window than actually is.

**Step 5: August 2020**\
Record-breaking heat wave hits the entire Western US. Everyone's running AC. Solar performs well during the mid-day peak but starts dropping off earlier and faster than grid models expected, partly because of cloud cover patterns that the original climate models hadn't captured accurately.

CAISO's root cause analysis [@caiso2021root] states: "Resource planning targets have not kept pace with the evolving power mix, wherein demand during peak hours outpaces the supply of solar-produced power."

Translation: "we thought we'd have more power available at this time of day than we actually do."

------------------------------------------------------------------------

## Why This Matters Beyond One Heat Wave

The marine layer bias in climate models was not necessarily the *primary* cause of California's blackouts, the CAISO analysis makes clear it was a perfect storm of factors, but the model bias was part of the systemic chain; one link in a series of assumptions and decisions that put the grid in a vulnerable position.

What strikes me is how invisible this connection is. When I experienced that blackout, I had no idea that decisions made 5-10 years earlier, based on climate projections that likely seemed rigorous and scientific, had quietly baked in errors that would manifest years later under stress.

------------------------------------------------------------------------

## The Technical Challenge

Climate modelers know about these biases. Marine boundary layer clouds remain one of the largest sources of uncertainty in climate models [@klein2017]. Understanding different types of bias, from measurement bias to model specification bias, is crucial for interpreting these limitations [@konno2024bias].

### The Resolution Problem

The core issue is that marine layer physics operates at scales GCMs can't resolve. Coastal fog and low stratus depend on boundary layer processes, cloud microphysics, and topographic features that occur at sub-grid scales [@klein2017].

### Downscaling Approaches

Downscaling helps, but it can't fix everything. The two main approaches are:

**Statistical downscaling** (e.g., LOCA - Localized Constructed Analogs): Uses historical relationships between large-scale climate patterns and local observations [@pierce2024downscaling]. Fast, cheap, widely used. But assumes future relationships will match past patterns, questionable under climate change [@ekstrom2015appraisal]. Also struggles with variables that don't have long observational records.

**Dynamical downscaling** (e.g., WRF - Weather Research and Forecasting model): Runs a high-resolution regional climate model nested within the GCM. Can explicitly simulate physics at 3-10km scale. But *inherits GCM boundary conditions and baises*.

### Current State of the Art

California's most recent downscaling effort (for the 5th Climate Assessment) used a hybrid approach; LOCA2 combines statistical methods with pattern libraries from dynamically downscaled model runs [@pierce2024downscaling]. A 2024 technical report notes this improves solar radiation estimates... but still recommends that "given our shift to clean energy, we have to change planning assumptions and analysis to account for" the limitations.

Translation: even our best current methods have biases that matter for decision-making.

### The Bias Correction Dilemma

Different bias correction methods can change answers significantly [@maraun2016bias]:

-   **Quantile mapping**: Matches model distribution to observed distribution, but assumes bias is stationary over time
-   **Delta method**: Uses model *changes* rather than absolute values, but assumes models capture trends correctly even if baseline is wrong
-   **Deep learning corrections**: Train neural networks on historical errors, but risk overfitting
-   **Ensemble averaging**: Combine multiple models to reduce individual biases, but may mask structural uncertainties

Each method makes different assumptions. Each gives different results. And the choice of correction method can change your projection as much as the climate change signal itself [@ekstrom2015appraisal].

### Cascading Consequences Across Sectors

This pattern shows up everywhere in environmental data science:

-   **Flood risk modeling** → Insurance pricing → Land use decisions → Who lives where
-   **Crop yield projections** → Agricultural investment → Food security policy → International trade
-   **Hydropower forecasting** → Energy mix decisions → Electricity rates → Industrial development
-   **Fire risk assessment** → Building codes → Insurance availability → Community resilience

In each case, model biases propagate through decision chains [@olteanu2019social]. The projections look authoritative, peer-reviewed, published, with confidence intervals. The agencies using them are often doing their best with available tools. But those limitations don't make it into the summary documents that inform decisions as often as they need to.

------------------------------------------------------------------------

## What We Can Do

I don't think the answer is to stop using climate models, they're the best tools we have for understanding future conditions. But we need to be much smarter about acknowledging and working with their limitations.

### 1. Bake Uncertainty Into Decisions

Design systems that can adapt if reality diverges from projections. For California's grid, this means maintaining flexible generation capacity and planning for higher evening peaks than models predict.

### 2. Cross-Validate With Multiple Methods

-   Historical records (when similar heat waves hit in the past, what happened?)
-   Empirical relationships (how does actual solar generation track with weather obs?)
-   Expert opinions (what do grid operators and solar engineers see on the ground?)
-   Stress testing (what if generation is 20% lower than projected?)

### 3. Create Feedback Loops Between Models and Reality

Once solar farms are operating, compare actual vs. projected generation. Feed that learning back into model improvement and planning assumptions. California is doing this now, CAISO has updated resource adequacy models based on 2020 experience, but it would have been better to catch the discrepancies earlier.

### 4. Invest in the Unglamorous Work

Bias correction and regional model improvement don't make headlines. But given how much rides on these projections, we massively under-invest in making them better. We need:

-   Better observations of marine layer dynamics (more buoys, coastal radar, satellite validation)
-   Higher resolution models that can resolve key processes
-   Long-term monitoring of how projections perform vs. reality
-   Transparent documentation of known biases and their magnitudes

### 5. Train Decision-Makers to Be Critical Consumers

Engineers, planners, and policy-makers need to understand what climate models can and can't tell them. A projection is not a prediction. An ensemble mean is not a forecast. Uncertainty is information, not a bug. Model limitations should be discussed upfront in every assessment, not buried in technical appendices.

------------------------------------------------------------------------

## Why I Think About This

For my family, the 2020 blackouts were uncomfortable and inconvenient, but manageable. For others, e.g. people dependent on medical equipment, elderly folks without AC, or families losing groceries, the stakes were higher.

California's grid planning documents still show tight supply margins during evening peaks. The marine layer isn't going anywhere, though there's some evidence it's changing. Research suggests California coastal fog has declined 33% since 1950 [@johnstone2010fog], but we don't fully understand why or what that means for the future [@torregrosa2014fog].

What stays with me from digging into this case is how easily biases can slip through. Smart people, rigorous methods, peer-reviewed projections, and still, systematic errors propagate through planning processes until systems fail under stress.

I see this pattern everywhere in environmental data science. Models of crop yields, water availability, species distributions, fire risk, all rest on climate projections with known limitations. Those limitations exist, they're documented in technical reports, but they don't always make it into the decision documents.

We're not going to build perfect models. But we can build better decision processes, ones that explicitly account for uncertainty, validate projections against reality, and adapt when mismatches appear. That starts with recognizing that environmental data always carries the fingerprints of choices and assumptions baked into its creation [@walker2018practicing].

Understanding those limitations isn't about being critical for its own sake. It's about learning from what happened and doing better next time.

------------------------------------------------------------------------

## References

::: {#refs}
:::
