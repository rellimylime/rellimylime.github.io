---
title: "The Fog the Models Missed"
description: "How climate model biases in California's marine layer contributed to the 2020 blackouts"
author: Emily Miller
date: 2024-12-06
categories: [MEDS, Climate Science, Data Science, Ethics]
format: html
image: eds242-final-cover2.jpeg
citation:
  url: https://rellimylime.github.io/posts/eds242-final/
bibliography: references.bib
---

August 2020, Davis, California. Opening the door at 3pm felt like opening an oven. Not unusual for a summer afternoon here, but this time someone had turned the setting to broil. My family had fallen into a routine that summer: one last blast of AC before the afternoon heat peaked, then we'd tough it out through the evening. The message from our utility was clear: if we wanted to avoid blackouts, everyone needed to cut power use during the hottest hours, right when we needed it most.

I thought I understood what was happening: extreme heat wave, everyone running AC, grid can't keep up. But while researching case studies for my Ethics and Bias in Environmental Data Science class, I discovered something unexpected. Part of California's blackout problem stemmed from decisions made years earlier, decisions based on climate models that systematically overestimated how much solar power would be available on hot summer evenings.

------------------------------------------------------------------------


## Following the Thread

I started looking into this because I wanted to understand case studies of model bias that had real consequences. The 2020 blackouts kept coming up, but not for the reasons I expected.

The California Independent System Operator's root cause analysis [@caiso2021root] pointed to three main factors: unprecedented heat, inadequate planning for evening "net peak" demand, and market forecasting errors. But embedded in those findings was something more subtle: resource planning hadn't kept pace with California's changing energy mix.

In the 2010s, California made massive investments in solar infrastructure based on projections from downscaled CMIP5 climate models. The problem? The models systematically overestimated solar generation potential in regions influenced by California's complex coastal meteorology.

## The Marine Layer Problem

California's coast has a distinctive climate feature: the marine layer. Cold Pacific waters create a stable layer of cool, moist air trapped under warmer air above, forming a temperature inversion that produces fog and low stratus clouds.

Coastal low clouds can reduce incoming solar radiation by 30-50% [@iacobellis2013variability]. They affect coastal temperatures, drive inland through gaps like the Golden Gate, and historically have provided moisture to ecosystems like redwood forests [@johnstone2010fog].

Here's where the models struggle: Global Climate Models (GCMs) like those in CMIP5 operate at coarse spatial resolution, typically 100-200km grid cells. California's marine layer dynamics happen at much finer scales, driven by sea surface temperature gradients, topographic channeling, diurnal cycles, and offshore upwelling.

Even "downscaled" versions refined to 10-20km resolution have trouble capturing the formation, persistence, and inland penetration of coastal low clouds.

A 2024 California Energy Commission technical report [@pierce2024downscaling] noted: "Coastal Low Clouds (CLC), including stratocumulus, stratus, and fog, are a persistent, seasonal feature of the region's climate... Accounting for these clouds improves solar energy forecasting."

CMIP5-era models systematically underestimated marine layer frequency, inland fog extent, and afternoon/evening cloud cover in coastal areas. The result? Solar irradiance projections that were too optimistic, especially for afternoon/evening hours—exactly when California's "net peak" demand occurs.

------------------------------------------------------------------------

## The Path From Models to Blackouts

So how does a bias in cloud cover modeling contribute to blackouts years later? The chain looks like this:

**Step 1: Overoptimistic generation forecasts (2010-2015)** Solar developers use downscaled CMIP5 projections to estimate generation potential. Models underestimate afternoon cloud cover → projected generation is 10-15% higher than reality.

**Step 2: Investment and siting decisions (2012-2018)**  
Billions flow into solar installations based on these projections. Agricultural land is converted to solar farms in the Central Valley.

**Step 3: Grid planning assumptions (2015-2020)**  
California's grid operator (CAISO) plans for solar providing increasing amounts of power, assuming generation levels that match the overoptimistic projections.

**Step 4: The "net peak" emerges**  
As solar capacity grows, managing the evening "ramp" becomes critical. CAISO assumes more solar will be available during this 5-8pm window than actually materializes.

**Step 5: August 2020**  
Record-breaking heat wave. Solar performs well mid-day but drops off earlier and faster than grid models expected, partly due to cloud cover patterns the original climate models hadn't captured accurately.

CAISO's root cause analysis [@caiso2021root] states: "Resource planning targets have not kept pace with the evolving power mix, wherein demand during peak hours outpaces the supply of solar-produced power."

Translation: we thought we'd have more power available at this time of day than we actually do.

------------------------------------------------------------------------

## Why This Matters Beyond One Heat Wave

The marine layer bias wasn't the *primary* cause of California's blackouts—the CAISO analysis makes clear it was a perfect storm. But the model bias was one link in a chain of assumptions that put the grid in a vulnerable position.

What strikes me is how invisible this connection is. When I experienced that blackout, I had no idea that decisions made years earlier, based on climate projections that seemed rigorous, had quietly baked in errors that would manifest under stress.

------------------------------------------------------------------------

## The Technical Challenge

Climate modelers know about these biases. Marine boundary layer clouds remain one of the largest sources of uncertainty in climate models [@klein2017]. Understanding different types of bias is crucial for interpreting these limitations [@konno2024bias].

The core issue: marine layer physics operates at scales GCMs can't resolve. Coastal fog and low stratus depend on boundary layer processes, cloud microphysics, and topographic features at sub-grid scales [@klein2017].

Downscaling helps, but can't fix everything. Statistical downscaling (e.g., LOCA) uses historical relationships but assumes future will match past patterns—questionable under climate change [@ekstrom2015appraisal]. Dynamical downscaling (e.g., WRF) simulates physics at finer scale but inherits GCM biases.

California's most recent effort (LOCA2) combines both methods [@pierce2024downscaling]. A 2024 technical report notes improvements but still recommends changing planning assumptions to account for limitations.

Different bias correction methods can change answers significantly [@maraun2016bias]. The choice of correction method can change your projection as much as the climate change signal itself [@ekstrom2015appraisal].

This pattern shows up across environmental data science—flood risk, crop yields, hydropower, fire risk. Model biases propagate through decision chains [@olteanu2019social]. The projections look authoritative, but limitations don't make it into summary documents as often as needed.

------------------------------------------------------------------------

## What We Can Do

Climate models are the best tools we have for understanding future conditions. But we need to be smarter about working with their limitations.

### 1. Bake Uncertainty Into Decisions

Design systems that can adapt if reality diverges from projections. For California's grid, this means maintaining flexible generation capacity and planning for higher evening peaks than models predict.

### 2. Cross-Validate With Multiple Methods

- Historical records (what happened during past heat waves?)
- Empirical relationships (how does actual solar generation track with weather?)
- Expert opinions (what do grid operators see on the ground?)
- Stress testing (what if generation is 20% lower than projected?)

### 3. Create Feedback Loops

Compare actual vs. projected generation once solar farms are operating. Feed that learning back into model improvement and planning. California is doing this now, but it would have been better to catch discrepancies earlier.

### 4. Invest in the Unglamorous Work

Bias correction and regional model improvement don't make headlines, but we massively under-invest in making them better. We need better observations, higher resolution models, long-term monitoring, and transparent documentation of known biases.

### 5. Train Decision-Makers to Be Critical Consumers

Engineers, planners, and policy-makers need to understand what climate models can and can't tell them. A projection is not a prediction. Uncertainty is information, not a bug. Model limitations should be discussed upfront, not buried in technical appendices.

------------------------------------------------------------------------

## Why I Think About This

For my family, the 2020 blackouts were uncomfortable but manageable. For others—people dependent on medical equipment, elderly folks without AC—the stakes were higher.

California's grid planning documents still show tight supply margins during evening peaks. The marine layer isn't going anywhere, though research suggests California coastal fog has declined 33% since 1950 [@johnstone2010fog], with uncertain causes and future trajectory [@torregrosa2014fog].

What stays with me is how easily biases slip through. Smart people, rigorous methods, peer-reviewed projections, and still, systematic errors propagate until systems fail under stress.

I see this pattern everywhere in environmental data science. Models of crop yields, water availability, species distributions, fire risk—all rest on climate projections with known limitations. Those limitations exist, they're documented in technical reports, but they don't always make it into decision documents.

We can build better decision processes, ones that explicitly account for uncertainty, validate projections against reality, and adapt when mismatches appear. That starts with recognizing that environmental data always carries the fingerprints of choices and assumptions baked into its creation [@walker2018practicing].

Understanding those limitations isn't about being critical for its own sake. It's about learning from what happened and doing better next time.

------------------------------------------------------------------------

## References

::: {#refs}
:::