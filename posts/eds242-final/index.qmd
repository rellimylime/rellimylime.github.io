---
title: "The Fog the Models Missed"
description: "How climate model biases in California's marine layer contributed to the 2020 blackouts"
author: Emily Miller
date: 2024-12-06
categories: [MEDS, Climate Science, Data Science, Ethics]
format: html
image: eds242-final-cover2.jpeg
citation:
  url: https://rellimylime.github.io/posts/eds242-final/
bibliography: references.bib
---

August 2020, Davis, California. Opening the door at 3pm felt like opening an oven set to broil. My family rationed AC before the afternoon heat peaked, then toughed it out through the evening. The message from our utility was clear: if we wanted to avoid blackouts, everyone needed to cut power during the hottest hours.

I thought I understood: heat wave, everyone running AC, grid overloaded. But while researching for my Ethics and Bias class, I discovered something unexpected. Part of California's blackout problem stemmed from decisions made years earlier, based on climate models that systematically overestimated how much solar power would be available on hot summer evenings.

------------------------------------------------------------------------

## Following the Thread

The California Independent System Operator's root cause analysis [@caiso2021root] pointed to unprecedented heat, inadequate planning for evening "net peak" demand, and market forecasting errors. But embedded in those findings: resource planning hadn't kept pace with California's changing energy mix.

In the 2010s, California made massive investments in solar infrastructure based on downscaled CMIP5 climate model projections. The problem? The models systematically overestimated solar generation in regions influenced by California's coastal meteorology.

## The Marine Layer Problem

California's coast has a distinctive climate feature: the marine layer. Cold Pacific waters create a stable layer of cool, moist air trapped under warmer air above, forming a temperature inversion that produces fog and low stratus clouds.

Coastal low clouds can reduce incoming solar radiation by 30-50% [@iacobellis2013variability]. They drive inland through gaps like the Golden Gate and historically provided moisture to redwood forests [@johnstone2010fog].

::: {.columns}
::: {.column width="48%"}
**What the Models See**  
100-200 km grid cells  
(size of SF Bay Area)
:::
::: {.column width="48%"}
**What Actually Matters**  
Marine layer dynamics at <10 km  
(Golden Gate, coastal valleys)
:::
:::

Global Climate Models (GCMs) in CMIP5 operate at coarse resolution—typically 100-200km grid cells. Marine layer dynamics happen at much finer scales, driven by sea surface gradients, topographic channeling, diurnal cycles, and upwelling.

Even "downscaled" versions refined to 10-20km resolution struggle to capture marine layer formation, persistence, and inland penetration.

A 2024 California Energy Commission report [@pierce2024downscaling] noted: "Coastal Low Clouds... are a persistent, seasonal feature... Accounting for these clouds improves solar energy forecasting."

CMIP5-era models systematically underestimated marine layer frequency, inland fog extent, and afternoon/evening cloud cover. Result? Solar irradiance projections that were too optimistic, especially for afternoon/evening hours—exactly when California's "net peak" demand occurs.

------------------------------------------------------------------------

## The Path From Models to Blackouts

So how does a bias in cloud cover modeling contribute to blackouts years later? 

The chain looks like this:

```{mermaid}
%%| fig-width: 8
flowchart TD
    A["2010-2015<br/>CMIP5 Models<br/>Overestimate Solar"] --> B["2012-2018<br/>Billions Invested<br/>in Solar Farms"]
    B --> C["2015-2020<br/>Grid Planning<br/>Assumes High Generation"]
    C --> D["Net Peak Problem<br/>Evening Ramp<br/>Larger Than Expected"]
    D --> E["August 2020<br/>Heat Wave<br/>Blackouts"]
    
    style A fill:#fee5d9,stroke:#a63603
    style B fill:#fcbba1,stroke:#a63603
    style C fill:#fc9272,stroke:#a63603
    style D fill:#fb6a4a,stroke:#a63603
    style E fill:#de2d26,stroke:#a63603
```

**Step 1: Overoptimistic generation forecasts (2010-2015)** Solar developers use downscaled CMIP5 projections. Models underestimate afternoon cloud cover → projected generation 10-15% higher than reality.

**Step 2: Investment decisions (2012-2018)**  
Billions flow into solar installations. Agricultural land converts to solar farms in Central Valley.

**Step 3: Grid planning (2015-2020)**  
CAISO plans for solar providing increasing power, assuming generation levels matching the overoptimistic projections.

**Step 4: The "net peak" emerges**  
Managing the evening "ramp" becomes critical. CAISO assumes more solar during the 5-8pm window than materializes.

**Step 5: August 2020**  
Heat wave. Solar performs well mid-day but drops off earlier and faster than expected, partly due to cloud patterns the models hadn't captured.

CAISO's root cause analysis [@caiso2021root] states: "Resource planning targets have not kept pace with the evolving power mix, wherein demand during peak hours outpaces the supply of solar-produced power."


> #### Translation
> We thought we'd have more power available at this time of day than we actually do.


------------------------------------------------------------------------

### Why This Matters Beyond One Heat Wave

The marine layer bias wasn't the *primary* cause of California's blackouts—CAISO's analysis shows it was a perfect storm. But the model bias was one link in a chain of assumptions that created vulnerability.

What strikes me is the invisibility. When I experienced that blackout, decisions made years earlier based on seemingly rigorous projections had baked in errors that would manifest under stress.

## Connections to EDS 242 Course Concepts

This case demonstrates several types of bias we studied:

**Model specification bias** (Konno et al.; Lecture 2's Catalogue of Bias)—systematic errors in how marine layer physics are represented in the model structure itself.

**Historical bias** (Lecture 2)—the LOCA downscaling method assumes future relationships will match past patterns, which breaks down under climate change.

**Reporting bias** (Lecture 2)—model limitations were documented in technical reports but didn't get reported in the summary documents that informed billion-dollar decisions. This is what Cori Lopazanski called the "problem of too much information" in her guest lecture—scientists have caveats, but those get lost in translation to policy.

**Data invisibility** (Lecture 1, Barrowman reading)—by 2020, the connection back to 2015 cloud modeling choices was completely invisible to everyone affected. As we learned in Lecture 9, these are "known unknowns" that become "unknown unknowns" as they propagate through decision systems.

The bias propagates through what Olteanu et al. call "decision chains," compounding at each step. This connects to **Environmental Data Justice** themes from Lecture 8—who gets hurt when data systems fail? Not the modelers or grid planners, but families who lost groceries, elderly folks without AC, people on medical equipment.

</details>

------------------------------------------------------------------------

## The Technical Challenge

Climate modelers know about these biases. Marine boundary layer clouds remain one of the largest sources of uncertainty in climate models [@klein2017]. Understanding different types of bias is crucial for interpreting these limitations [@konno2024bias].

::: {.panel-tabset}

### The Problem

Marine layer physics operates at scales GCMs can't resolve. Coastal fog and low stratus depend on boundary layer processes, cloud microphysics, and topographic features at sub-grid scales [@klein2017].

### Downscaling

**Statistical downscaling (LOCA):** Uses historical relationships but assumes future will match past patterns—questionable under climate change [@ekstrom2015appraisal]. 

**Dynamical downscaling (WRF):** Simulates physics at finer scale but inherits GCM biases.

**California's approach (LOCA2):** Combines both methods [@pierce2024downscaling]. A 2024 technical report notes improvements but still recommends changing planning assumptions to account for limitations.

### Bias Correction

Different bias correction methods can change answers significantly [@maraun2016bias]. The choice of method can shift your projection as much as the climate signal itself [@ekstrom2015appraisal].

Common approaches: quantile mapping (assumes stationary bias), delta methods (assumes correct trends), deep learning (overfitting risk), and ensemble averaging (masks uncertainties).

### Broader Pattern

This pattern shows up across environmental data science—flood risk, crop yields, hydropower, fire risk. Model biases propagate through decision chains [@olteanu2019social]. The projections look authoritative, but limitations don't make it into summary documents as often as needed.

:::

------------------------------------------------------------------------

## What We Can Do

Climate models are our best tools for understanding future conditions, but we need to work smarter with their limitations.

#### 1. Bake Uncertainty Into Decisions

Design systems that adapt if reality diverges. For California's grid: maintain flexible generation capacity, plan for higher evening peaks than models predict.

#### 2. Cross-Validate With Multiple Methods

- Historical records (past heat wave outcomes)
- Empirical relationships (actual solar vs. weather)
- Expert opinions (grid operator observations)
- Stress testing (20% lower generation scenarios)

#### 3. Create Feedback Loops

Compare actual vs. projected generation. Feed learning back into models and planning. California is doing this now, but earlier detection would have helped.

#### 4. Invest in the Unglamorous Work

Bias correction and regional model improvement don't make headlines, but we under-invest badly. Need better observations, higher resolution models, long-term monitoring, transparent documentation of biases.

#### 5. Train Decision-Makers

Engineers, planners, policy-makers need to understand what models can and can't tell them. A projection isn't a prediction. Uncertainty is information. Limitations should be upfront, not buried in appendices.

------------------------------------------------------------------------

## Why I Think About This

For my family, the 2020 blackouts were uncomfortable but manageable. For others—people dependent on medical equipment, elderly folks without AC—the stakes were higher.

California's grid planning documents still show tight supply margins during evening peaks. The marine layer isn't going anywhere, though research suggests California coastal fog has declined 33% since 1950 [@johnstone2010fog], with uncertain causes and future trajectory [@torregrosa2014fog].

What stays with me is how easily biases slip through. Smart people, rigorous methods, peer-reviewed projections, and still, systematic errors propagate until systems fail under stress.

I see this pattern everywhere in environmental data science. Models of crop yields, water availability, species distributions, fire risk—all rest on climate projections with known limitations. Those limitations exist, they're documented in technical reports, but they don't always make it into decision documents.

We can build better decision processes, ones that explicitly account for uncertainty, validate projections against reality, and adapt when mismatches appear. That starts with recognizing that environmental data always carries the fingerprints of choices and assumptions baked into its creation [@walker2018practicing].

Understanding those limitations isn't about being critical for its own sake. It's about learning from what happened and doing better next time.

------------------------------------------------------------------------

## References

::: {#refs}
:::
