---
title: "When Climate Models Miss the Fog: What I Learned From California's Blackouts"
description: "..."
author: Emily Miller
date: 2025-11-29
categories: [...]
format: html
image: eds242-final-cover.jpg
citation:
  url: https://rellimylime.github.io/posts/eds242-final/
bibliography: references.bib
---

August 2020. Opening the door is akin to opening the oven in the middle of summer in Davis CA. For the past few years, we had adopted the practice of doing one last blast chill from the AC before avoiding power use after 3pm. This was a responsibility communicated to our whole community; if we want to avoid blackouts, we need to limit our use during the hottest hours of the evening.

Before digging into this topic further, my understanding was that this was straightforward heat wave business; everyone running AC, grid can't keep up, but while doing some exploratory research for my Ethics and Bias in Environmental Data Science class, I stumbled upon something that connected directly to this experience in an interesting way.

Part of California's blackout problem stemmed from systematic errors in how we modeled solar generation capacity years earlier. This isn't a story about renewable energy failing. It's about what happens when the climate models we use to make billion-dollar infrastructure decisions contain biases remain invisible until it's too late.

------------------------------------------------------------------------

## Following the Thread

I started looking into this because I wanted to understand case studies of model bias that had real consequences. The 2020 blackouts kept coming up in my reading, but not for the reasons I expected.

The California Independent System Operator's root cause analysis [@caiso2021root] pointed to three main factors: unprecedented heat, inadequate planning for evening "net peak" demand, and market forecasting errors. But embedded in those findings was something more subtle: resource planning hadn't kept pace with California's changing energy mix.

Here's what I found when I dug deeper: In the 2010s, California made massive investments in solar infrastructure—billions of dollars' worth—based on projections of where and how much solar energy these installations would generate over their 25-30 year lifespans. Those projections came from downscaled climate models, specifically CMIP5 outputs that had been refined to California's regional scale.

The problem? The models systematically overestimated solar generation potential in certain regions, particularly areas influenced by California's complex coastal meteorology.

## The Marine Layer Problem

California's coast has a distinctive climate feature: the marine layer. Cold Pacific waters create a stable layer of cool, moist air that gets trapped under warmer air above, forming a strong temperature inversion. When conditions are right, this marine layer produces fog and low stratus clouds that can persist for hours or days.

This isn't just a beach nuisance—it's climatically critical. Coastal low clouds can reduce incoming solar radiation by 30-50% when they're present [@iacobellis2013variability]. They affect coastal temperatures, drive inland through gaps like the Golden Gate, and historically have provided moisture to ecosystems like redwood forests [@johnstone2010fog].

Here's where the models struggle: Global Climate Models (GCMs) like those in CMIP5 operate at coarse spatial resolution—typically 100-200km grid cells. California's marine layer dynamics happen at much finer scales [@koracin2005fog], driven by: - Sea surface temperature gradients - Topographic channeling through coastal ranges - Diurnal heating/cooling cycles - Offshore upwelling patterns

Even "downscaled" versions of these models—refined to 10-20km resolution using statistical or dynamical techniques—have trouble capturing the formation, persistence, and inland penetration of coastal low clouds.

A 2024 California Energy Commission technical report on downscaling methods [@pierce2024downscaling] noted: "Coastal Low Clouds (CLC), including stratocumulus, stratus, and fog, are a persistent, seasonal feature of the region's climate... Accounting for these clouds improves solar energy forecasting."

The problem is that CMIP5-era models systematically underestimated: 1. Marine layer frequency and persistence 2. The inland extent of coastal fog 3. Afternoon/evening cloud cover in coastal and near-coastal areas

The result? Solar irradiance projections that were too optimistic, especially for afternoon/evening hours—exactly when California's "net peak" demand (after accounting for mid-day solar) occurs.

------------------------------------------------------------------------

## The Path From Models to Blackouts

So how does a bias in cloud cover modeling in the 2010s contribute to blackouts in 2020? The chain looks like this:

**Step 1: Overoptimistic generation forecasts (2010-2015)** Solar developers and utilities use downscaled CMIP5 projections to estimate generation potential. Models underestimate afternoon cloud cover → projected generation is 10-15% higher than reality in affected regions.

**Step 2: Investment and siting decisions (2012-2018)**\
Based on these projections, billions flow into solar installations. Project economics assume certain capacity factors (how much power gets generated relative to maximum capacity). Land use changes—agricultural land converted to solar farms in the Central Valley.

**Step 3: Grid planning assumptions (2015-2020)**\
California's grid operator (CAISO) plans for a future where solar provides increasing amounts of power. But their "resource adequacy" models assume generation levels that match the overoptimistic projections.

**Step 4: The "net peak" emerges**\
As solar capacity grows, a new challenge appears: managing the evening "ramp" when solar generation drops off but demand stays high (people come home, turn on AC, cook dinner, etc.). CAISO's planning assumes more solar will be available during this critical 5-8pm window than actually materializes.

**Step 5: August 2020**\
Record-breaking heat wave hits the entire Western US. Everyone's running AC. Solar performs well during the mid-day peak—but starts dropping off earlier and faster than grid models expected, partly because of cloud cover patterns that the original climate models hadn't captured accurately.

CAISO's root cause analysis [@caiso2021root] states: "Resource planning targets have not kept pace with the evolving power mix, wherein demand during peak hours outpaces the supply of solar-produced power."

That's bureaucratic language for: we thought we'd have more power available at this time of day than we actually do.

Combined with other factors (neighboring states also in crisis, some gas plants offline, market forecasting errors), the result was my phone buzzing with a blackout alert on a sweltering August evening.

------------------------------------------------------------------------

## Why This Matters Beyond One Heat Wave

I'm not claiming that marine layer bias in climate models was the *primary* cause of California's blackouts. The CAISO analysis makes clear it was a perfect storm of factors. But the model bias was part of the systemic chain—one link in a series of assumptions and decisions that put the grid in a vulnerable position.

What strikes me is how invisible this connection is. When I experienced that blackout, I had no idea that decisions made 5-10 years earlier, based on climate projections that seemed rigorous and scientific, had quietly baked in errors that would manifest years later under stress.

This pattern shows up everywhere in environmental data science:

## The Technical Challenge

Climate modelers know about these biases. Coastal low cloud representation is a notorious problem in GCMs—it's been flagged in every CMIP assessment going back decades.

The core issue is that marine layer physics operates at scales GCMs can't resolve: - **Boundary layer dynamics**: The marine layer is typically 300-800m deep. GCMs have vertical resolution of \~100m in the lowest levels—barely enough to capture the inversion structure. - **Cloud microphysics**: Fog and low stratus formation depends on aerosol concentrations (sea salt, iodine from kelp), which most models treat as fixed background values rather than dynamic variables. - **Topographic interactions**: Coastal ranges create gaps (Golden Gate, Carquinez Strait) that channel marine air inland. A 100km GCM grid cell can't represent these features.

Downscaling helps, but it can't fix everything. The two main approaches are:

**Statistical downscaling** (e.g., LOCA - Localized Constructed Analogs): Uses historical relationships between large-scale climate patterns and local observations [@pierce2023loca]. Fast, cheap, widely used. But assumes future relationships will match past patterns—questionable under climate change [@ekstrom2015appraisal]. Also struggles with variables like cloud cover that don't have long observational records.

**Dynamical downscaling** (e.g., WRF - Weather Research and Forecasting model): Runs a high-resolution regional climate model nested within the GCM [@cec2022evaluating]. Can explicitly simulate physics at 3-10km scale. But inherits GCM boundary conditions and biases. And "high resolution" still isn't fine enough to fully capture marine layer processes.

California's most recent downscaling effort (for the 5th Climate Assessment) uses a hybrid approach—LOCA2 combines statistical methods with pattern libraries from dynamically downscaled runs [@pierce2024downscaling]. A 2024 technical report notes this improves solar radiation estimates... but still recommends that "given our shift to clean energy, we have to change planning assumptions and analysis to account for" the limitations.

Translation: even our best current methods have biases that matter for decision-making.

Different bias correction methods can change answers significantly [@maraun2016bias]: - **Quantile mapping**: Matches model distribution to observed distribution, but assumes bias is stationary over time - **Delta method**: Uses model *changes* rather than absolute values, but assumes models capture trends correctly even if baseline is wrong - **Deep learning corrections**: Train neural networks on historical errors, but risk overfitting [@mcgovern2024identifying] - **Ensemble averaging**: Combine multiple models to reduce individual biases, but can mask structural uncertainties

Each method makes different assumptions. Each gives different results. And the choice of correction method can change your projection as much as the climate change signal itself [@ekstrom2015appraisal].

-   **Flood risk modeling** → Insurance pricing → Land use decisions → Who lives where
-   **Crop yield projections** → Agricultural investment → Food security policy → International trade
-   **Hydropower forecasting** → Energy mix decisions → Electricity rates → Industrial development
-   **Fire risk assessment** → Building codes → Insurance availability → Community resilience

In each case, biased models generate biased projections, which inform biased decisions, which create real-world consequences—often for people who have no idea the original model even existed.

And here's what makes it particularly insidious: the bias doesn't announce itself. The projections look authoritative. They come with error bars and confidence intervals. They're peer-reviewed, published, cited. The agencies using them are staffed with smart, well-trained people doing their best with the tools available.

The problem isn't incompetence. It's that we've built systems that rely on models with known limitations, and those limitations propagate silently through decision chains until something breaks.

------------------------------------------------------------------------

## What We Can Do

I don't think the answer is to stop using climate models—they're the best tools we have for understanding future conditions. But we need to be much smarter about acknowledging and working with their limitations.

**1. Bake uncertainty into decisions from the start**\
Don't make billion-dollar commitments based on single-model projections. Design systems that can adapt if reality diverges from projections. For California's grid: that means maintaining flexible generation capacity, investing in storage, planning for higher evening peaks than models predict.

**2. Cross-validate with multiple methods**\
Don't rely solely on climate models. Use: - Historical analogs (when similar heat waves hit in the past, what happened?) - Empirical relationships (how does actual solar generation track with weather obs?) - Expert elicitation (what do grid operators and solar engineers see on the ground?) - Stress testing (what if generation is 20% lower than projected?)

**3. Create feedback loops between models and reality**\
Once solar farms are operating, compare actual vs. projected generation. Feed that learning back into model improvement and planning assumptions. California is doing this now—CAISO has updated resource adequacy models based on 2020 experience—but it would have been better to catch the discrepancies earlier.

**4. Invest in the unglamorous work**\
Bias correction and regional model improvement don't make headlines. But given how much rides on these projections, we massively under-invest in making them better. We need: - Better observations of marine layer dynamics (more buoys, coastal radar, satellite validation) - Higher resolution models that can resolve key processes - Long-term monitoring of how projections perform vs. reality - Transparent documentation of known biases and their magnitudes

**5. Train decision-makers to be critical consumers**\
Engineers, planners, and policy-makers need to understand what climate models can and can't tell them. A projection is not a prediction. An ensemble mean is not a forecast. Uncertainty is information, not a bug. Model limitations should be discussed upfront in every assessment, not buried in technical appendices.

------------------------------------------------------------------------

## Why I Think About This

For my family, these summertime blackouts were uncomfortable, inconvenient, but not catastrophic. For others, people dependent on medical equipment, elderly folks without AC, families losing food in fridges, these blackouts are far more impactful.

And the thing is, these grid reliability issues will keep happening. California's 2024-2025 planning documents still show tight supply margins during evening peaks. The marine layer isn't going away. Neither is climate change, which may be altering fog patterns in ways we still don't fully understand [@torregrosa2014fog]—research suggests California coastal fog has declined 33% since 1950 [@johnstone2010fog], but the causes and future trajectory are uncertain.

What bugs me most is how easily these issues slip through. Smart people, good intentions, rigorous methods—and still, systematic biases propagate through planning processes and don't surface until systems fail under stress.

This isn't unique to energy systems. I see it in every domain of environmental data science I look at. Models of crop yields, water availability, species distributions, fire risk, flood hazards—all rest on climate projections with known limitations. Those limitations rarely make it into the summary documents that inform decisions [@young2014improving].

We can do better. Not by building perfect models, but by building decision processes that explicitly account for model uncertainty and biases, and learning from mismatches between predictions and reality.

That starts with recognizing that data, especially in environmental science, is never neutral, complete, or certain [@barrowman2018raw; @walker2018practicing]. Every dataset carries the fingerprints of choices, assumptions, and limitations baked into its creation.

Understanding those limitations isn't pedantic. It's how we avoid making the same mistakes again.

------------------------------------------------------------------------

## References

::: {#refs}
:::
